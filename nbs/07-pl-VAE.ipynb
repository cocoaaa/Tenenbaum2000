{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import re\n",
    "import math\n",
    "from datetime import datetime\n",
    "import time\n",
    "sys.dont_write_bytecode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Set, Dict, Tuple, Optional, Iterable, Mapping, Union, Callable, TypeVar\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "from ipdb import set_trace as brpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from  torch.linalg import norm as tnorm\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.tuner.tuning import Tuner\n",
    "\n",
    "# Select Visible GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Path \n",
    "1. Add project root and src folders to `sys.path`\n",
    "2. Set DATA_ROOT to `maptile_v2` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_nb_path = Path(os.getcwd())\n",
    "ROOT = this_nb_path.parent\n",
    "SRC = ROOT/'src'\n",
    "DATA_ROOT = Path(\"/data/hayley-old/maptiles_v2/\")\n",
    "paths2add = [this_nb_path, ROOT]\n",
    "\n",
    "print(\"Project root: \", str(ROOT))\n",
    "print('Src folder: ', str(SRC))\n",
    "print(\"This nb path: \", str(this_nb_path))\n",
    "\n",
    "\n",
    "for p in paths2add:\n",
    "    if str(p) not in sys.path:\n",
    "        sys.path.insert(0, str(p))\n",
    "        print(f\"\\n{str(p)} added to the path.\")\n",
    "        \n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.datasets.maptiles import MaptilesDataset, MapStyles\n",
    "from src.data.datamodules.maptiles_datamodule import MaptilesDataModule\n",
    "\n",
    "from src.data.transforms.transforms import Identity, Unnormalizer, LinearRescaler\n",
    "from src.data.transforms.functional import unnormalize\n",
    "from src.models.plmodules.three_fcs import ThreeFCs\n",
    "\n",
    "from src.visualize.utils import show_timgs, show_batch\n",
    "from src.utils.misc import info\n",
    "from src.models.model_wrapper import ModelWrapper\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dm: MaptilesDataModule, n_show: int=16, **kwargs):\n",
    "    \"\"\"\n",
    "    -kwargs will be passed into `show_timgs` function\n",
    "        - titles: a list of titles for axes; must be the same length as the number of npimgs\n",
    "        - nrows\n",
    "        - factor\n",
    "        - cmap (str): eg. \"gray\"\n",
    "        - title (for the main figure's suptitle)\n",
    "    \"\"\"\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", module=\"matplotlib*\")\n",
    "        # Show a batch of train data\n",
    "        # - before/after channelwise normalization\n",
    "        x,y = next(iter(dm.train_dataloader()))\n",
    "        train_mean, train_std = dm.train_mean, dm.train_std\n",
    "\n",
    "        # Undo normalization\n",
    "        x_unnormed = unnormalize(x, train_mean, train_std)\n",
    "        \n",
    "        info(x, \"batch x\")\n",
    "        info(x_unnormed, \"unnormalized x\")\n",
    "        show_timgs(x_unnormed[:n_show], **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainer(pl.Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs) #check if this sets train/val dataloaders\n",
    "        self.num_data = len(self.train_dataloader().dataset)\n",
    "\n",
    "    def get_dl(mode: str, dl_idx:int=0):\n",
    "        if mode == 'train':\n",
    "            dl = getattr(self, \"train_dataloader\")\n",
    "        else:\n",
    "            dl = getattr(self, f\"{mode}_dataloaders\")[dl_idx]\n",
    "        print(dl)\n",
    "        print(dl.dataset)\n",
    "        return dl\n",
    "    \n",
    "    def get_next_batch(mode: str, dl_idx):\n",
    "        dl = self.get_dl(mode, dl_idx)\n",
    "        return next(iter(dl))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_maptiles_dataset():\n",
    "    cities = ['paris']\n",
    "    styles = ['CartoVoyagerNoLabels', 'StamenTonerBackground']\n",
    "    zooms = ['15']\n",
    "    n_channels = 3\n",
    "    dset = MaptilesDataset(\n",
    "        data_root=DATA_ROOT,\n",
    "        cities=cities, \n",
    "        styles=styles, \n",
    "        zooms=zooms, \n",
    "        n_channels=in_channels\n",
    "    )\n",
    "    train_dset, val_dset = MaptilesDataset.random_split(dset, 0.9)\n",
    "    train_channel_mean, train_channel_std = train_dset.channel_mean, train_dset.channel_std\n",
    "\n",
    "    [print(len(x)) for x in [dset, train_dset, val_dset]];\n",
    "    [print(x.channel_mean, x.channel_std) for x in [dset, train_dset, val_dset]];\n",
    "    \n",
    "def test_maptiles_datamodule_factory():\n",
    "    cities = ['paris']\n",
    "    styles = ['CartoVoyagerNoLabels', 'StamenTonerBackground']\n",
    "    zooms = ['15']\n",
    "    n_channels = 3\n",
    "    dset = MaptilesDataset(\n",
    "        data_root=DATA_ROOT,\n",
    "        cities=cities, \n",
    "        styles=styles, \n",
    "        zooms=zooms, \n",
    "        n_channels=in_channels\n",
    "    )\n",
    "    \n",
    "    in_shape = (n_channels, 64,64)\n",
    "    dm = MaptilesDataModule.from_maptiles_dataset(dset, in_shape=in_shape )\n",
    "    dm.setup('test')\n",
    "    print(dm.train_ds.channel_mean, dm.train_ds.channel_std)\n",
    "    print(dm.train_ds.transform)\n",
    "    print(dm.val_ds.transform)\n",
    "    print(dm.test_ds.transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start experiment \n",
    "Given a maptile, predict its style as one of OSM, CartoVoyager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.plmodules.vanilla_vae import VanillaVAE\n",
    "from src.data.datamodules.maptiles_datamodule import MaptilesDataModule\n",
    "from src.data.datamodules.mnist_datamodule import MNISTDataModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instantiate data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = ['berlin']#['paris'] #\n",
    "# styles = ['OSMDefault', 'CartoVoyagerNoLabels', 'StamenTonerBackground']\n",
    "styles = ['StamenTonerBackground']#['CartoVoyagerNoLabels']#, 'StamenTonerBackground']\n",
    "\n",
    "zooms = ['15']\n",
    "n_channels = 1\n",
    "dset = MaptilesDataset(\n",
    "        data_root=DATA_ROOT,\n",
    "        cities=cities, \n",
    "        styles=styles, \n",
    "        zooms=zooms, \n",
    "        n_channels=n_channels\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_shape = (n_channels, 32,32)\n",
    "bs = 128\n",
    "dm = MaptilesDataModule.from_maptiles_dataset(dset, in_shape=in_shape, batch_size=bs )\n",
    "dm.setup('fit')\n",
    "print(dm.train_ds.channel_mean, dm.train_ds.channel_std)\n",
    "print(dm.train_ds.transform)\n",
    "print(dm.val_ds.transform)\n",
    "# print(dm.test_ds.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dm.train_ds.show_samples(order='chw')\n",
    "# show_batch(dm, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instantiate the pl Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 10\n",
    "hidden_dims = [32,64,128,256]#,512]\n",
    "act_fn = nn.ReLU()\n",
    "lr = 1e-3\n",
    "size_average = False\n",
    "model = VanillaVAE(in_shape=dm.size(), #dm.in_shape, \n",
    "                    latent_dim=latent_dim,\n",
    "                    hidden_dims=hidden_dims,\n",
    "                    act_fn=act_fn,\n",
    "                  learning_rate=lr,\n",
    "                  size_average=size_average)\n",
    "pprint(dm.hparams)\n",
    "pprint(model.hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instantiate a PL `Trainer` object\n",
    "    - most basic trainer: uses good defaults, eg: auto-tensorboard logging, checkpoints, logs, etc.\n",
    "    - Pass the data module along with a pl module\n",
    "    - Ref: https://www.learnopencv.com/tensorboard-with-pytorch-lightning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Callbacks\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "callbacks = [\n",
    "        HistogramLogger(hist_epoch_interval=1),\n",
    "        ReconLogger(recon_epoch_interval=1),\n",
    "#         EarlyStopping('val_loss', patience=10),\n",
    "]\n",
    "\n",
    "# Start the experiment\n",
    "exp_name = f'{model.name}_{dm.train_ds}'\n",
    "tb_logger = pl_loggers.TensorBoardLogger(save_dir=f'{ROOT}/temp-logs', \n",
    "                                         name=exp_name,\n",
    "                                         log_graph=False,\n",
    "                                        default_hp_metric=False)\n",
    "print(tb_logger.log_dir)\n",
    "\n",
    "# Log computational graph\n",
    "model_wrapper = ModelWrapper(model)\n",
    "tb_logger.experiment.add_graph(model_wrapper, model.example_input_array.to(model.device))\n",
    "# tb_logger.log_graph(model)\n",
    "\n",
    "trainer_config = {\n",
    "    'gpus':1,\n",
    "    'max_epochs': 3,\n",
    "    'progress_bar_refresh_rate':20,\n",
    "#     'auto_lr_find': True,\n",
    "    'terminate_on_nan':True,\n",
    "#     'num_sanity_val_steps':0.25,\n",
    "    'check_val_every_n_epoch':10,\n",
    "    'logger':tb_logger,\n",
    "    'callbacks':callbacks,\n",
    "}\n",
    "\n",
    "\n",
    "# trainer = pl.Trainer(fast_dev_run=3)\n",
    "trainer = pl.Trainer(**trainer_config)\n",
    "# trainer.tune(model=model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "trainer.fit(model, dm)\n",
    "print(f\"Finished at ep {trainer.current_epoch, trainer.batch_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log  hparmeters and `best_score` to tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = model.hparams.copy()\n",
    "hparams.update(dm.hparams)\n",
    "best_score = trainer.checkpoint_callback.best_model_score.item()\n",
    "metrics = {'hparam/best_score': best_score} #todo: define a metric and use it here\n",
    "print(hparams)\n",
    "print(\"=\")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pl.Logger's method \"log_hyperparameters\" which handles the \n",
    "# hparams' element's formats to be suitable for Tensorboard logging\n",
    "# See: \n",
    "# https://sourcegraph.com/github.com/PyTorchLightning/pytorch-lightning@be3e8701cebfc59bec97d0c7717bb5e52afc665e/-/blob/pytorch_lightning/loggers/tensorboard.py#explorer:~:text=def%20log_hyperparams\n",
    "best_score = trainer.checkpoint_callback.best_model_score.item()\n",
    "metrics = {'hparam/best_score': best_score} #todo: define a metric and use it here\n",
    "trainer.logger.log_hyperparams(hparams, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "1. Reconstructions\n",
    "    - Given x from train/val/test dataset, show N (eg. 16) number of possible reconstruction\n",
    "    - Workflow: \n",
    "        - x --> model.encoder(x) --> theta_z --> sample N latent codes from the Pr(z; theta_z) --> model.decoder(z) for each sampled z's \n",
    "2. Inspect the topology/landscape of the learned latent space\n",
    "    - Latent traversal: Pick a dimension of the latent space. Keep all other dimensions' values constant. Vary the chosen dimenion's values (eg. linearly, spherically) and decode the latent codes. Show the outputs of the decoder.\n",
    "    \n",
    "3. Mutual information\n",
    "    - Between x and x_sample for N number of x_samples.\n",
    "    - Between each dimensions of a latent code\n",
    "    "
   ]
  },
  {
   "attachments": {
    "6eb9ebcd-08de-4913-9ea4-65f74df5f466.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (estimate) Marginal Loglikelihood of train/val/test dataset\n",
    "- Option1: $ELBO_{VAE}$\n",
    "\n",
    "Steps:\n",
    "1. Sample a bunch of z's from the chosen prior. Note **not!** from the learned q dist.\n",
    "2. mu_x = model.decoder(sampled_z)\n",
    "3. Compute (avg) likelihood of input batch x under Gaussian(mu_x, Identity cov)\n",
    "4. Return the log of the avged likelihood. Note: log cannot be moved inside the averaging/expectation.\n",
    "\n",
    "See [reddit](https://www.reddit.com/r/MachineLearning/comments/5qm6ag/d_how_to_calculate_variational_autoencoder_log/dd0xt0o?utm_source=share&utm_medium=web2x&context=3)\n",
    "\n",
    "- Option2: $\\mathcal{L}_{lwae}$ (better) \n",
    "\n",
    "![image.png](attachment:6eb9ebcd-08de-4913-9ea4-65f74df5f466.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Normal(torch.tensor(0.),1.)\n",
    "d = d.expand([10,2])\n",
    "d.sample()\n",
    "# x = torch.tensor("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todo:\n",
    "- [ ] test this function\n",
    "- [ ] make into a functino \n",
    "- [ ] loop over dl; compute for each single batch x\n",
    "- Use log-exp-sum?\n",
    "- do it in cpu?\n",
    "- see https://github.com/tonywu95/eval_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute log marginal likelihood via IWAE\n",
    "# x is a single batch: (BS=1, c, h,w)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    mu, log_var = model.encode(x[[0]]) #mu: (1, latent_dim), log_var: (1, latent_dim)\n",
    "    breakpoint()\n",
    "\n",
    "    q_z = Normal(mu.squeeze(), log_var.exp().squeeze())\n",
    "    # Collect z's sampled from q(.|mu, log_var)\n",
    "    n_samples = 5\n",
    "    z_samples = model.reparameterize(\n",
    "        mu.repeat(n_samples,1), \n",
    "        log_var.repeat(n_samples,1)\n",
    "    ) # (n_samples, latent_dim)\n",
    "\n",
    "    log_probs_q = q_z.log_prob(z_samples).sum(dim=1) #(n_samples)\n",
    "    probs_q = log_probs.exp()\n",
    "    # ---denom\n",
    "\n",
    "    #--- top\n",
    "    # p(x,z) = p(x|z)*p(z)\n",
    "    # First compute logp(x,z) = logp(x|z) + log p(z)\n",
    "    # Then exp.\n",
    "    prior = Normal(0, 1)\n",
    "    log_probs_prior = prior.log_prob(z_samples).sum(dim=1) #(n_samples,)\n",
    "\n",
    "    mu_x = model.generate(z_samples) #(n_samples,c,h,w)\n",
    "    p_xgz = Normal(mu_x,1)\n",
    "    print(p_xgz.batch_shape, p_xgz.event_shape)\n",
    "    # vs. x is a single batch (1, chw); we want each mu_x[i]\n",
    "    # to be used to compute (n_sample) number of (chw) sized log_probs -> sum over (chw)\n",
    "    # -> end up with (n_sample,) for (a part of ) numerator\n",
    "    log_probs_p_xgz = p_xgz.log_prob(x.repeat((n_samples, *x.shape))).sum(dim=(1,2,3))\n",
    "    log_top = log_probs_p_xgz + log_probs_prior\n",
    "    top = log_top.exp()\n",
    "\n",
    "    #log-exp-sum?\n",
    "    les = logexpsum(log_top - log_probs_q)\n",
    "    lwse = (les.exp()/n_samples).log()\n",
    "\n",
    "\n",
    "    inside = torch.mean(top/probs_q)\n",
    "\n",
    "    lwae = inside.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import MultivariateNormal, Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given mu_x (shape: (nC, H, W)) and x = (bs=1, C,H,W), compute its loglikelihood\n",
    "# N(x; mu_x, Identity)\n",
    "logp = torch.tensor(0.)\n",
    "x = torch.tensor([-2., 0., 1.])\n",
    "d = Normal(torch.tensor(0.), torch.tensor(1.0))\n",
    "d.log_prob(x).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian = lambda x: (-.5*(x)**2).exp() / np.sqrt(2*np.pi)\n",
    "gaussian(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = mu_x.shape[0] \n",
    "assert bs == 1\n",
    "mu_x = mu_x[0]\n",
    "d = Normal(mu_x, torch.ones(mu_x.shape))\n",
    "d.log_prob(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llhs = {}\n",
    "with torch.no_grad():\n",
    "    for mode in ['train', 'val']:\n",
    "        prob_sum = torch.tensor(0.)\n",
    "        n_imgs = 0\n",
    "        dl = getattr(dm, f\"{mode}_dataloader\")()\n",
    "        for (x,y) in dl:           \n",
    "            bs = x.shape[0]\n",
    "            \n",
    "            # sample z -> decoder -> mu_x\n",
    "            z_sample = torch.randn((bs, model.latent_dim), device=model.device)\n",
    "            mu_x = model.decode(z_sample)\n",
    "            \n",
    "            dist = Normal(mu_x, torch.ones(mu_x.shape))\n",
    "            log_prob = torch.sum(dist.log_prob(x), dim=(1,2,3)) # (bs,1)\n",
    "            prob = log_prob#.exp()\n",
    "            \n",
    "            # Accumulate the sum over this batch\n",
    "            prob_sum += torch.sum(prob)\n",
    "            n_imgs += bs\n",
    "\n",
    "        # Log of average likelihood of an image\n",
    "        llhs[mode] = (prob_sum/n_imgs).item()\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                       \n",
    "        x,y = next(iter(dl))\n",
    "        x = x.to(model.device)\n",
    "        x_recon = model.generate(x)    \n",
    "sum_px = torch.tensor(0.0)\n",
    "\n",
    "for x in batch_x:\n",
    "    z_sampled = torch.distribution.Normal\n",
    "    d = MultivariateNormal(mu[i], torch.diagflat(log_var[i].exp()))\n",
    "assert d.batch_shape == ()\n",
    "assert d.event_shape == (latent_dim,)\n",
    "x = d.sample()\n",
    "assert x.shape == (latent_dim,)            # == batch_shape + event_shape\n",
    "assert d.log_prob(x[i]).shape == ()  # == batch_shape\n",
    "print(d.log_prob(x[i]).exp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "n_samples = 36\n",
    "with torch.no_grad():\n",
    "    sampled_recons = model.sample(n_samples, model.device)\n",
    "    show_timgs(sampled_recons.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_samples = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recons of inputs from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();\n",
    "\n",
    "# Method1: Use a trainer to load a best checkpt.\n",
    "# trainer.test(model, test_dataloaders=dm.test_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method2: Manually get the checkpoint path to the best model's weight\n",
    "# -- See https://pytorch-lightning.readthedocs.io/en/stable/_modules/pytorch_lightning/trainer/trainer.html#Trainer.test\n",
    "from pytorch_lightning.utilities.cloud_io import load as pl_load\n",
    "\n",
    "ckpt_path = trainer.checkpoint_callback.best_model_path\n",
    "ckpt = pl_load(ckpt_path, map_location=lambda storage, loc: storage) #dict object\n",
    "model.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "for k,v in ckpt.items():c\n",
    "    if 'state' in k:\n",
    "        continue\n",
    "    pprint(f\"{k}:{v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method3: Get the last model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grid_from_tensors(tensors: List[torch.Tensor], dim: int=-1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    - dim (int): \n",
    "        - use -1 to put the tensors side-by-side\n",
    "        - use -2 to put them one below another\n",
    "        \n",
    "    Example:\n",
    "    grid = make_grid_from_tensors([grid_input, grid_recon], dim=-1)\n",
    "    tb_writer.add_image(\"input-recon\", grid, global_step=0)\n",
    "    \"\"\"\n",
    "    grids = [torchvision.utils.make_grid(t) for t in tensor] # each element has size, eg.(C, gridh, gridw)\n",
    "    combined = torch.cat(grids, dim=dim)\n",
    "    return combined\n",
    "\n",
    "def log_recon(trainer, model, **kwargs):\n",
    "    \"\"\"\n",
    "    Run it after train/val epoch\n",
    "    kwargs:\n",
    "    - dim (int): \n",
    "        - Use -1 for side-by-side stacking of x and x_recon.\n",
    "        - Use -2 for up-down stacking of x and x_recon\n",
    "    \"\"\" \n",
    "#     model.eval()\n",
    "    train_mean, train_std = trainer.datamodule.train_mean, trainer.datamodule.train_std\n",
    "    with torch.no_grad():\n",
    "        for mode in ['train', 'val']:\n",
    "            print(\"mode: \", mode, \"\\t\\t is training: \", model.training)\n",
    "            dl = getattr(model, f\"{mode}_dataloader\")()\n",
    "            x,y = next(iter(dl))\n",
    "            x = x.to(model.device)\n",
    "            x_recon = model.generate(x)\n",
    "\n",
    "            # unnormalize for visualization\n",
    "            x = x.cpu()\n",
    "            x_recon = x_recon.cpu()\n",
    "            x_unnormed = unnormalize(x, train_mean, train_std)\n",
    "            x_recon_unnormed = unnormalize(x_recon, train_mean, train_std)\n",
    "\n",
    "            # Log input-recon grid to TB\n",
    "            input_grid = torchvision.utils.make_grid(x_unnormed) # (C, gridh, gridw)\n",
    "            recon_grid = torchvision.utils.make_grid(x_recon_unnormed) # (C, gridh, gridw)\n",
    "            grid = torch.cat([input_grid, recon_grid], dim=-1) #inputs | recons\n",
    "            trainer.logger.experiment.add_image(f\"{mode}/recons\", grid, global_step=model.current_epoch)\n",
    "            \n",
    "            \n",
    "# log_recon(trainer, model)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_hist_img_of_muz_varz(trainer, model, **kwargs):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\" \n",
    "    is_training = model.training\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mu,log_var = model.encode(x)\n",
    "        f, ax = plt.subplots(1,2)\n",
    "        ax = ax.flatten()\n",
    "        ax[0].hist(mu, label='mu')\n",
    "        ax[0].set_title('mu')\n",
    "        ax[1].hist(log_var.exp(), label='var')\n",
    "        ax[1].set_title('var')\n",
    "            \n",
    "        min_var = kwargs.get('min_var', None) or 1e-3\n",
    "        n_tiny_vars = (log_var.exp() < min_var).sum()\n",
    "        p_tiny = n_tiny_vars/log_var.numel()\n",
    "        f.title(f\"%tiny: {p_tiny}\")\n",
    "        \n",
    "        # Log the figure to tensorboard\n",
    "#         trainer.logger.experiment.add_img(f\"{mode}/mu-and-var_z\", f\n",
    "    model.train(is_training)\n",
    "    \n",
    "def log_hist_of_params_of_q(trainer, model, **kwargs):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\" \n",
    "    is_training = model.training\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mu,log_var = model.encode(x)\n",
    "        var_thresh = kwargs.get('var_thresh', None) or 1e-3\n",
    "        n_tiny_vars = (log_var.exp() < var_thresh).sum()\n",
    "        p_tiny = n_tiny_vars/log_var.numel()\n",
    "        \n",
    "        # Log the histograms to tensorboard\n",
    "        trainer.logger.experiment.add_img(f\"{mode}/centers of q\", mu, global_step=model.current_epoch)\n",
    "        trainer.logger.experiment.add_img(f\"{mode}/vars of q\", var, global_step=model.current_epoch)\n",
    "        trainer.logger.experiment.add_scalar(f\"{mode}/p_tiny_var\", p_tiny, gloval_step=model.current_epoch)\n",
    "    model.train(is_training)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean, train_std = dm.train_mean, dm.train_std\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for mode in ['train', 'val']:\n",
    "        dl = getattr(model, f\"{mode}_dataloader\")()\n",
    "        x,y = next(iter(dl))\n",
    "        x = x.to(model.device)\n",
    "        x_recon = model.generate(x)\n",
    "        \n",
    "        # unnormalize for visualization\n",
    "        x = x.cpu()\n",
    "        x_recon = x_recon.cpu()\n",
    "        x_unnormed = unnormalize(x, train_mean, train_std)\n",
    "        x_recon_unnormed = unnormalize(x_recon, train_mean, train_std)\n",
    "        show_timgs(x_unnormed, title=f\"{mode} dataset\", cmap='gray')\n",
    "        show_timgs(x_recon_unnormed, title=f\"{mode}: recon\", cmap='gray')\n",
    "        show_timgs(LinearRescaler()(x_recon_unnormed), title=f\"{mode}: recon\", cmap='gray')\n",
    "        \n",
    "        # Print out\n",
    "        info(x, f\"{mode}_x\")\n",
    "        info(x_recon, f\"{mode}_x_recon\")\n",
    "        print(\"===\")\n",
    "        info(x_unnormed, f\"{mode}_x_unnormed\")\n",
    "        info(x_recon_unnormed, f\"{mode}_x_recon_unnormed\")\n",
    "        \n",
    "        # Log input-recon grid to TB\n",
    "        input_grid = torchvision.utils.make_grid(x_unnormed) # (C, gridh, gridw)\n",
    "        recon_grid = torchvision.utils.make_grid(x_recon_unnormed) # (C, gridh, gridw)\n",
    "#         normed_recon_grid = torchvision.utils.make_grid(LinearRescaler()(x_recon_unnormed))\n",
    "        grid = torch.cat([input_grid, recon_grid], dim=-1) #inputs | recons\n",
    "        tb_logger.experiment.add_image(f\"{mode}/recons\", grid, global_step=0)\n",
    "                            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo:\n",
    "- [ ] log recons at certain interval of train or val steps\n",
    "- [ ] at the same time, log the below histogram of mu and var of latent space\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    mu,log_var = model.encode(x)\n",
    "    f, ax = plt.subplots(1,2)\n",
    "    ax = ax.flatten()\n",
    "    ax[0].hist(mu, label='mu')\n",
    "    ax[0].set_title('mu')\n",
    "    ax[1].hist(log_var.exp(), label='var')\n",
    "    ax[1].set_title('var')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_var = 1e-3\n",
    "n_tiny_vars = (log_var.exp() < min_var).sum()\n",
    "n_tiny_vars/log_var.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_diag_gaussian(n_samples: int, mu: torch.Tensor, logvar: torch.Tensor)-> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns a tensor of shape (n_samples x dimension)\n",
    "    from a gaussian with diagonal covariance and mean=mu.\n",
    "    \n",
    "    - mu: mini-batch of mu vectors; (bs, d)\n",
    "    - logvar: mini-batch of logvar vectors; (bs, d)\n",
    "    \n",
    "    Note: logvar = torch.zeros((bs,d)) --> var = torch.ones((bs,d))\n",
    "    Note: Preserves the gradient flow from the output to the input argument mu and logvar\n",
    "    \"\"\"\n",
    "\n",
    "    std = (0.5*logvar).exp()\n",
    "    eps = torch.randn_like(std)\n",
    "    return mu + eps * std\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recons of samples from learned latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 36\n",
    "with torch.no_grad():\n",
    "    sampled_recons = model.sample(n_samples, model.device)\n",
    "    show_timgs(sampled_recons.detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize embeddings\n",
    "- collect a batch of inputs -> encoder -> [mu, log_var] -> sample -> a batch of z's (embeddings)\n",
    "- use tb logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x, y = next(iter(trainer.train_dataloader))\n",
    "    mu, log_var = model.encode(x)\n",
    "    z = model.reparameterize(mu, log_var)\n",
    "#     out = model.get_embeddings(x) # dict of mu, log_var, z\n",
    "#     z = out['z']\n",
    "    \n",
    "    # log embedding to tensorboard \n",
    "    writer = model.logger.experiment\n",
    "    writer.add_embedding(z,\n",
    "                         label_img=LinearRescaler()(x), \n",
    "                         metadata=y.tolist(),\n",
    "                         global_step=trainer.global_step, #todo\n",
    "                        )\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize original images of the close neighbors in the latent space\n",
    "- Compute pairwise distance using cosine similarity\n",
    "- For each row (ie. a latent code), get the index of the smallest values. \n",
    "- Select the images in the batch x and visualize (can do this all in show_timgs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x, y = next(iter(trainer.train_dataloader))\n",
    "    mu, log_var = model.encode(x)\n",
    "    z = model.reparameterize(mu, log_var)\n",
    "    #     out = model.get_embeddings(x) # dict of mu, log_var, z\n",
    "    #     z = out['z']metric = 'cosine'\n",
    "    pdists = pairwise_distances(z.numpy(), metric=metric)\n",
    "    plt.imshow(pdists, cmap='gray')\n",
    "    plt.title(\"Pairwise dists of z's\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # smaller values means closer in distance\n",
    "    n_ngbrs = 5\n",
    "    n_rows = 100\n",
    "    \n",
    "    selected_rows = np.random.choice(len(x), size=n_rows)\n",
    "    for idx in selected_rows:\n",
    "        args = np.argsort(pdists[idx])[:n_ngbrs]\n",
    "#         print(args)\n",
    "        show_timgs(LinearRescaler()(x[args]), cmap='gray', factor=2, \n",
    "                   nrows=1, title=f'Nearest of img {idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smaller values means closer in distance\n",
    "n_ngbrs = 5\n",
    "n_rows = 10\n",
    "selected_rows = np.random.choice(len(x), size=n_rows)\n",
    "for idx in selected_rows:\n",
    "    args = np.argsort(pdists[idx])[:n_ngbrs]\n",
    "    print(args)\n",
    "    show_timgs(LinearRescaler()(x[args]), cmap='gray', factor=2, \n",
    "               nrows=1, title=f'Nearest of img {idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(pdists[1])[:n_ngbrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ngbrs = 5\n",
    "args = np.argsort(pdists, axis=1)[:n_ngbrs]\n",
    "print(args.shape)\n",
    "# show_timgs(LinearRescaler()(x[args]), cmap='gray', factor=2, nrows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Space Traversal\n",
    "1. Linear traversal in a single dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_dim = 0 # must be in range(latent_dim)\n",
    "fixed_vec = torch.randn((1, model.latent_dim-1))\n",
    "fixed_values = fixed_vec.repeat((n_samples,1))\n",
    "n_samples = 16\n",
    "zi_min, zi_max = -2,2\n",
    "varying = torch.linspace(zi_min, zi_max, n_samples).view((-1,1))\n",
    "\n",
    "varying.shape,fixed_values.shape\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_from(a_col:torch.Tensor, other_cols:torch.Tensor, ind):\n",
    "    \"\"\"\n",
    "    Make a tensor from a column vector and a matrx containing all the other columns\n",
    "    by inserting the `onc_column` at the final matrix's `ind`th column.\n",
    "    \"\"\"\n",
    "    assert a_\n",
    "    n_cols = 1 + \n",
    "    out = a_col.new_zeros(("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement an evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.plmodules.utils import get_best_ckpt, load_model, load_best_model\n",
    "from pytorch_lightning.utilities.cloud_io import load as pl_load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, \n",
    "             ckpt_path:str=None,\n",
    "            tb_writer=None):\n",
    "    if ckpt_path is None:\n",
    "        load_best_model(model) #inplace\n",
    "    else:\n",
    "        load_model(model, ckpt_path) #inplace\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # Reconstruction \n",
    "    # ------------------------------------------------------------------------\n",
    "    eval_recon(model, tb_writer=tb_writer)\n",
    "    \n",
    "    \n",
    "                  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_recon(model, unnormalize:bool=True, to_show:bool=False, verbose:bool=False):\n",
    "    model.eval()\n",
    "    dm = model.trainer.datamodule\n",
    "    cmap = 'gray' if dm.in_shape[\n",
    "    train_mean, train_std = dm.train_mean, dm.train_std\n",
    "    with torch.no_grad():\n",
    "        for mode in ['train', 'val']:\n",
    "            dl = getattr(model, f\"{mode}_dataloader\")()\n",
    "            x,y = next(iter(dl))\n",
    "            x = x.to(model.device)\n",
    "            x_recon = model.generate(x)\n",
    "\n",
    "            x = x.cpu()\n",
    "            x_recon = x_recon.cpu()\n",
    "            \n",
    "            if unnormalize:\n",
    "                x_unnormed = unnormalize(x, train_mean, train_std)\n",
    "                x_recon_unnormed = unnormalize(x_recon, train_mean, train_std)\n",
    "            show_timgs(x_unnormed, title=f\"{mode} dataset\", cmap='gray')\n",
    "            show_timgs(x_recon_unnormed, title=f\"{mode}: recon\", cmap='gray')\n",
    "            show_timgs(LinearRescaler()(x_recon_unnormed), title=f\"{mode}: recon\", cmap='gray')\n",
    "\n",
    "            if verbose:\n",
    "                info(x, f\"{mode}_x\")\n",
    "                info(x_recon, f\"{mode}_x_recon\")\n",
    "                print(\"===\")\n",
    "                info(x_unnormed, f\"{mode}_x_unnormed\")\n",
    "                info(x_recon_unnormed, f\"{mode}_x_recon_unnormed\")\n",
    "\n",
    "            # Log input-recon grid to TB\n",
    "            input_grid = torchvision.utils.make_grid(x_unnormed) # (C, gridh, gridw)\n",
    "            recon_grid = torchvision.utils.make_grid(x_recon_unnormed) # (C, gridh, gridw)\n",
    "    #         normed_recon_grid = torchvision.utils.make_grid(LinearRescaler()(x_recon_unnormed))\n",
    "            grid = torch.cat([input_grid, recon_grid], dim=-1) #inputs | recons\n",
    "            tb_logger.experiment.add_image(f\"{mode}/recons\", grid, global_step=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconLogger(pl.Callback):\n",
    "    def __init__(\n",
    "        self, \n",
    "        recon_epoch_interval: int = 20,\n",
    "        normalize: bool=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.recon_epoch_interval = recon_epoch_interval\n",
    "        self.normalize = normalize\n",
    "    \n",
    "    def on_fit_start(self, *args, **kwargs):\n",
    "        print(f\"{self.__class__.__name__} is called\")\n",
    "    \n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        if (trainer.current_epoch + 1) % self.recon_epoch_interval == 0:\n",
    "            self.log_recon(trainer, pl_module)\n",
    "               \n",
    "    @classmethod\n",
    "    def log_recon(cls, trainer, model, **kwargs):\n",
    "        \"\"\"\n",
    "        Run it after train/val epoch\n",
    "        kwargs:\n",
    "        - dim (int): \n",
    "            - Use -1 for side-by-side stacking of x and x_recon.\n",
    "            - Use -2 for up-down stacking of x and x_recon\n",
    "        \"\"\" \n",
    "        is_training = model.training\n",
    "        model.eval()\n",
    "        train_mean, train_std = trainer.datamodule.train_mean, trainer.datamodule.train_std\n",
    "        with torch.no_grad():\n",
    "            for mode in ['train', 'val']:\n",
    "                dl = getattr(model, f\"{mode}_dataloader\")()\n",
    "                x,y = next(iter(dl))\n",
    "                x = x.to(model.device)\n",
    "                x_recon = model.generate(x)\n",
    "\n",
    "                # unnormalize for visualization\n",
    "                x = x.cpu()\n",
    "                x_recon = x_recon.cpu()\n",
    "                x_unnormed = unnormalize(x, train_mean, train_std)\n",
    "                x_recon_unnormed = unnormalize(x_recon, train_mean, train_std)\n",
    "\n",
    "                # Log input-recon grid to TB\n",
    "                input_grid = torchvision.utils.make_grid(x_unnormed) # (C, gridh, gridw)\n",
    "                recon_grid = torchvision.utils.make_grid(x_recon_unnormed) # (C, gridh, gridw)\n",
    "                grid = torch.cat([input_grid, recon_grid], dim=-1) #inputs | recons\n",
    "                trainer.logger.experiment.add_image(f\"{mode}/recons\", grid, global_step=model.current_epoch)\n",
    "        model.train(is_training)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_recon_logger():\n",
    "    ReconLogger().log_recon(trainer, model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistogramLogger(pl.Callback):\n",
    "    \"\"\"\n",
    "    Distribution of the outputs of the encoder, upon the input of a mini-batch x\n",
    "    - eg. \n",
    "    \n",
    "    [batch_mu_z, batch_logvar_z] = model.encoder(x)\n",
    "    Then, plot all elements of batch_mu_z as a historgram. Same for batch_logvar_z\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        hist_epoch_interval: int = 20,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hist_epoch_interval = hist_epoch_interval\n",
    "    \n",
    "    def on_fit_start(self, *args, **kwargs):\n",
    "        print(f\"{self.__class__.__name__} is called\")\n",
    "    \n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        if (trainer.current_epoch + 1) % self.hist_epoch_interval == 0:\n",
    "            self.log_hist_of_params_of_q(trainer, pl_module)\n",
    "    \n",
    "    @classmethod\n",
    "    def log_hist_of_params_of_q(cls, trainer, model, **kwargs):\n",
    "        is_training = model.training\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for mode in ['train', 'val']:\n",
    "                dl = getattr(model, f\"{mode}_dataloader\")()\n",
    "                x,y = next(iter(dl))\n",
    "                x = x.to(model.device)\n",
    "\n",
    "                mu,log_var = model.encode(x)\n",
    "                var = log_var.exp()\n",
    "                var_thresh = kwargs.get('var_thresh', None) or 1e-3\n",
    "                n_tiny_vars = (var < var_thresh).sum()\n",
    "                p_tiny = n_tiny_vars/log_var.numel()\n",
    "\n",
    "                # Log the histograms to tensorboard\n",
    "                trainer.logger.experiment.add_histogram(f\"{mode}/centers of q\", mu, global_step=model.current_epoch)\n",
    "                trainer.logger.experiment.add_histogram(f\"{mode}/vars of q\", var, global_step=model.current_epoch)\n",
    "                trainer.logger.experiment.add_scalar(f\"{mode}/p_tiny_var\", p_tiny, global_step=model.current_epoch)\n",
    "        model.train(is_training)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HistogramLogger.log_hist_of_params_of_q(trainer,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Misc experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: Does `torch`'s `dtype` conversion (eg. my_tensor.to(torch.float64)) keeps the new tensor attached to the original tensor's computational graph?\n",
    "Related:\n",
    "- `is_leaf`\n",
    "- `requires_grad`\n",
    "- `retain_grad`: See [doc](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.is_leaf:~:text=Only%20leaf%20Tensors%20will%20have%20their,non%2Dleaf%20Tensors%2C%20you%20can%20use%20retain_grad().)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.ones(1, dtype=torch.float32, requires_grad=True)\n",
    "t2 = t.to(torch.float64)\n",
    "# t2.retain_grad()\n",
    "print(t.requires_grad, t2.requires_grad)\n",
    "print(t.is_leaf, t2.is_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = 2*t2**3\n",
    "out.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2.grad, t.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, yes, the gradient flows via the tensor generated from the original tensor (`t`) with `.to` operation. Therefore, we conclude the tensor generated from `.to` method remains attached to the orignal tensor's computational graph and acts as a medium (ie. a non-leaf node) through which downstream operation's gradient can flow through to be accumulated at the original tensor `t`'s `.grad` attribute. \n",
    "\n",
    "Unless I want to look at the `.grad` of the derived tensor (`t2`), I don't need to call `.retain_grad()` method on `t2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test]",
   "language": "python",
   "name": "conda-env-test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
