{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import re\n",
    "import math\n",
    "from datetime import datetime\n",
    "import time\n",
    "sys.dont_write_bytecode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Set, Dict, Tuple, Optional, Iterable, Mapping, Union, Callable\n",
    "\n",
    "from pprint import pprint\n",
    "from ipdb import set_trace as brpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from  torch.linalg import norm as tnorm\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "# Select Visible GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Path \n",
    "1. Add project root and src folders to `sys.path`\n",
    "2. Set DATA_ROOT to `maptile_v2` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_nb_path = Path(os.getcwd())\n",
    "ROOT = this_nb_path.parent\n",
    "SRC = ROOT/'src'\n",
    "DATA_ROOT = Path(\"/data/hayley-old/maptiles_v2/\")\n",
    "paths2add = [this_nb_path, ROOT]\n",
    "\n",
    "print(\"Project root: \", str(ROOT))\n",
    "print('Src folder: ', str(SRC))\n",
    "print(\"This nb path: \", str(this_nb_path))\n",
    "\n",
    "\n",
    "for p in paths2add:\n",
    "    if str(p) not in sys.path:\n",
    "        sys.path.insert(0, str(p))\n",
    "        print(f\"\\n{str(p)} added to the path.\")\n",
    "        \n",
    "# print(sys.path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.data.datasets.maptiles import Maptiles, MapStyles\n",
    "# from src.data.datamodules.mnist_datamodule import MNISTDataModule\n",
    "# from src.data.datamodules.maptiles_datamodule import MaptilesDataModule\n",
    "from src.data.datamodules.multisource_maptiles_datamodule import MultiMaptilesDataModule\n",
    "\n",
    "\n",
    "# from src.models.plmodules.three_fcs import ThreeFCs\n",
    "# from src.models.plmodules.vanilla_vae import VanillaVAE\n",
    "# from src.models.plmodules.beta_vae import BetaVAE\n",
    "from src.models.plmodules.bilatent_vae import BiVAE\n",
    "\n",
    "from src.visualize.utils import show_timgs\n",
    "from src.utils.misc import info, get_next_version_path, n_iter_per_epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start experiment \n",
    "Given a maptile, predict its style as one of OSM, CartoVoyager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate MNIST Datamodule\n",
    "# in_shape = (1,32,32)\n",
    "# batch_size = 32\n",
    "# dm = MNISTDataModule(data_root=ROOT/'data', \n",
    "#                        in_shape=in_shape,\n",
    "#                       batch_size=batch_size)\n",
    "# dm.setup('fit')\n",
    "# print(\"DM: \", dm.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Multisource Maptiles DataModule for OSMnxRoads\n",
    "data_root = Path(\"/data/hayley-old/osmnx_data/images\")\n",
    "\n",
    "# Collect all images from these cities\n",
    "all_cities = sorted(\n",
    "    ['la', 'charlotte', 'vegas', 'boston', 'paris', \\\n",
    "     'amsterdam', 'shanghai', 'seoul', 'chicago', 'manhattan', \\\n",
    "     'berlin', 'montreal', 'rome']#'london'\n",
    ") \n",
    "\n",
    "# Style parameters\n",
    "edge_color = 'cyan'\n",
    "lw_factor = 0.5\n",
    "bgcolors = ['r','g','b'] # ['k', 'r', 'g', 'b', 'y']\n",
    "    \n",
    "cities = ['paris'] #all_cities # ['berlin', 'rome', 'la', 'amsterdam', 'seoul'] #['paris']\n",
    "styles =[f'OSMnxR-{bgcolor}-{edge_color}-{lw_factor}' for bgcolor in bgcolors]#['StamenTonerBackground','OSMDefault', 'CartoVoyagerNoLabels']#'StamenWatercolor']#, 'StamenTonerLines']\n",
    "zooms = ['14']\n",
    "in_shape = (3, 64, 64)\n",
    "batch_size = 32\n",
    "print('cities: ', cities)\n",
    "print('styes: ', styles)\n",
    "\n",
    "dm = MultiMaptilesDataModule(\n",
    "    data_root=data_root,\n",
    "    cities=cities,\n",
    "    styles=styles,\n",
    "    zooms=zooms,\n",
    "    in_shape=in_shape,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "dm.setup('fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pickle this datamodule\n",
    "# import joblib\n",
    "# nb_name = '16-a'\n",
    "# joblib.dump(dm, ROOT/'cache'/f'dm_{nb_name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train size: ', len(dm.train_ds))\n",
    "# show a batch\n",
    "dl = dm.train_dataloader()\n",
    "batch = next(iter(dl))\n",
    "x, label_c, label_s = dm.unpack(batch)\n",
    "info(x)\n",
    "show_timgs(x, titles=label_s.tolist(), cmap='gray' if in_shape[0]==1 else None)\n",
    "print(label_c)\n",
    "print(label_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the pl Module\n",
    "from src.models.plmodules.bilatent_vae import BiVAE\n",
    "\n",
    "# betas = [0.1 * 3**i for i in range(10)]\n",
    "# for kld_weight in [1.0]\n",
    "n_styles = len(styles)\n",
    "latent_dim = 20\n",
    "hidden_dims = [32, 64, 128, 256, 512]\n",
    "adversary_dims = [32,32,32]\n",
    "act_fn = nn.LeakyReLU()\n",
    "learning_rate = 1e-3\n",
    "\n",
    "is_contrasive = True\n",
    "kld_weight = 1024.0 #1.0 # vae_loss = recon_loss + kld_weight * kld_weight; betas[0];\n",
    "adv_loss_weight = 45.#15. # loss = vae_loss + adv_loss_weight * adv_loss\n",
    "\n",
    "# enc_type = 'resnet'\n",
    "enc_type = 'conv'\n",
    "\n",
    "# dec_type = 'conv'\n",
    "dec_type = 'resnet'\n",
    "\n",
    "if enc_type == 'resnet':\n",
    "    hidden_dims = [32, 32, 64, 128, 256]\n",
    "\n",
    "model = BiVAE(\n",
    "    in_shape=in_shape, \n",
    "    n_styles=n_styles,\n",
    "    latent_dim=latent_dim,\n",
    "    hidden_dims=hidden_dims,\n",
    "    adversary_dims=adversary_dims,\n",
    "    learning_rate=learning_rate,\n",
    "    act_fn=act_fn,\n",
    "    is_contrasive=is_contrasive,\n",
    "    kld_weight=kld_weight,\n",
    "    adv_loss_weight=adv_loss_weight,\n",
    "    enc_type=enc_type,\n",
    "    dec_type=dec_type,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BiVAE-C-conv-resnet-1024.0-45.0'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: /data/hayley-old/Tenanbaum2000/temp-logs/BiVAE-C-conv-resnet-1024.0-45.0_Maptiles_paris_OSMnxR-b-cyan-0.5-OSMnxR-g-cyan-0.5-OSMnxR-r-cyan-0.5_14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log dir:  /data/hayley-old/Tenanbaum2000/temp-logs/BiVAE-C-conv-resnet-1024.0-45.0_Maptiles_paris_OSMnxR-b-cyan-0.5-OSMnxR-g-cyan-0.5-OSMnxR-r-cyan-0.5_14/version_0\n",
      "\n",
      "Created:  /data/hayley-old/Tenanbaum2000/temp-logs/BiVAE-C-conv-resnet-1024.0-45.0_Maptiles_paris_OSMnxR-b-cyan-0.5-OSMnxR-g-cyan-0.5-OSMnxR-r-cyan-0.5_14/version_0\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a PL `Trainer` object\n",
    "# Start the experiment\n",
    "max_epochs = 500\n",
    "exp_name = f'{model.name}_{dm.name}'\n",
    "tb_logger = pl_loggers.TensorBoardLogger(save_dir=f'{ROOT}/temp-logs', \n",
    "                                         name=exp_name,\n",
    "                                         log_graph=False,\n",
    "                                        default_hp_metric=False)\n",
    "print(\"Log dir: \", tb_logger.log_dir)\n",
    "\n",
    "log_dir = Path(tb_logger.log_dir)\n",
    "if not log_dir.exists():\n",
    "    log_dir.mkdir(parents=True)\n",
    "    print(\"\\nCreated: \", log_dir)\n",
    "    \n",
    "\n",
    "# Log computational graph\n",
    "# model_wrapper = ModelWrapper(model)\n",
    "# tb_logger.experiment.add_graph(model_wrapper, model.example_input_array.to(model.device))\n",
    "# tb_logger.log_graph(model)\n",
    "\n",
    "trainer_config = {\n",
    "    'gpus':1,\n",
    "    'max_epochs': max_epochs,\n",
    "    'progress_bar_refresh_rate':0,\n",
    "    'terminate_on_nan':True,\n",
    "    'check_val_every_n_epoch':10,\n",
    "    'logger':tb_logger,\n",
    "#     'callbacks':callbacks,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training is logged to (on May 18, 2021)\n",
    "\n",
    "- /data/hayley-old/Tenanbaum2000/temp-logs/BiVAE-C-conv-resnet-1.0-15.0_Maptiles_paris_OSMnxR-b-cyan-0.5-OSMnxR-g-cyan-0.5-OSMnxR-r-cyan-0.5_14/version_0\n",
    "\n",
    "\n",
    "- Log dir:  /data/hayley-old/Tenanbaum2000/temp-logs/BiVAE-C-conv-resnet-1.0-15.0_Maptiles_paris_OSMnxR-b-cyan-0.5-OSMnxR-g-cyan-0.5-OSMnxR-r-cyan-0.5_14/version_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]\n",
      "/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: RuntimeWarning: Found unsupported keys in the lr scheduler dict: ['name']\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "   | Name               | Type          | Params\n",
      "------------------------------------------------------\n",
      "0  | act_fn             | LeakyReLU     | 0     \n",
      "1  | out_fn             | Tanh          | 0     \n",
      "2  | encoder            | Sequential    | 1.6 M \n",
      "3  | fc_flatten2qparams | Linear        | 82.0 K\n",
      "4  | fc_latent2flatten  | Linear        | 43.0 K\n",
      "5  | decoder            | ResNetDecoder | 6.3 M \n",
      "6  | out_layer          | Sequential    | 84    \n",
      "7  | adversary          | Sequential    | 2.6 K \n",
      "8  | train_style_acc    | Accuracy      | 0     \n",
      "9  | val_style_acc      | Accuracy      | 0     \n",
      "10 | test_style_acc     | Accuracy      | 0     \n",
      "------------------------------------------------------\n",
      "8.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.0 M     Total params\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics:  dict_keys([])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 0, batch: 0\n",
      "Ep: 0, batch: 0\n",
      "Ep: 10, batch: 0\n",
      "Ep: 20, batch: 0\n",
      "Ep: 30, batch: 0\n",
      "Ep: 40, batch: 0\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Ep: 50, batch: 0\n",
      "Ep: 60, batch: 0\n",
      "Ep: 70, batch: 0\n",
      "Ep: 80, batch: 0\n",
      "Ep: 90, batch: 0\n",
      "Epoch    91: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Ep: 100, batch: 0\n",
      "Ep: 110, batch: 0\n",
      "Ep: 120, batch: 0\n",
      "Ep: 130, batch: 0\n",
      "Epoch   131: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Ep: 140, batch: 0\n",
      "Epoch   142: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Ep: 150, batch: 0\n",
      "Epoch   153: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Ep: 160, batch: 0\n",
      "Ep: 170, batch: 0\n",
      "Ep: 180, batch: 0\n",
      "Ep: 190, batch: 0\n",
      "Ep: 200, batch: 0\n",
      "Ep: 210, batch: 0\n",
      "Ep: 220, batch: 0\n",
      "Ep: 230, batch: 0\n",
      "Ep: 240, batch: 0\n",
      "Ep: 250, batch: 0\n",
      "Ep: 260, batch: 0\n",
      "Ep: 270, batch: 0\n",
      "Ep: 280, batch: 0\n",
      "Ep: 290, batch: 0\n",
      "Ep: 300, batch: 0\n",
      "Ep: 310, batch: 0\n",
      "Ep: 320, batch: 0\n",
      "Ep: 330, batch: 0\n",
      "Ep: 340, batch: 0\n",
      "Ep: 350, batch: 0\n",
      "Ep: 360, batch: 0\n",
      "Ep: 370, batch: 0\n",
      "Ep: 380, batch: 0\n",
      "Ep: 390, batch: 0\n",
      "Ep: 400, batch: 0\n",
      "Ep: 410, batch: 0\n",
      "Ep: 420, batch: 0\n",
      "Ep: 430, batch: 0\n",
      "Ep: 440, batch: 0\n",
      "Ep: 450, batch: 0\n",
      "Ep: 460, batch: 0\n",
      "Ep: 470, batch: 0\n",
      "Ep: 480, batch: 0\n",
      "Ep: 490, batch: 0\n",
      "Finished at ep (499, 3)\n"
     ]
    }
   ],
   "source": [
    "# trainer = pl.Trainer(fast_dev_run=3)\n",
    "trainer = pl.Trainer(**trainer_config)\n",
    "# trainer.tune(model=model, datamodule=dm)\n",
    "print(\"\\nMetrics: \", trainer.callback_metrics.keys())# todo: delete\n",
    "\n",
    "# Fit model\n",
    "trainer.fit(model, dm)\n",
    "print(f\"Finished at ep {trainer.current_epoch, trainer.batch_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(499,\n",
       " '/data/hayley-old/Tenanbaum2000/temp-logs/BiVAE-C-conv-resnet-1024.0-45.0_Maptiles_paris_OSMnxR-b-cyan-0.5-OSMnxR-g-cyan-0.5-OSMnxR-r-cyan-0.5_14/version_0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.current_epoch, model.logger.log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log  hparmeters and `best_score` to tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'act_fn': LeakyReLU(negative_slope=0.01),\n",
      " 'adv_loss_weight': 45.0,\n",
      " 'adversary_dims': [32, 32, 32],\n",
      " 'batch_size': 32,\n",
      " 'cities': ['paris'],\n",
      " 'dec_type': 'resnet',\n",
      " 'enc_type': 'conv',\n",
      " 'hidden_dims': [32, 64, 128, 256, 512],\n",
      " 'in_shape': (3, 64, 64),\n",
      " 'is_contrasive': True,\n",
      " 'kld_weight': 1024.0,\n",
      " 'latent_dim': 20,\n",
      " 'learning_rate': 0.001,\n",
      " 'n_contents': 1,\n",
      " 'n_styles': 3,\n",
      " 'out_fn': Tanh(),\n",
      " 'size_average': False,\n",
      " 'source_names': ['OSMnxR-b-cyan-0.5',\n",
      "                  'OSMnxR-g-cyan-0.5',\n",
      "                  'OSMnxR-r-cyan-0.5'],\n",
      " 'styles': ['OSMnxR-b-cyan-0.5', 'OSMnxR-g-cyan-0.5', 'OSMnxR-r-cyan-0.5'],\n",
      " 'zooms': ['14']}\n",
      "{'hparam/best_score': 339217.46875}\n"
     ]
    }
   ],
   "source": [
    "hparams = model.hparams.copy()\n",
    "hparams.update(dm.hparams)\n",
    "best_score = trainer.checkpoint_callback.best_model_score.item()\n",
    "metrics = {'hparam/best_score': best_score} #todo: define a metric and use it here\n",
    "pprint(hparams)\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pl.Logger's method \"log_hyperparameters\" which handles the \n",
    "# hparams' element's formats to be suitable for Tensorboard logging\n",
    "# See: \n",
    "# https://sourcegraph.com/github.com/PyTorchLightning/pytorch-lightning@be3e8701cebfc59bec97d0c7717bb5e52afc665e/-/blob/pytorch_lightning/loggers/tensorboard.py#explorer:~:text=def%20log_hyperparams\n",
    "best_score = trainer.checkpoint_callback.best_model_score.item()\n",
    "metrics = {'hparam/best_score': best_score} #todo: define a metric and use it here\n",
    "trainer.logger.log_hyperparams(hparams, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.plmodules.utils import get_best_ckpt, load_model, load_best_model\n",
    "from pytorch_lightning.utilities.cloud_io import load as pl_load\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load best model recorded during the training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = get_best_ckpt(model, verbose=True)\n",
    "ckpt = pl_load(ckpt_path, map_location=lambda storage, loc: storage)  # dict object\n",
    "print(ckpt['epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load bestmodel\n",
    "model.load_state_dict(ckpt['state_dict'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from src.visualize.utils import unnormalize\n",
    "def show_recon(model: BiVAE, \n",
    "               tb_writer: SummaryWriter=None,\n",
    "               global_step:int=0,\n",
    "               unnorm:bool=True, \n",
    "               to_show:bool=True, \n",
    "               verbose:bool=False):\n",
    "    model.eval()\n",
    "    dm = model.trainer.datamodule\n",
    "    cmap = 'gray' if dm.size()[0] ==1 else None\n",
    "    train_mean, train_std = dm.train_mean, dm.train_std\n",
    "    with torch.no_grad():\n",
    "        for mode in ['train', 'val']:\n",
    "            dl = getattr(model, f\"{mode}_dataloader\")()\n",
    "            batch = next(iter(dl))\n",
    "            \n",
    "            x = batch['img']\n",
    "#             label_c = batch['digit']  # digit/content label (int) -- currently not used\n",
    "#             label_s = batch['color']\n",
    "            x = x.to(model.device)\n",
    "            x_recon = model.generate(x)\n",
    "            \n",
    "            # Move to cpu for visualization\n",
    "            x = x.cpu()\n",
    "            x_recon = x_recon.cpu()\n",
    "            \n",
    "            if verbose: \n",
    "                info(x, f\"{mode}_x\")\n",
    "                info(x_recon, f\"{mode}_x_recon\")\n",
    "                \n",
    "            if unnorm:\n",
    "                x_unnormed = unnormalize(x, train_mean, train_std)\n",
    "                x_recon_unnormed = unnormalize(x_recon, train_mean, train_std)\n",
    "                if verbose:\n",
    "                    print(\"===After unnormalize===\")\n",
    "                    info(x_unnormed, f\"{mode}_x_unnormed\")\n",
    "                    info(x_recon_unnormed, f\"{mode}_x_recon_unnormed\")\n",
    "                    \n",
    "            if to_show:\n",
    "                _x = x_unnormed if unnorm else x\n",
    "                _x_recon = x_recon_unnormed if unnorm else x_recon\n",
    "                show_timgs(_x, title=f\"Input: {mode}\", cmap=cmap)\n",
    "#                 show_timgs(_x_recon, title=f\"Recon: {mode}\", cmap=cmap)\n",
    "                show_timgs(LinearRescaler()(_x_recon), title=f\"Recon(linearized): {mode}\", cmap=cmap)\n",
    "\n",
    "            # Log input-recon grid to TB\n",
    "            if tb_writer is not None:\n",
    "                input_grid = torchvision.utils.make_grid(x_unnormed) # (C, gridh, gridw)\n",
    "                recon_grid = torchvision.utils.make_grid(x_recon_unnormed) # (C, gridh, gridw)\n",
    "                normed_recon_grid = torchvision.utils.make_grid(LinearRescaler()(x_recon_unnormed))\n",
    "                \n",
    "                grid = torch.cat([input_grid, normed_recon_grid], dim=-1) #inputs | recons\n",
    "                tb_writer.add_image(f\"{mode}/recons\", grid, global_step=global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_recon(model, tb_logger.experiment, global_step=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test]",
   "language": "python",
   "name": "conda-env-test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
