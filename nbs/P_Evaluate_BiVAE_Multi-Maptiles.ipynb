{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate models trained on Multisource Maptiles datamodule\n",
    "Models can be:\n",
    "- beta_vae\n",
    "- BiVae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import re\n",
    "import math\n",
    "from datetime import datetime\n",
    "import time\n",
    "sys.dont_write_bytecode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Set, Dict, Tuple, Optional, Iterable, Mapping, Union, Callable, TypeVar\n",
    "\n",
    "from pprint import pprint\n",
    "from ipdb import set_trace as brpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from  torch.linalg import norm as tnorm\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.tuner.tuning import Tuner\n",
    "\n",
    "\n",
    "# Select Visible GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Path \n",
    "1. Add project root and src folders to `sys.path`\n",
    "2. Set DATA_ROOT to `maptile_v2` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_nb_path = Path(os.getcwd())\n",
    "ROOT = this_nb_path.parent\n",
    "SRC = ROOT/'src'\n",
    "DATA_ROOT = Path(\"/data/hayley-old/maptiles_v2/\")\n",
    "paths2add = [this_nb_path, ROOT]\n",
    "\n",
    "print(\"Project root: \", str(ROOT))\n",
    "print('Src folder: ', str(SRC))\n",
    "print(\"This nb path: \", str(this_nb_path))\n",
    "\n",
    "\n",
    "for p in paths2add:\n",
    "    if str(p) not in sys.path:\n",
    "        sys.path.insert(0, str(p))\n",
    "        print(f\"\\n{str(p)} added to the path.\")\n",
    "        \n",
    "# print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transforms\n",
    "from src.data.transforms.transforms import Identity, Unnormalizer, LinearRescaler\n",
    "from src.data.transforms.functional import unnormalize\n",
    "\n",
    "# Utils\n",
    "from src.visualize.utils import show_timg, show_timgs, show_batch, make_grid_from_tensors\n",
    "from src.utils.misc import info, get_next_version_path, get_ckpt_path\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataModules\n",
    "from src.data.datamodules import MNISTDataModule, MNISTMDataModule, MonoMNISTDataModule\n",
    "from src.data.datamodules import MultiMonoMNISTDataModule\n",
    "# from src.data.datamodules.maptiles_datamodule import MaptilesDataModule # for BetaVAE use this datamodule\n",
    "from src.data.datamodules.multisource_maptiles_datamodule import MultiMaptilesDataModule #for BiVAE\n",
    "\n",
    "\n",
    "\n",
    "# plModules\n",
    "from src.models.plmodules.vanilla_vae import VanillaVAE\n",
    "from src.models.plmodules.iwae import IWAE\n",
    "from src.models.plmodules.beta_vae import BetaVAE\n",
    "from src.models.plmodules.bilatent_vae import BiVAE\n",
    "\n",
    "# Evaluations\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pytorch_lightning.utilities.cloud_io import load as pl_load\n",
    "# from src.evaluator.qualitative import save_content_transfers, save_style_transfers, run_both_transfers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation NB to load and evaluate a trained model\n",
    "Steps:\n",
    "- Define the architecutre of the model to load\n",
    "- Load the model at `ckpt_path`\n",
    "- Run the following evaluations\n",
    "\n",
    "Evaluations:\n",
    "1. Evaluation of the generative model\n",
    "- Quantitative: `best_score`, which is the lowest loss computed as an average loss per datapt in the validation set. The loss is the estimate of the negative maginal log-likelihood of the observed data based on the trained model\n",
    "\n",
    "- Qualitative: \n",
    "  - Reconstruction of datapts from train/val datasets\n",
    "    - This evaluates how well the generative model (encoder-decoder) preserves the information needed to reconstruct the input data after having learned/trained/optimized jointly with/in the presence of its adversary, the style-classifier/discriminator\n",
    "\n",
    "2. Evaluation of the discriminator\n",
    "- How well does it discriminate? \n",
    "  - based on a style code: the model should predict the style label of the input datapt well\n",
    "    - Compute the `loss_s` over the train/val datasets (as an expectation, ie. loss value per datapt/image)\n",
    "  - based on a content code: the model should say \"I'm not sure, aka. all style labels seem equally probable\"\n",
    "    - Compute the `loss_s` over the train/val datasets (as an expectation, ie. loss value per datapt/image)\n",
    "  - Q: what is the range of the `loss_s` or `loss_c` for a good style-classifer?\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Init Multisource-Monochrome MNIST datamodule\n",
    "# mono_dir = ROOT/'data/Mono-MNIST'\n",
    "# colors = ['red', 'green', 'blue']\n",
    "# seed = 123\n",
    "# in_shape = (3,32,32)\n",
    "# batch_size = 128\n",
    "\n",
    "# # Create a multi-source dataset\n",
    "# dm = MultiMonoMNISTDataModule(\n",
    "#     data_root=mono_dir,\n",
    "#     colors=colors,\n",
    "#     seed=seed,\n",
    "#     in_shape=in_shape,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=True,\n",
    "# )\n",
    "# dm.setup('fit')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Multisource Maptiles DataModule\n",
    "all_cities = ['la', 'charlotte', 'vegas', 'boston', 'paris', \\\n",
    "              'amsterdam', 'shanghai', 'seoul', 'chicago', 'manhattan', \\\n",
    "             'berlin', 'montreal', 'rome']\n",
    "\n",
    "data_root = Path(\"/data/hayley-old/maptiles_v2/\")\n",
    "cities = all_cities # ['berlin', 'rome', 'la', 'amsterdam', 'seoul'] #['paris']\n",
    "styles = ['StamenTonerBackground'] #'OSMDefault', 'CartoVoyagerNoLabels']#'StamenWatercolor']#, 'StamenTonerLines']\n",
    "zooms = ['14']\n",
    "in_shape = (1, 128, 128)\n",
    "batch_size = 32\n",
    "print('cities: ', cities)\n",
    "print('styes: ', styles)\n",
    "dm = MultiMaptilesDataModule(\n",
    "    data_root=data_root,\n",
    "    cities=cities,\n",
    "    styles=styles,\n",
    "    zooms=zooms,\n",
    "    in_shape=in_shape,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "dm.setup('fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: Mar 1, 2021\n",
    "# # NEED TO REVIEW THIS -- I must've been working on adding city name as \n",
    "# # a label in tensorboard\n",
    "\n",
    "# # Instantiate Maptiles data module\n",
    "# # Note: We use a different target_transform for the evaluation\n",
    "# # During training, we used a default target_transform which set the label returned in (x,label) = ds[i]\n",
    "# # is a style index. In this anaylsis with beta-vae on Maptiles, since beta-vae is about a single source dataset,\n",
    "# # we don't use the style label at all, neither for the training, nor for the testing.\n",
    "# # Instead, we are insterested in the (lack of) relation between the embedded latent code (z)'s similarities \n",
    "# # and (lack of ) relations to the physical proximities of the maptiles.\n",
    "# # So, here, when we create a dataset (with the same maptiles and orders as in the training), \n",
    "# # we retrieve the information about lat-long-zoom from the recorded maptile metadata\n",
    "# # __getitem__ returns label_dict = {\n",
    "#         #    \"city\": city,\n",
    "#         #    \"style\": style,\n",
    "#         #    \"zoom\": zoom,\n",
    "#         #    \"coord\": coord}\n",
    "# # Then, if target_transform is specified (ie. not None), then it will be applied to the label_dict\n",
    "# # before returning as the target_label\n",
    "\n",
    "# all_cities = ['la', 'charlotte', 'vegas', 'boston', 'paris', \\\n",
    "#               'amsterdam', 'shanghai', 'seoul', 'chicago', 'manhattan', \\\n",
    "#              'berlin', 'montreal', 'rome']\n",
    "# cities = all_cities #['berlin']#['paris']\n",
    "# styles = ['StamenTonerBackground']#['OSMDefault', 'CartoVoyagerNoLabels']\n",
    "# zooms = ['14']\n",
    "# in_shape = (1, 64, 64)\n",
    "# batch_size = 32\n",
    "\n",
    "# city2idx = {c:i for i,c in enumerate(cities)}\n",
    "# idx2city = {i:c for i,c in enumerate(cities)} \n",
    "\n",
    "# target_transform = transforms.Lambda(\n",
    "#                     lambda label_dict: city2idx[label_dict[\"city\"]] #todo: try with \"coord\" for more granular distance comparison using lat-lon\n",
    "#                 )\n",
    "# dm = MaptilesDataModule(data_root=DATA_ROOT,\n",
    "#                         cities=cities,\n",
    "#                         styles=styles,\n",
    "#                         zooms=zooms,\n",
    "#                        in_shape=in_shape,\n",
    "#                        batch_size=batch_size,\n",
    "#                         target_transform=target_transform,\n",
    "#                        )\n",
    "# dm.setup('fit')\n",
    "# print(\"DM: \", dm.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = dm.train_ds\n",
    "x, *meta = dm.unpack(dset[0])\n",
    "print('x_shape: ', x.shape)\n",
    "\n",
    "coord_str, label_s = meta\n",
    "print('coord_str: ', coord_str)\n",
    "print('label_s: ', label_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = dm.train_dataloader()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_model_class(model_name: str) -> object:\n",
    "#     model_name = model_name.lower()\n",
    "#     return {\n",
    "#         \"three_fcs\": ThreeFCs,\n",
    "#         \"vae\": VanillaVAE,\n",
    "#         \"iwae\": IWAE,\n",
    "#         \"bivae\": BiVAE,\n",
    "\n",
    "#     }[model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate BetaVAE\n",
    "# # BetaVAE-resnet-conv-1.000'\n",
    "\n",
    "# from src.models.plmodules.beta_vae import BetaVAE\n",
    "\n",
    "# # betas = [0.1 * 3**i for i in range(10)]\n",
    "# # for kld_weight in [1.0]\n",
    "# latent_dim = 10\n",
    "# hidden_dims = [32, 64, 128, 256] #,512]\n",
    "# act_fn = nn.LeakyReLU()\n",
    "# learning_rate = 3e-4\n",
    "# kld_weight = 1.0 #betas[0]\n",
    "# enc_type = 'resnet'\n",
    "# dec_type = 'conv'\n",
    "# # dec_type = 'resnet'\n",
    "\n",
    "# if enc_type == 'resnet':\n",
    "#     hidden_dims = [32, 32, 64, 128, 256, 512]\n",
    "\n",
    "# model = BetaVAE(\n",
    "#     in_shape=in_shape, \n",
    "#     latent_dim=latent_dim,\n",
    "#     hidden_dims=hidden_dims,\n",
    "#     learning_rate=learning_rate,\n",
    "#     act_fn=act_fn,\n",
    "#     kld_weight=kld_weight,\n",
    "#     enc_type=enc_type,\n",
    "#     dec_type=dec_type,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Init BiVAE\n",
    "from src.models.plmodules.bilatent_vae import BiVAE\n",
    "\n",
    "# betas = [0.1 * 3**i for i in range(10)]\n",
    "# for kld_weight in [1.0]\n",
    "n_styles = len(styles)\n",
    "latent_dim = 10\n",
    "hidden_dims = [32, 64, 128, 256] #,512]\n",
    "adversary_dims = [100,100,100]\n",
    "act_fn = nn.LeakyReLU()\n",
    "learning_rate = 3e-4\n",
    "\n",
    "is_contrasive = True\n",
    "kld_weight = 1.0 #betas[0]\n",
    "adv_loss_weight = 1.0\n",
    "\n",
    "enc_type = 'resnet'\n",
    "# dec_type = 'conv'\n",
    "dec_type = 'resnet'\n",
    "\n",
    "if enc_type == 'resnet':\n",
    "    hidden_dims = [32, 32, 64, 128, 256]\n",
    "\n",
    "model = BiVAE(\n",
    "    in_shape=in_shape, \n",
    "    n_styles=n_styles,\n",
    "    latent_dim=latent_dim,\n",
    "    hidden_dims=hidden_dims,\n",
    "    adversary_dims=adversary_dims,\n",
    "    learning_rate=learning_rate,\n",
    "    act_fn=act_fn,\n",
    "    is_contrasive=is_contrasive,\n",
    "    kld_weight=kld_weight,\n",
    "    adv_loss_weight=adv_loss_weight,\n",
    "    enc_type=enc_type,\n",
    "    dec_type=dec_type,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_dir_root = Path(\"/data/hayley-old/Tenanbaum2000/lightning_logs\")\n",
    "# log_dir_root = Path(\"/data/hayley-old/Tenanbaum2000/temp_logs\")\n",
    "log_dir_root = Path(\"/data/hayley-old/Tenanbaum2000/temp-logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Analyzing if we can get a better reconstruction on Maptiles\n",
    "  - 2021-02-08 (M)\n",
    "\n",
    "All models were trained with the following same params:\n",
    "- lr=3e-4\n",
    "- batch_size=32;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # beta=1.0l lr=3e-4; batch_size=32;\n",
    "# # Model 1: enc,dec = resnet-conv\n",
    "# log_dir = log_dir_root/ (\"BetaVAE-resnet-conv-1.000_\"\n",
    "#                          \"Maptiles_la-charlotte-vegas-boston-paris-amsterdam-shanghai-seoul-chicago-manhattan-berlin-montreal-rome_\"\n",
    "#                          \"StamenTonerBackground_14/version_0\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model 2: enc,dec = res-conv; beta=10.\n",
    "# log_dir = log_dir_root/ (\"BetaVAE-resnet-conv-10.000_\"\n",
    "#                          \"Maptiles_la-charlotte-vegas-boston-paris-amsterdam-shanghai-seoul-chicago-manhattan-berlin-montreal-rome_\"\n",
    "#                          \"StamenTonerBackground_14/version_0\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model 1: enc,dec = \n",
    "# log_dir = log_dir_root/ (\"BetaVAE-resnet-conv-1.000_\"\n",
    "#                          \"Maptiles_la-charlotte-vegas-boston-paris-amsterdam-shanghai-seoul-chicago-manhattan-berlin-montreal-rome_\"\n",
    "#                          \"StamenTonerBackground_14/version_0\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model 1: enc,dec \n",
    "# log_dir = log_dir_root/ (\"BetaVAE-resnet-conv-1.000_\"\n",
    "#                          \"Maptiles_la-charlotte-vegas-boston-paris-amsterdam-shanghai-seoul-chicago-manhattan-berlin-montreal-rome_\"\n",
    "#                          \"StamenTonerBackground_14/version_0\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BiVAE-resnet-resnet: single style maptiles\n",
    "# -- Analysis: started on Mar 4, 2021\n",
    "log_dir = log_dir_root/(\"BiVAE-C-resnet-resnet-1.0-1.0_\"\n",
    "                        \"Maptiles_la-charlotte-vegas-boston-paris-amsterdam-shanghai-seoul-chicago-manhattan-berlin-montreal-rome_\"\n",
    "                        \"StamenTonerBackground_14\"\n",
    "                        \"/version_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Init BiVAE\n",
    "from src.models.plmodules.bilatent_vae import BiVAE\n",
    "\n",
    "# betas = [0.1 * 3**i for i in range(10)]\n",
    "# for kld_weight in [1.0]\n",
    "n_styles = len(styles)\n",
    "latent_dim = 10\n",
    "hidden_dims = [32, 64, 128, 256] #,512]\n",
    "adversary_dims = [100,100,100]\n",
    "act_fn = nn.LeakyReLU()\n",
    "learning_rate = 3e-4\n",
    "\n",
    "is_contrasive = True\n",
    "kld_weight = 1.0 #betas[0]\n",
    "adv_loss_weight = 1.0\n",
    "\n",
    "enc_type = 'resnet'\n",
    "# dec_type = 'conv'\n",
    "dec_type = 'resnet'\n",
    "\n",
    "if enc_type == 'resnet':\n",
    "    hidden_dims = [32, 32, 64, 128, 256]\n",
    "\n",
    "model = BiVAE(\n",
    "    in_shape=in_shape, \n",
    "    n_styles=n_styles,\n",
    "    latent_dim=latent_dim,\n",
    "    hidden_dims=hidden_dims,\n",
    "    adversary_dims=adversary_dims,\n",
    "    learning_rate=learning_rate,\n",
    "    act_fn=act_fn,\n",
    "    is_contrasive=is_contrasive,\n",
    "    kld_weight=kld_weight,\n",
    "    adv_loss_weight=adv_loss_weight,\n",
    "    enc_type=enc_type,\n",
    "    dec_type=dec_type,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get path to checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = get_ckpt_path(log_dir)\n",
    "ckpt = pl_load(ckpt_path, map_location=lambda storage, loc: storage)  # dict object\n",
    "print(ckpt.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model state from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(ckpt['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a TB writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the root log_dir correpsonding to the ckpt_path\n",
    "log_dir = ckpt_path.parent.parent # eg. Folder called `temp-logs/f{model.name+dm.name}/version7`\n",
    "tb_writer = SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_writer.log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recons of inputs from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluator.qualitative import show_recon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_global_step = ckpt['global_step']\n",
    "best_global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_recon(\n",
    "    model=model, \n",
    "    dm=dm, \n",
    "    tb_writer=tb_writer, \n",
    "    global_step=best_global_step, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "- Test src.evaluator.qualitative.evaluate_transfers\n",
    "  - when constant_code is 'c' (ie. content transfers)\n",
    "  - when 's' is contant (ie. style transfers)\n",
    "  \n",
    "- The evaulation script on the top 3 trained BiVAE from yesterday\n",
    "- Run more training?\n",
    "- OSMNX - compare embedding\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_samples(dm: pl.LightningDataModule, \n",
    "                     inds: Optional[List]=None,\n",
    "                     n_reps: int = 10) -> Dict[Union[str,int], torch.Tensor]:\n",
    "    \"\"\"Get `n_reps` number of maptiles from random locations, random style(s)\"\"\"\n",
    "    ds = dm.train_dataloader().dataset\n",
    "    data = {}\n",
    "    if inds is None:\n",
    "        inds = np.random.choice(len(ds), n_reps)\n",
    "    for ind in inds:\n",
    "        x, y, d = dm.unpack(ds[ind]) #(img, coord_str, style_idx) for MultiSourceMaptiles\n",
    "        data[(d)] = (x,y)\n",
    "#         data[str(i)] = x\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reps = get_data_samples(dm, n_reps=10)\n",
    "# Show content-representative images\n",
    "for d, (timg, y) in data_reps.items():\n",
    "    show_timg(timg, title=f\"coord: {y}, style: {d}\")#, cmap='gray' if in_shape[0]==1 else None)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# def get_style_reps(dl: DataLoader, n_reps = 10) -> Dict[Union[str,int], torch.Tensor]:\n",
    "#     \"\"\"Get `n_reps` number of maptiles from random locations, random style(s)\"\"\"\n",
    "#     ds = dl.dataset\n",
    "# #     n_styles = \n",
    "#     reps = {}\n",
    "#     inds = np.random.choice(len(ds), n_reps)\n",
    "#     for i in inds:\n",
    "#         x, label_s = ds[i] #(img, style_id) for Maptiles in maptiles.py\n",
    "#         reps[str(i)] = x\n",
    "#     return reps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper: Get country name from a coord_str\n",
    "- coord_str: format is {{lat_deg}}-{{lng_deg}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "def getGeoFromTile(x, y, zoom):\n",
    "    lon_deg = x / (2.0 ** zoom) * 360.0 - 180.0\n",
    "    lat_rad = math.atan(math.sinh(math.pi * (1 - 2 * y / (2.0 ** zoom))))\n",
    "    lat_deg = lat_rad * (180.0 / math.pi)\n",
    "    return lat_deg, lon_deg\n",
    "\n",
    "def getCountryFromTile(x,y,z, \n",
    "                       language='en',\n",
    "                       reverse_zoom=3) -> str:\n",
    "    \"\"\"Given x,y,z tile coords, return Country name.\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "    x,y,z: tile x,y,z\n",
    "    language: to be used to express the output of OSM's Nominatim service\n",
    "    zoom: to be passed to Norminatim.reverse -- level of returned address's detail\n",
    "        - Eg: 3 for country-level, 10 for city, 16 for major streets\n",
    "        - See more: https://nominatim.org/release-docs/latest/api/Reverse/\n",
    "    \"\"\"\n",
    "    lat_deg, lng_deg = getGeoFromTile(x,y,z)\n",
    "    geolocator = Nominatim(user_agent=\"temp\")\n",
    "    location = geolocator.reverse(f\"{lat_deg}, {lng_deg}\", language=language, zoom=reverse_zoom)\n",
    "    if location is None:\n",
    "        location = geolocator.reverse(f\"{lat_deg}, {lng_deg}\", language=language, zoom=10)\n",
    "    addr = location.address.split(\" \")\n",
    "    return addr[-1]\n",
    "\n",
    "def test_getCountryFromTile():\n",
    "    print(\"shanghai: \", \n",
    "          getCountryFromTile(13703, 6671, 14,\n",
    "                            language='en',\n",
    "                            reverse_zoom=3),\n",
    "          \n",
    "         )\n",
    "    print(\"paris: \", getCountryFromTile(8301, 5639, 14))\n",
    "test_getCountryFromTile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coord2xyz(coord_str:str, \n",
    "              delimiter='-',\n",
    "              z=14) -> Tuple[int]:\n",
    "    lat_deg, lng_deg = list(map(int, coord_str.split(delimiter)))\n",
    "    return (lat_deg, lng_deg, z)\n",
    "\n",
    "def coord2country(coord_str: str,\n",
    "                 delimiter='-', \n",
    "                 z: int = 14) -> str:\n",
    "    tile_xyz = coord2xyz(coord_str, delimiter, z)\n",
    "    return getCountryFromTile(*tile_xyz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis on the latent space\n",
    "1. Visualize embeddings\n",
    "    - collect a batch of inputs -> encoder -> [mu, log_var] -> sample -> a batch of z's (embeddings)\n",
    "    - use tb logger\n",
    "  \n",
    "2. Nearest neighbor query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize embeddings\n",
    "- collect a batch of inputs -> encoder -> [mu, log_var] -> sample -> a batch of z's (embeddings)\n",
    "- use tb logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "best_global_step = ckpt[\"global_step\"] + 1\n",
    "mode = 'train'\n",
    "ds = getattr(dm, f\"{mode}_ds\")\n",
    "query_size = len(ds) #4096#2048#1024\n",
    "dl = DataLoader(ds, \n",
    "                batch_size=query_size,\n",
    "                num_workers=16,\n",
    "                pin_memory=True)\n",
    "\n",
    "# define embedding/code types to use as the embedding vector\n",
    "emb_types = ['c','s'] #'z'\n",
    "with torch.no_grad():\n",
    "    batch = next(iter(dl))\n",
    "    x, label_c, label_s = dm.unpack(batch)\n",
    "    if not isinstance(label_c, np.ndarray):\n",
    "        label_c = np.array(label_c)\n",
    "    if not isinstance(label_s, torch.Tensor):\n",
    "        label_s = torch.tensor(label_s)\n",
    "    dict_emb = model.get_embedding(x)\n",
    "    # todo: update dict_emb with z = [c,s] -- if interested in looking at the emb based on the whole latent z\n",
    "    # z = model.combine(c,s)\n",
    "    # dict_emb[\"z\"] = z\n",
    "\n",
    "#     for name, embedding in zip([\"z\"], [z]):\n",
    "    for emb_type in emb_types:\n",
    "        embedding = dict_emb[emb_type]\n",
    "        # Compute pairwise distance of the embeddings\n",
    "\n",
    "        \n",
    "        # log embedding of z_c to tensorboard \n",
    "        tb_writer.add_embedding(embedding,\n",
    "                             label_img=LinearRescaler()(x), \n",
    "#                              metadata=[idx2city[city_id.item()] for city_id in label_s],\n",
    "                             global_step=best_global_step, #todo\n",
    "                             tag=emb_type\n",
    "                            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize original images of the close neighbors in the latent space\n",
    "- Compute pairwise distance using cosine similarity\n",
    "- For each row (ie. a latent code), get the index of the smallest values. \n",
    "- Select the images in the batch x and visualize (can do this all in show_timgs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "mode = 'train'\n",
    "\n",
    "ds = getattr(dm, f\"{mode}_ds\")\n",
    "query_size = len(ds) #4096#2048#1024\n",
    "dl = DataLoader(ds, batch_size=query_size, shuffle=True)\n",
    "\n",
    "max_n_query = 64 # Max number of queries for Nearest Neighbors\n",
    "metric = 'cosine' #pairwise distance metric in content space\n",
    "\n",
    "# tsne params\n",
    "tsne_dim = 2\n",
    "tsne_p = 5. #10 #perplexity\n",
    "# tsne_metric = 'euclidean'\n",
    "tsne_metric = 'cosine' \n",
    "tsne = TSNE(n_components=tsne_dim, metric=tsne_metric, perplexity=tsne_p )\n",
    "emb_types = ['c']\n",
    "with torch.no_grad():\n",
    "    batch = next(iter(dl))\n",
    "    x, label_c, label_s = dm.unpack(batch)\n",
    "    if not isinstance(label_c, np.ndarray):\n",
    "        label_c = np.array(label_c)\n",
    "    dict_emb = model.get_embedding(x)\n",
    "    # todo: update dict_emb with z = [c,s] -- if interested in looking at the emb based on the whole latent z\n",
    "    # z = model.combine(c,s)\n",
    "    # dict_emb[\"z\"] = z\n",
    "\n",
    "#     for name, embedding in zip([\"z\"], [z]):\n",
    "    for emb_type in emb_types:\n",
    "        embedding = dict_emb[emb_type]\n",
    "        # Compute pairwise distance of the embeddings\n",
    "        pdists = pairwise_distances(embedding.numpy(), metric=metric)\n",
    "        plt.imshow(pdists, cmap='gray')\n",
    "        plt.title(f\"Pairwise dists of {emb_type}'s\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        # 1. show the 2dim view on the codes\n",
    "        embedding_2d = tsne.fit_transform(embedding)\n",
    "        # -- plot the 2dim embeddings and color-code by style-id\n",
    "        f, ax = plt.subplots(1, figsize=(20,10))        \n",
    "        ax.scatter(embedding_2d[:,0], embedding_2d[:,1], c=label_s),\n",
    "        ax.set_title(f\"Code: {emb_type}, colored by style-id\")\n",
    "        ax.axis('equal')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        # 2. Nearest neighbor queries\n",
    "        # smaller values means closer in distance\n",
    "        n_ngbrs = 10\n",
    "        n_rows = min(query_size, max_n_query)\n",
    "        selected_rows = np.random.choice(len(x), size=n_rows)\n",
    "        for idx in selected_rows:\n",
    "            args = np.argsort(pdists[idx])[:n_ngbrs]; #print(args)\n",
    "            country_names = list(map(coord2country, label_c[args]))\n",
    "            show_timgs(LinearRescaler()(x[args]), \n",
    "                       cmap='gray', \n",
    "                       factor=2, \n",
    "                       nrows=1, \n",
    "                       titles=country_names, #cities_of_nn\n",
    "#                        title=f'Nearest Maptile, based on {emb_type}',\n",
    "#                        title=f'Nearest of digit {label_s[idx].item()}: {emb_type}'\n",
    "                      )\n",
    "            plt.show()\n",
    "        \n",
    "        # 3. Add the embedding of this code to Tensorboard Projector\n",
    "        # Create a list of (country, style_idx) for all data \n",
    "        meta = []\n",
    "        for coord_str, style_idx in zip(label_c, label_s):\n",
    "            country = coord2country(coord_str)\n",
    "            meta.append((country, style_idx))\n",
    "\n",
    "        tb_writer.add_embedding(embedding,\n",
    "                             label_img=LinearRescaler()(x), \n",
    "                             metadata=meta,\n",
    "                             global_step=best_global_step, #todo\n",
    "                             tag=emb_type\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mar 1, 2021 -- pause 7pm\n",
    "Mar 4, 2021 -- reumed at 2:30pm\n",
    "TODO\n",
    "- [ ] Is there evidence that the emb of content code is ignorant of geolocation (ie. city), as well as modality of source?\n",
    "  - this will show us that our spatial similarity considers not physical location but rather more unintuitive, but meaningful geospatial features\n",
    "  \n",
    "- [ ] Save the results to OneNote\n",
    "- [ ] Test other BiVAE (with different enc,dec types) on this embedding evaluation \n",
    "- [ ] Put into OneNote -- for each model setting, add one para summary about the analysis\n",
    "\n",
    "---\n",
    "- [ ] try tsne perplexity = higher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the min,max range of the latent space (for each dimension of the latent space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from torch import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "mode = 'train'\n",
    "metric = 'cosine' #pairwise distance metric in content space\n",
    "ds = getattr(dm, f\"{mode}_ds\")\n",
    "query_size = 1024 #len(ds) #128 #1024\n",
    "dl = DataLoader(ds, batch_size=query_size, shuffle=True)\n",
    "\n",
    "\n",
    "# tsne params\n",
    "tsne_dim = 2\n",
    "tsne_p = 5. #10 #perplexity\n",
    "# tsne_metric = 'euclidean'\n",
    "tsne_metric = 'cosine' \n",
    "tsne = TSNE(n_components=tsne_dim, metric=tsne_metric, perplexity=tsne_p )\n",
    "\n",
    "emb_type = 'c'\n",
    "with torch.no_grad():\n",
    "    batch = next(iter(dl))\n",
    "    x, label_c, label_s = dm.unpack(batch)\n",
    "    if not isinstance(label_c, np.ndarray):\n",
    "        label_c = np.array(label_c)\n",
    "    if isinstance(label_s, torch.Tensor):\n",
    "        label_s = label_s.numpy()\n",
    "    dict_q_params = model.encode(x)\n",
    "    mu_qc, logvar_qc = dict_q_params[\"mu_qc\"], dict_q_params[\"logvar_qc\"]\n",
    "    \n",
    "    # mean norm of code\n",
    "#     z = dict_emb[\"z\"] # (BS, dim_z)\n",
    "#     norm_z = LA.norm(z, dim=-1)\n",
    "#     print(\"Avg. norm of z: \", norm_z.mean())\n",
    "    \n",
    "    mu_qc_min = mu_qc.min(dim=0).values\n",
    "    mu_qc_max = mu_qc.max(dim=0).values\n",
    "    print(\"Min mu_qz's range: \", mu_qc_min)\n",
    "    print(\"Max mu_qz's range: \", mu_qc_max)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Space Traversal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear traversal in each dimension\n",
    "\n",
    "For each content-id's representative code:\n",
    "  - Fix a style code representative\n",
    "  - Traverse for each dim of the content code\n",
    "  - Put the results in a grid where each colm is the dimension of the content and each row shows the traversed content code's reconstruction result (in combination with the fixed style code representative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluator.qualitative import get_traversals#, run_content_traversal, run_style_traversal\n",
    "from src.utils.misc import now2str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_latent_traversal(\n",
    "    model: pl.LightningModule,\n",
    "    code: torch.Tensor,\n",
    "    traversal_start: Union[float, Iterable[float]],\n",
    "    traversal_end: Union[float, Iterable[float]],\n",
    "    n_traversals: int,\n",
    "    show: bool = True,\n",
    "    title: str = '',\n",
    "    to_save: bool = True,\n",
    "    out_path: Optional[Path]=None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Given a latent code z, traverse each dimension  $j$ independently, \n",
    "    from `traversal_start[j]` to `traversal_end[j]` at `n_traversals` steps. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        model : pl.LightningModule; Not a BiVAE; Eg. beta_vae\n",
    "        code : torch.Tensor; shape (dim_code, ); A single latent code, thus 1Dim vector\n",
    "\n",
    "        traversal_start : Iterable; length == dim_content\n",
    "            a vector of floats that indicate the starting point of the traversal\n",
    "            for each dimsion of the content code\n",
    "        traversal_end : Iterable; length == dim_content\n",
    "            a vector of floats that indicate the ending point of the traversal\n",
    "            for each dimsion of the content code\n",
    "        n_traversals : int\n",
    "            how many traversal steps per dimension\n",
    "        show : bool\n",
    "            True to show the result in a grid where $j$th col is the content dim $j$, \n",
    "            and the $i$th row shows the $i$th step in that direction.\n",
    "        title : str\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        torch.Tensor; shape (`n_traversals`, dim_content, *dim_input_x)\n",
    "            a batch of reconstructions from each dimension's traversals.\n",
    "            Eg: output[n][j] contains a (C,H,W) image reconstructed by a $n$th \n",
    "            step at content_code's dim jth direction with the fixed style code.\n",
    "    \"\"\"\n",
    "    is_training = model.training\n",
    "    model.eval()\n",
    "    code_dim = code.shape[-1]\n",
    "    try:\n",
    "        traversal_start[0]\n",
    "    except TypeError:\n",
    "        traversal_start = torch.zeros(code_dim).fill_(traversal_start)\n",
    "    try:\n",
    "        traversal_end[0]\n",
    "    except TypeError:\n",
    "        traversal_end = torch.zeros(code_dim).fill_(traversal_end)\n",
    "    with torch.no_grad():\n",
    "        # Traverse for each dim\n",
    "        grids = [] #k,v = dim_i, batch of recons while traversing in dim_i direction (n_traversals, *dim_x)\n",
    "        for dim_i in range(code_dim):\n",
    "            min_dim_i = traversal_start[dim_i]\n",
    "            max_dim_i = traversal_end[dim_i]\n",
    "#             print(min_dim_i, max_dim_i)\n",
    "            c_traversals = get_traversals(code, dim_i, min_dim_i, max_dim_i, n_traversals) # (n_samples, dim_code)\n",
    "            \n",
    "            # Pass to the decoder\n",
    "            recons = model.decode(c_traversals)\n",
    "\n",
    "            grid = torchvision.utils.make_grid(LinearRescaler()(recons), nrow=1) # Caveat: nrow is num of colms! # here: linearlize is applied\n",
    "            grids.append(grid)\n",
    "        grids = torch.cat(grids, dim=2)\n",
    "\n",
    "        if show:\n",
    "            show_timg(grids, title=title)\n",
    "        if to_save:\n",
    "            out_path = out_path or Path(f'./z_traversal_{now2str()}.png')\n",
    "            torchvision.utils.save_image(grids, out_path)\n",
    "        model.train(is_training)\n",
    "#         return recons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run latent traversals for each code in the randomly sampled input x's\n",
    "# TODO: Get style_reps\n",
    "n_reps = 20\n",
    "n_traversals = 20\n",
    "out_dir = log_dir/\"latent_traversals\"\n",
    "run_name = now2str()\n",
    "to_save = True\n",
    "show = False\n",
    "if not out_dir.exists():\n",
    "    out_dir.mkdir(parents=True)\n",
    "    print(\"Created: \", out_dir)\n",
    "\n",
    "# Here content_id is maptile index for __getitem__ to Maptiles Dataset\n",
    "with torch.no_grad():\n",
    "\n",
    "    data_reps = get_data_samples(dm.train_dataloader(), n_reps=n_reps)\n",
    "    for data_id, x in data_reps.items():\n",
    "        dict_emb = model.get_embedding(x[None]) # single datapoint\n",
    "        z = dict_emb[\"z\"][0].detach() # (dim_z,)\n",
    "        z_recon = model.decode(z[None])\n",
    "        x_and_recon = torch.cat([x[None], z_recon])\n",
    "        if to_save:\n",
    "            torchvision.utils.save_image(LinearRescaler()(x_and_recon), out_dir/f\"z_traversals_tile-{data_id}_{run_name}_original.png\")\n",
    "\n",
    "        # Traverse for each dim\n",
    "        title = f\"Latent Traversal: Tile {data_id}\"\n",
    "        traversal_start = mu_qz_min\n",
    "        traversal_end = mu_qz_max\n",
    "        run_latent_traversal(model, z, \n",
    "                              traversal_start=traversal_start,\n",
    "                             traversal_end=traversal_end,\n",
    "                             n_traversals=n_traversals,\n",
    "                            show=show, \n",
    "                            title=title,\n",
    "                             to_save=to_save,\n",
    "                             out_path=out_dir/f\"z_traversals_tile-{data_id}_{run_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "TO CONTINUE HERE FOR ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General traversal on an input data point\n",
    "Unlike #1 above where we traversed per content-id code, we do a general traversal on the whole content space, where we used the content/style representative codes, we traverse on the learned latent dimension.\n",
    "\n",
    "input x -> mu_c, logvar_c\n",
    "        -> mu_s, logvar_s -> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "#### Latent content space -- Conditioned on the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "mode = 'train'\n",
    "ds = getattr(dm, f\"{mode}_ds\")\n",
    "query_size = 1\n",
    "dl = DataLoader(ds, batch_size=query_size, shuffle=True, \n",
    "                num_workers=16, pin_memory=True)\n",
    "\n",
    "\n",
    "n_samples = 10\n",
    "for i in range(n_samples):\n",
    "    # Encode x to c,s\n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(dl))\n",
    "        x = batch['img']\n",
    "        label_c = batch['digit']  \n",
    "        label_s = batch['color']\n",
    "\n",
    "        dict_qparams = model.encode(x)\n",
    "        dict_z = model.rsample(dict_qparams)\n",
    "        c = dict_z['c'][0]\n",
    "        s = dict_z['s'][0]\n",
    "\n",
    "\n",
    "    # Content Traversal\n",
    "    title = f\"Content Traversal of {mode} datapt\"\n",
    "    out_dir = log_dir/\"content_traversals_of_input\"\n",
    "    to_save = True\n",
    "    if not out_dir.exists():\n",
    "        out_dir.mkdir(parents=True)\n",
    "        print(\"Created: \", out_dir)\n",
    "\n",
    "    n_traversals = 20\n",
    "    traversal_start = -3\n",
    "    traversal_end = 3\n",
    "    run_content_traversal(model, c, s, \n",
    "                          traversal_start=traversal_start,\n",
    "                         traversal_end=traversal_end,\n",
    "                         n_traversals=n_traversals,\n",
    "                        show=True, \n",
    "                        title=title,\n",
    "                         to_save=to_save,\n",
    "                         out_path=out_dir/f\"content_traversals_{now2str()}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent content space -- Unconditioned on the input; Based on the prior distirbution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "mode = 'train'\n",
    "ds = getattr(dm, f\"{mode}_ds\")\n",
    "query_size = 1\n",
    "dl = DataLoader(ds, batch_size=query_size, shuffle=True, \n",
    "                num_workers=16, pin_memory=True)\n",
    "\n",
    "\n",
    "n_samples = 10\n",
    "for i in range(n_samples):\n",
    "    # Encode x to c,s\n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(dl))\n",
    "        x = batch['img']\n",
    "        label_c = batch['digit']  \n",
    "        label_s = batch['color']\n",
    "\n",
    "        dict_qparams = model.encode(x)\n",
    "        dict_z = model.rsample(dict_qparams)\n",
    "        c = dict_z['c'][0]\n",
    "        s = dict_z['s'][0]\n",
    "\n",
    "\n",
    "    # Content Traversal\n",
    "    # Use c from the prior distribution\n",
    "    title = f\"Content Traversal based on prior\"\n",
    "    out_dir = log_dir/\"content_traversals_unconditioned\"\n",
    "    to_save = True\n",
    "    if not out_dir.exists():\n",
    "        out_dir.mkdir(parents=True)\n",
    "        print(\"Created: \", out_dir)\n",
    "\n",
    "    n_traversals = 20\n",
    "    traversal_start = -4\n",
    "    traversal_end = 4\n",
    "    c_prior = torch.zeros_like(c)\n",
    "    run_content_traversal(model, c_prior, s, \n",
    "                          traversal_start=traversal_start,\n",
    "                         traversal_end=traversal_end,\n",
    "                         n_traversals=n_traversals,\n",
    "                        show=True, \n",
    "                        title=title,\n",
    "                         to_save=to_save,\n",
    "                         out_path=out_dir/(\n",
    "                             \"content_traversals_unconditioned_\"\n",
    "                             f\"start-{traversal_start}_end-{traversal_end}_{now2str()}.png\"\n",
    "                         )\n",
    "                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "## Latent Space Traversal: Style space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear traversal in a single dimension\n",
    "\n",
    "For each content-id's representative code:\n",
    "  - Fix a style code representative\n",
    "  - Traverse for each dim of the content code\n",
    "  - Put the results in a grid where each colm is the dimension of the content and each row shows the traversed content code's reconstruction result (in combination with the fixed style code representative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluator.qualitative import get_traversals\n",
    "from src.utils.misc import now2str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run content traversals for each content code that is a representative of each content class\n",
    "n_traversals = 20\n",
    "out_dir = log_dir/\"style_traversals_of_reps\"\n",
    "run_name = now2str()\n",
    "\n",
    "to_save = True\n",
    "if not out_dir.exists():\n",
    "    out_dir.mkdir(parents=True)\n",
    "    print(\"Created: \", out_dir)\n",
    "\n",
    "for style_id in range(n_styles):\n",
    "    s = style_reps[style_id]\n",
    "    for content_id in range(n_contents):\n",
    "        c = content_reps[content_id]\n",
    "        \n",
    "        # Traverse for each dim\n",
    "        traversal_start = mu_qs_mins[style_id]\n",
    "        traversal_end = mu_qs_maxs[style_id]\n",
    "    #     traversal_start = torch.zeros_like(c).fill_(-3.0)#mu_qc_mins[content_id]\n",
    "    #     traversal_end =  torch.zeros_like(c).fill_(3.0)# mu_qc_maxs[content_id]\n",
    "        title = f\"Style Traversal: content {content_id}, style {style_id}\"\n",
    "        run_style_traversal(model, c, s, \n",
    "                              traversal_start=traversal_start,\n",
    "                             traversal_end=traversal_end,\n",
    "                             n_traversals=n_traversals,\n",
    "                            show=True, \n",
    "                            title=title,\n",
    "                             to_save=to_save,\n",
    "                             out_path=out_dir/f\"style_traversals_c-{content_id}_s-{style_id}_{run_name}.png\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for content_id in range(n_contents):\n",
    "#     c = content_reps[content_id]\n",
    "#     content_dim = c.shape[-1]\n",
    "\n",
    "#     for style_id in range(n_styles):\n",
    "#         s = style_reps[style_id]\n",
    "        \n",
    "#         # Traverse for each dim\n",
    "#         n_traversals = 10\n",
    "#         grids = [] #k,v = dim_i, batch of recons while traversing in dim_i direction (n_traversals, *dim_x)\n",
    "#         for dim_i in range(content_dim):\n",
    "#             min_dim_i = mu_qc_mins[content_id][dim_i]\n",
    "#             max_dim_i = mu_qc_maxs[content_id][dim_i]\n",
    "#             print(min_dim_i, max_dim_i)\n",
    "\n",
    "#             c_traversals = get_traversals(c, dim_i, min_dim_i, max_dim_i, n_traversals)\n",
    "#             dict_z = {\n",
    "#                 \"c\": c_traversals, \n",
    "#                 \"s\": s.repeat((n_traversals, 1))\n",
    "#                      }\n",
    "#             z = model.combine_content_style(dict_z)\n",
    "#             recons = model.decode(z)\n",
    "#         #     show_timgs(recons, nrows=n_traversals, title=f\"Traversal: Content_id:{content_id}, dim:{dim_i}\")\n",
    "\n",
    "#             grid = torchvision.utils.make_grid(recons, nrow=1) # Caveat: nrow is num of colms!\n",
    "#             grids.append(grid)\n",
    "#         grids = torch.cat(grids, dim=2)\n",
    "\n",
    "#         show_timg(grids, title=f\"Content Traversal: content {content_id}, style {style_id}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General traversal on an input data point\n",
    "Unlike #1 above where we traversed per style-id code, we do a general traversal on the whole style space, where we used the content/style representative codes, we traverse on the learned latent style dimension.\n",
    "\n",
    "input x -> mu_c, logvar_c\n",
    "        -> mu_s, logvar_s -> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent style space -- Conditioned on the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "mode = 'train'\n",
    "ds = getattr(dm, f\"{mode}_ds\")\n",
    "query_size = 1\n",
    "dl = DataLoader(ds, batch_size=query_size, shuffle=True, \n",
    "                num_workers=16, pin_memory=True)\n",
    "\n",
    "\n",
    "n_samples = 10\n",
    "for i in range(n_samples):\n",
    "    # Encode x to c,s\n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(dl))\n",
    "        x = batch['img']\n",
    "        label_c = batch['digit']  \n",
    "        label_s = batch['color']\n",
    "\n",
    "        dict_qparams = model.encode(x)\n",
    "        dict_z = model.rsample(dict_qparams)\n",
    "        c = dict_z['c'][0]\n",
    "        s = dict_z['s'][0]\n",
    "\n",
    "\n",
    "    # Style Traversal\n",
    "    title = f\"Style Traversal conditioned on input\"\n",
    "    out_dir = log_dir/\"style_traversals_of_input\"\n",
    "    to_save = True\n",
    "    if not out_dir.exists():\n",
    "        out_dir.mkdir(parents=True)\n",
    "        print(\"Created: \", out_dir)\n",
    "\n",
    "    n_traversals = 30\n",
    "    traversal_start = -3.5\n",
    "    traversal_end = 3.5\n",
    "    run_style_traversal(model, c, s, \n",
    "                          traversal_start=traversal_start,\n",
    "                         traversal_end=traversal_end,\n",
    "                         n_traversals=n_traversals,\n",
    "                        show=True, \n",
    "                        title=title,\n",
    "                         to_save=to_save,\n",
    "                         out_path=out_dir/(\n",
    "                         \"style_traversals_conditioned_\"\n",
    "                             f\"start-{traversal_start}_end-{traversal_end}_{now2str()}.png\"\n",
    "                         )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent content space -- Unconditioned on the input; Based on the prior distirbution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "mode = 'train'\n",
    "ds = getattr(dm, f\"{mode}_ds\")\n",
    "query_size = 1\n",
    "dl = DataLoader(ds, batch_size=query_size, shuffle=True, \n",
    "                num_workers=16, pin_memory=True)\n",
    "\n",
    "\n",
    "n_samples = 10\n",
    "for i in range(n_samples):\n",
    "    # Encode x to c,s\n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(dl))\n",
    "        x = batch['img']\n",
    "        label_c = batch['digit']  \n",
    "        label_s = batch['color']\n",
    "\n",
    "        dict_qparams = model.encode(x)\n",
    "        dict_z = model.rsample(dict_qparams)\n",
    "        c = dict_z['c'][0]\n",
    "        s = dict_z['s'][0]\n",
    "\n",
    "\n",
    "    # Style Traversal\n",
    "    # Use c from the prior distribution\n",
    "    title = f\"Style Traversal based on prior\"\n",
    "    out_dir = log_dir/\"style_traversals_unconditioned\"\n",
    "    to_save = True\n",
    "    if not out_dir.exists():\n",
    "        out_dir.mkdir(parents=True)\n",
    "        print(\"Created: \", out_dir)\n",
    "\n",
    "    n_traversals = 20\n",
    "    traversal_start = -4\n",
    "    traversal_end = 4\n",
    "    s_prior = torch.zeros_like(s)\n",
    "    run_content_traversal(model, c, s_prior, \n",
    "                          traversal_start=traversal_start,\n",
    "                         traversal_end=traversal_end,\n",
    "                         n_traversals=n_traversals,\n",
    "                        show=True, \n",
    "                        title=title,\n",
    "                         to_save=to_save,\n",
    "                         out_path=out_dir/(\n",
    "                             \"style_traversals_unconditioned_\"\n",
    "                             f\"start-{traversal_start}_end-{traversal_end}_{now2str()}.png\"\n",
    "                         )\n",
    "                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "## Generate new style codes via. style code interpolation\n",
    "Added: Jan 18, 2021\n",
    "1. Get the cluster centers from style code embeddings: r,g,b codes\n",
    "2. new style code = interpolate (r,g,b)\n",
    "3. Take an image -> get its content code -> z = [c, new_style] -> generator?\n",
    "  - z = [c, r] -> generator\n",
    "  - z = [c, g] -> gen\n",
    "  - z = [c, b] -> gen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test]",
   "language": "python",
   "name": "conda-env-test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
