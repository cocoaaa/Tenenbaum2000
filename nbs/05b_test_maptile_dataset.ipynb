{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import re\n",
    "import math\n",
    "from datetime import datetime\n",
    "import time\n",
    "sys.dont_write_bytecode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Set, Dict, Tuple, Optional, Iterable, Mapping, Union, Callable\n",
    "\n",
    "from pprint import pprint\n",
    "from ipdb import set_trace as brpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import holoviews as hv\n",
    "# from holoviews import opts\n",
    "# hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from  torch.linalg import norm as tnorm\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "\n",
    "# Select Visible GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Path \n",
    "1. Add project root and src folders to `sys.path`\n",
    "2. Set DATA_ROOT to `maptile_v2` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_nb_path = Path(os.getcwd())\n",
    "ROOT = this_nb_path.parent\n",
    "SRC = ROOT/'src'\n",
    "DATA_ROOT = Path(\"/data/hayley-old/maptiles_v2/\")\n",
    "paths2add = [this_nb_path, ROOT, SRC]\n",
    "\n",
    "print(\"Project root: \", str(ROOT))\n",
    "print('Src folder: ', str(SRC))\n",
    "print(\"This nb path: \", str(this_nb_path))\n",
    "\n",
    "\n",
    "for p in paths2add:\n",
    "    if str(p) not in sys.path:\n",
    "        sys.path.insert(0, str(p))\n",
    "        print(f\"\\n{str(p)} added to the path.\")\n",
    "        \n",
    "# print(sys.path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.datasets.maptiles import Maptiles, MapStyles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styles = MapStyles.get_longnames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_maptiles_1():\n",
    "    cities = ['la']\n",
    "    styles = ['OSMDefault']\n",
    "    zooms = ['14']\n",
    "    dset = Maptiles(\n",
    "        data_root=DATA_ROOT, \n",
    "        cities=cities, \n",
    "        styles=styles, \n",
    "        zooms=zooms)\n",
    "    dset.print_meta()\n",
    "    dset.show_samples()\n",
    "    \n",
    "test_maptiles_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_maptiles_xform(in_size=64):\n",
    "    xform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize(in_size)\n",
    "    ])\n",
    "    cities = ['paris']\n",
    "    styles = ['OSMDefault']\n",
    "    zooms = ['14']\n",
    "    dset = Maptiles(data_root=DATA_ROOT, \n",
    "                    cities=cities, \n",
    "                    styles=styles, \n",
    "                    zooms=zooms, \n",
    "                    transform=xform)\n",
    "    dset.print_meta()\n",
    "#     dset.show_samples(order='chw')\n",
    "    \n",
    "test_maptiles_xform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a simple NN model to classify a city\n",
    "#LightningModule is a subclass of nn.Module. We can think of it as an abstraction of a NN model (plus sugars for easier experimentation)\n",
    "# Model specification\n",
    "# - architecture and parameters in `__init__` method\n",
    "# - forward computation in `forward` method\n",
    "# - training step: in `training_step(self, batch, batch_idx)`\n",
    "\n",
    "class LitModel(LightningModule):\n",
    "    def __init__(self, nh1, nh2, \n",
    "                 dim_in=28*28, n_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define model architecture\n",
    "        self.layer1 = nn.Linear(dim_in, nh1)\n",
    "        self.layer2 = nn.Linear(nh1, nh2)\n",
    "        self.layer3 = nn.Linear(nh2, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs, n_channels, height, width = x.size()\n",
    "        \n",
    "        # (b, 1, 28,28) -> (b, 1*28*28)\n",
    "        x = x.view(bs, -1)\n",
    "        x = self.layer1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"Implements one mini-batch iteration from batch intack -> pass through model -> return loss (ie. computational graph)\"\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_ids):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        return loss\n",
    "    \n",
    "    # Second component: Optimization Solver\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "# -- we need torch.utils.data.DataLoader objects that specifies the datasets and how to load the data for train/valtest\n",
    "from src.data.datasets.maptiles import MapStyles\n",
    "from src.data.transforms.transforms import Identity\n",
    "bs = 16\n",
    "pin_memory = True\n",
    "num_workers = 8\n",
    "in_size = 64\n",
    "n_channels = 3\n",
    "cities = ['la', 'seoul']\n",
    "# styles = ['StamenTerrainLines', 'OSMDefault', 'CartoVoyagerNoLabels']\n",
    "styles = ['OSMDefault', 'CartoVoyagerNoLabels']\n",
    "\n",
    "zooms = ['14']\n",
    "xform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(in_size),\n",
    "    transforms.Grayscale() if n_channels > 1  else Identity,\n",
    "#     transforms.Lambda(lambda t: t[[0]]) # get the first channel only\n",
    "])\n",
    "\n",
    "\n",
    "target_xform = transforms.Lambda(\n",
    "    lambda label_dict: 0 if label_dict[\"style\"]==styles[0] else 1\n",
    ")\n",
    "dset = Maptiles(\n",
    "    data_root=DATA_ROOT, \n",
    "    cities=cities, \n",
    "    styles=styles, \n",
    "    zooms=zooms,# verbose=True,\n",
    "    transform=xform, \n",
    "    target_transform=target_xform)\n",
    "dset.print_meta()\n",
    "\n",
    "dl_train = DataLoader(dset, batch_size=bs, shuffle=True, \n",
    "                      num_workers=num_workers, pin_memory=pin_memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "from src.visualize.utils import show_timgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(dl_train))\n",
    "print(x.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y0, y1 = y[y==0], y[y==1]\n",
    "len(y0), len(y1)\n",
    "x0, x1 = x[y==0], x[y==1]\n",
    "x0.shape, x1.shape\n",
    "\n",
    "show_timgs(x0, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_timgs(x1, titles=y1.numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid = make_grid(x, padding=10)\n",
    "# grid.size()\n",
    "# show(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a functiion to split a Maptiles (dataset) object into two groups by splitting the indices into 2 random groups\n",
    "# -- usage: define the whole maptiles dataset, then split it into train and val datasets\n",
    "from src.data.datasets.maptiles import Maptiles\n",
    "def test_random_split_maptiles():\n",
    "    dset = Maptiles(data_root=DATA_ROOT, \n",
    "                    cities=['paris'], styles=['OSMDefault'], zooms=['14'])\n",
    "    dset0, dset1 = Maptiles.random_split(dset, 0.5)\n",
    "    dset0.show_samples()\n",
    "    dset1.show_samples()\n",
    "    print(len(dset0), len(dset1), len(dset))\n",
    "    assert len(dset0)+len(dset1) == len(dset)\n",
    "test_random_split_maptiles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start experiment\n",
    "Instantiate the LighteningModule, and the PL's `Trainer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the pl Module\n",
    "nh1, nh2 = 100,100\n",
    "model = LitModel(nh1=nh1, nh2=nh2, dim_in=in_size**2*n_channels, n_classes=2)\n",
    "\n",
    "# Instantiate a PL `Trainer` object\n",
    "# -- most basic trainer: uses good defaults, eg: auto-tensorboard logging, checkpoints, logs, etc.\n",
    "max_eps = 300\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=max_eps)\n",
    "trainer.fit(model, dl_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightning Data Module\n",
    "A better way to encaptulate/modularize the train/val datasets and train/val dataloading. Also handles:\n",
    "- setting up the data in a machine: eg. downloading\n",
    "\n",
    "Dec 9, 2020\n",
    "- lightning callbacks\n",
    "- logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transfer the data loading script to pl.LightningDataModule\n",
    "```python\n",
    "# Data loading\n",
    "# -- we need torch.utils.data.DataLoader objects that specifies the datasets and how to load the data for train/valtest\n",
    "from src.data.datasets.maptiles import MapStyles\n",
    "from src.data.transforms.transforms import Identity\n",
    "bs = 16\n",
    "pin_memory = True\n",
    "num_workers = 8\n",
    "in_size = 64\n",
    "n_channels = 3\n",
    "cities = ['la', 'seoul']\n",
    "# styles = ['StamenTerrainLines', 'OSMDefault', 'CartoVoyagerNoLabels']\n",
    "styles = ['OSMDefault', 'CartoVoyagerNoLabels']\n",
    "\n",
    "zooms = ['14']\n",
    "xform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(in_size),\n",
    "    if n_channels > 1:transforms.Grayscale() else Identity,\n",
    "#     transforms.Lambda(lambda t: t[[0]]) # get the first channel only\n",
    "])\n",
    "\n",
    "\n",
    "target_xform = transforms.Lambda(\n",
    "    lambda label_dict: 0 if label_dict[\"style\"]==styles[0] else 1\n",
    ")\n",
    "dset = Maptiles(DATA_ROOT, cities, styles, zooms,# verbose=True,\n",
    "                transform=xform, target_transform=target_xform)\n",
    "dset.print_meta()\n",
    "\n",
    "dl_train = DataLoader(dset, batch_size=bs, shuffle=True, \n",
    "                      num_workers=num_workers, pin_memory=pin_memory)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaptilesModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, *,\n",
    "                 cities: Iterable[str],\n",
    "                 styles: Iterable[str],\n",
    "                 zooms: Iterable[str],\n",
    "                 transform: Callable = None,\n",
    "                 target_transform: Callable = None,\n",
    "                 df_fns: pd.DataFrame=None,\n",
    "                 data_root: Path=None, # --end of Maptile init args\n",
    "                 in_size: int=64,\n",
    "                 n_channels: int=3,\n",
    "                 bs: int=32,\n",
    "                verbose: bool=False,\n",
    "                pin_memory = True,\n",
    "                num_workers = 8):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.df_fns = df_fns\n",
    "        self.data_root = data_root\n",
    "        self.cities = cities\n",
    "        self.styles = styles\n",
    "        self.zooms = zooms\n",
    "        \n",
    "        # transforms\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.in_size = in_size\n",
    "        self.n_channels = n_channels\n",
    "        \n",
    "        # default transforms\n",
    "        if self.transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize(self.in_size),\n",
    "#             transforms.Grayscale() if self.n_channels > 1 else Identity,\n",
    "            ])\n",
    "\n",
    "        # Default transsforms for Maptile dataset's target label_dict\n",
    "        # Maptiles class's `__getitems__` returns (x, label_dict) \n",
    "        # -- where label_dict = {\n",
    "        #    \"city\": city,\n",
    "        #    \"style\": style,\n",
    "        #    \"zoom\": zoom,\n",
    "        #    \"coord\": coord}\n",
    "        # Default: returns the style label \n",
    "        # -- ie. prepare a sample for the style prediction problem\n",
    "        if self.target_transform is None:\n",
    "            self.target_transform = transforms.Lambda(\n",
    "            lambda label_dict: 0 if label_dict[\"style\"]==styles[0] else 1\n",
    "        )\n",
    "        # data loading\n",
    "        self.bs = bs\n",
    "        self.verbose = verbose\n",
    "        self.pin_memory = pin_memory\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "    def prepare_data(self, download_dir: Path=None):\n",
    "        if download_dir is None:\n",
    "            download_dir = Path.cwd() #or self.data_dir?\n",
    "        # TODO: \n",
    "        # download maptile dataset to the download_dir\n",
    "        pass\n",
    "    \n",
    "    def setup(self, stage: str):\n",
    "        # This function is called on every GPU in a node/machine\n",
    "        # Sets self.train_ds, self.val_ds\n",
    "        # -- this also configures this DataModule to have a specified transforms \n",
    "        # -- that will be applied to each sample in the dataset\n",
    "        \n",
    "#         # Default transforms for Maptile dataset's imgs\n",
    "#         if self.transform is None:\n",
    "#             xform = transforms.Compose([\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Resize(self.in_size),\n",
    "#             transforms.Grayscale() if self.n_channels > 1 else Identity,\n",
    "#             ])\n",
    "\n",
    "#         # Default transsforms for Maptile dataset's target label_dict\n",
    "#         # Maptiles class's `__getitems__` returns (x, label_dict) \n",
    "#         # -- where label_dict = {\n",
    "#         #    \"city\": city,\n",
    "#         #    \"style\": style,\n",
    "#         #    \"zoom\": zoom,\n",
    "#         #    \"coord\": coord\n",
    "#         #}\n",
    "#         target_xform = transforms.Lambda(\n",
    "#             lambda label_dict: 0 if label_dict[\"style\"]==styles[0] else 1\n",
    "#         )\n",
    "        dset = Maptiles(\n",
    "            df_fns=self.df_fns,\n",
    "            data_root=self.data_root, \n",
    "            cities=self.cities, \n",
    "            styles=self.styles, \n",
    "            zooms=self.zooms, \n",
    "            transform=self.transform, \n",
    "            target_transform=self.target_transform)\n",
    "        \n",
    "        # split to train/val or test\n",
    "        if stage == 'fit':\n",
    "            self.train_ds, self.val_ds = random_split_maptiles(dset, 0.7)\n",
    "            assert len(self.train_ds)+len(self.val_ds) == len(dset)\n",
    "        \n",
    "        if stage == 'test':\n",
    "            # split the whole dataset into tr:val:test=4:3:3\n",
    "            self.tv_ds, self.test_ds = random_split_maptiles(dset, 0.7)\n",
    "            self.train_ds, self.val_ds = random_split_maptiles(self.tv_ds, 4./7.)\n",
    "            print([len(x) for x in [self.train_ds, self.val_ds, self.test_ds]])\n",
    "    \n",
    "    \n",
    "    # return the dataloader for each split\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds, batch_size=self.bs, pin_memory=self.pin_memory, num_workers=self.num_workers)\n",
    "        \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_ds, batch_size=self.bs, pin_memory=self.pin_memory, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_ds, batch_size=self.bs, pin_memory=self.pin_memory, num_workers=self.num_workers)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def test_maptiles_module():\n",
    "    cities = ['paris']\n",
    "    styles = ['OSMDefault']\n",
    "    zooms = ['14']\n",
    "    data_root = DATA_ROOT\n",
    "    dm = MaptilesModule(data_root=data_root,\n",
    "                        cities=cities,\n",
    "                        styles=styles,\n",
    "                        zooms=zooms)\n",
    "    dm.setup(stage='test')\n",
    "    train_dl, val_dl, test_dl = dm.train_dataloader(), dm.val_dataloader(), dm.test_dataloader()\n",
    "    train_dl.dataset.show_samples(order='chw')\n",
    "#     brpt()\n",
    "test_maptiles_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## Start experiment - version 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate data module\n",
    "cities = ['paris']\n",
    "styles = ['OSMDefault', 'CartoVoyagerNoLabels']\n",
    "dm = MaptilesModule(data_root=DATA_ROOT,\n",
    "                        cities=cities,\n",
    "                        styles=styles,\n",
    "                        zooms=zooms)\n",
    "\n",
    "# Instantiate the pl Module\n",
    "nh1, nh2 = 100,100\n",
    "dim_in = dm.in_size**2*dm.n_channels\n",
    "model = LitModel(nh1=nh1, nh2=nh2, dim_in=dim_in, n_classes=2)\n",
    "\n",
    "# Instantiate a PL `Trainer` object\n",
    "# -- most basic trainer: uses good defaults, eg: auto-tensorboard logging, checkpoints, logs, etc.\n",
    "# -- Pass the data module along with a pl module\n",
    "max_eps = 300\n",
    "trainer_config = {\n",
    "    'gpus':1,\n",
    "    'max_epochs': max_eps,\n",
    "    'progress_bar_refresh_rate':10,\n",
    "    'auto_lr_find': True,\n",
    "    'terminate_on_nan':True,\n",
    "    'val_check_interval': 10, #iterations\n",
    "    \n",
    "    \n",
    "}\n",
    "trainer = pl.Trainer(**trainer_config)\n",
    "# trainer = pl.Trainer(fast_dev_run=True)\n",
    "\n",
    "trainer.fit(model, dm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch2]",
   "language": "python",
   "name": "conda-env-torch2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
