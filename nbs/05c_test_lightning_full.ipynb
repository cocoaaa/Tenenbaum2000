{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import re\n",
    "import math\n",
    "from datetime import datetime\n",
    "import time\n",
    "sys.dont_write_bytecode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Set, Dict, Tuple, Optional, Iterable, Mapping, Union, Callable\n",
    "\n",
    "from pprint import pprint\n",
    "from ipdb import set_trace as brpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from  torch.linalg import norm as tnorm\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "# Select Visible GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Path \n",
    "1. Add project root and src folders to `sys.path`\n",
    "2. Set DATA_ROOT to `maptile_v2` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root:  /data/hayley-old/Tenanbaum2000\n",
      "Src folder:  /data/hayley-old/Tenanbaum2000/src\n",
      "This nb path:  /data/hayley-old/Tenanbaum2000/nbs\n",
      "\n",
      "/data/hayley-old/Tenanbaum2000 added to the path.\n"
     ]
    }
   ],
   "source": [
    "this_nb_path = Path(os.getcwd())\n",
    "ROOT = this_nb_path.parent\n",
    "SRC = ROOT/'src'\n",
    "DATA_ROOT = Path(\"/data/hayley-old/maptiles_v2/\")\n",
    "paths2add = [this_nb_path, ROOT]\n",
    "\n",
    "print(\"Project root: \", str(ROOT))\n",
    "print('Src folder: ', str(SRC))\n",
    "print(\"This nb path: \", str(this_nb_path))\n",
    "\n",
    "\n",
    "for p in paths2add:\n",
    "    if str(p) not in sys.path:\n",
    "        sys.path.insert(0, str(p))\n",
    "        print(f\"\\n{str(p)} added to the path.\")\n",
    "        \n",
    "# print(sys.path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.data.datasets.maptiles import Maptiles, MapStyles\n",
    "from src.data.datamodules.mnist_datamodule import MNISTDataModule\n",
    "from src.data.datamodules.maptiles_datamodule import MaptilesDataModule\n",
    "\n",
    "from src.models.plmodules.three_fcs import ThreeFCs\n",
    "from src.models.plmodules.vanilla_vae import VanillaVAE\n",
    "from src.models.plmodules.beta_vae import BetaVAE\n",
    "\n",
    "from src.visualize.utils import show_timgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start experiment \n",
    "Given a maptile, predict its style as one of OSM, CartoVoyager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DM:  MNIST\n"
     ]
    }
   ],
   "source": [
    "# Instantiate MNIST Datamodule\n",
    "in_shape = (1,32,32)\n",
    "batch_size = 32\n",
    "dm = MNISTDataModule(data_root=ROOT/'data', \n",
    "                       in_shape=in_shape,\n",
    "                      batch_size=batch_size)\n",
    "dm.setup('fit')\n",
    "print(\"DM: \", dm.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique styles:  ['StamenTonerBackground']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-9fcecfb3fddd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m                        \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                        )\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DM: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/test/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_prepared_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/hayley-old/Tenanbaum2000/src/data/datamodules/maptiles_datamodule.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, stage, use_training_stat)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \"\"\"\n\u001b[1;32m    157\u001b[0m         \u001b[0;31m# breakpoint()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         dset = MaptilesDataset(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0mdf_fns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_fns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mdata_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_root\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/hayley-old/Tenanbaum2000/src/data/datasets/maptiles.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_root, cities, styles, zooms, n_channels, transform, target_transform, df_fns, verbose)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unique styles: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_fns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"style\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannel_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannel_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_channelwise_mean_std\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/hayley-old/Tenanbaum2000/src/data/datasets/maptiles.py\u001b[0m in \u001b[0;36mget_channelwise_mean_std\u001b[0;34m(dset, n_channels)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mn_pixels\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0mchannel_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m             \u001b[0mchannel_squared_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mchannel_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchannel_sum\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_pixels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/test/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2241\u001b[0;31m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0m\u001b[1;32m   2242\u001b[0m                           initial=initial, where=where)\n\u001b[1;32m   2243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/test/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Instantiate data module\n",
    "# all_cities = ['la', 'charlotte', 'vegas', 'boston', 'paris', \\\n",
    "#               'amsterdam', 'shanghai', 'seoul', 'chicago', 'manhattan', \\\n",
    "#              'berlin', 'montreal', 'rome']\n",
    "# cities = all_cities #['berlin']#['paris']\n",
    "# styles = ['StamenTonerBackground']#['OSMDefault', 'CartoVoyagerNoLabels']\n",
    "# zooms = ['14']\n",
    "# in_shape = (1, 64, 64)\n",
    "# batch_size = 32\n",
    "# dm = MaptilesDataModule(data_root=DATA_ROOT,\n",
    "#                         cities=cities,\n",
    "#                         styles=styles,\n",
    "#                         zooms=zooms,\n",
    "#                        in_shape=in_shape,\n",
    "#                        batch_size=batch_size\n",
    "#                        )\n",
    "# dm.setup('fit')\n",
    "# print(\"DM: \", dm.name)\n",
    "\n",
    "# # Instantiate the pl Module\n",
    "# latent_dim = 10\n",
    "# hidden_dims = [32,64,128,256,512]\n",
    "# act_fn = nn.LeakyReLU()\n",
    "# learning_rate = 3e-4\n",
    "# model = VanillaVAE(\n",
    "#     in_shape=in_shape,\n",
    "#     latent_dim=latent_dim,\n",
    "#     hidden_dims=hidden_dims,\n",
    "#     learning_rate=learning_rate,\n",
    "#     act_fn=act_fn\n",
    "# )\n",
    "# print(model.hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the pl Module\n",
    "betas = [0.1 * 3**i for i in range(10)]\n",
    "# for kld_weight in [1.0]\n",
    "latent_dim = 10\n",
    "hidden_dims = [32,64,128,256]#,512]\n",
    "act_fn = nn.LeakyReLU()\n",
    "learning_rate = 3e-4\n",
    "kld_weight = betas[0]\n",
    "model = BetaVAE(\n",
    "    in_shape=in_shape, \n",
    "    latent_dim=latent_dim,\n",
    "    hidden_dims=hidden_dims,\n",
    "    learning_rate=learning_rate,\n",
    "    act_fn=act_fn,\n",
    "    kld_weight=kld_weight\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'betaVAE-0.100'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: /data/hayley-old/Tenanbaum2000/temp-logs/betaVAE-0.100_MNIST\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log dir:  /data/hayley-old/Tenanbaum2000/temp-logs/betaVAE-0.100_MNIST/version_0\n",
      "Created:  /data/hayley-old/Tenanbaum2000/temp-logs/betaVAE-0.100_MNIST/version_0\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a PL `Trainer` object\n",
    "# Start the experiment\n",
    "max_epochs = 200\n",
    "exp_name = f'{model.name}_{dm.name}'\n",
    "tb_logger = pl_loggers.TensorBoardLogger(save_dir=f'{ROOT}/temp-logs', \n",
    "                                         name=exp_name,\n",
    "                                         log_graph=False,\n",
    "                                        default_hp_metric=False)\n",
    "print(\"Log dir: \", tb_logger.log_dir)\n",
    "\n",
    "log_dir = Path(tb_logger.log_dir)\n",
    "if not log_dir.exists():\n",
    "    log_dir.mkdir(parents=True)\n",
    "    print(\"Created: \", log_dir)\n",
    "    \n",
    "\n",
    "# Log computational graph\n",
    "# model_wrapper = ModelWrapper(model)\n",
    "# tb_logger.experiment.add_graph(model_wrapper, model.example_input_array.to(model.device))\n",
    "# tb_logger.log_graph(model)\n",
    "\n",
    "trainer_config = {\n",
    "    'gpus':1,\n",
    "    'max_epochs': max_epochs,\n",
    "    'progress_bar_refresh_rate':0,\n",
    "    'terminate_on_nan':True,\n",
    "    'check_val_every_n_epoch':10,\n",
    "    'logger':tb_logger,\n",
    "#     'callbacks':callbacks,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BetaVAE is called\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name              | Type       | Params | In sizes        | Out sizes      \n",
      "-------------------------------------------------------------------------------------\n",
      "0 | act_fn            | LeakyReLU  | 0      | [1, 32, 16, 16] | [1, 32, 16, 16]\n",
      "1 | encoder           | Sequential | 388 K  | [1, 1, 32, 32]  | [1, 256, 2, 2] \n",
      "2 | fc_mu             | Linear     | 10.2 K | [1, 1024]       | [1, 10]        \n",
      "3 | fc_var            | Linear     | 10.2 K | [1, 1024]       | [1, 10]        \n",
      "4 | fc_latent2decoder | Linear     | 11.3 K | [1, 10]         | [1, 1024]      \n",
      "5 | decoder           | Sequential | 387 K  | [1, 256, 2, 2]  | [1, 32, 16, 16]\n",
      "6 | final_layer       | Sequential | 301    | [1, 32, 16, 16] | [1, 1, 32, 32] \n",
      "-------------------------------------------------------------------------------------\n",
      "808 K     Trainable params\n",
      "0         Non-trainable params\n",
      "808 K     Total params\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 0, batch: 0\n",
      "{'kld': tensor(0.0119, device='cuda:0'),\n",
      " 'loss': tensor(33562.8828, device='cuda:0'),\n",
      " 'recon_loss': tensor(33562.8828, device='cuda:0')}\n",
      "Ep: 0, batch: 0\n",
      "{'kld': tensor(0.0105, device='cuda:0'),\n",
      " 'loss': tensor(26996.2949, device='cuda:0'),\n",
      " 'recon_loss': tensor(26996.2930, device='cuda:0')}\n",
      "Ep: 0, batch: 0\n",
      "{'kld': tensor(5.4438, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(23900.6543, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(23900.1094, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 0, batch: 300\n",
      "{'kld': tensor(48.0854, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(12250.5068, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(12245.6982, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 0, batch: 600\n",
      "{'kld': tensor(53.0465, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(11050.9785, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(11045.6738, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 0, batch: 900\n",
      "{'kld': tensor(56.9667, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(11482.3145, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(11476.6182, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 0, batch: 1200\n",
      "{'kld': tensor(59.8018, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(10157.7441, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(10151.7637, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 0, batch: 1500\n",
      "{'kld': tensor(60.8615, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(11346.0264, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(11339.9404, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 10, batch: 0\n",
      "{'kld': tensor(64.9752, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(8966.1338, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(8959.6367, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 10, batch: 300\n",
      "{'kld': tensor(64.3011, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(8993.4502, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(8987.0205, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 10, batch: 600\n",
      "{'kld': tensor(66.7932, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(9204.4014, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(9197.7217, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 10, batch: 900\n",
      "{'kld': tensor(65.8363, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(10036.9609, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(10030.3770, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 10, batch: 1200\n",
      "{'kld': tensor(64.3580, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(8451.9805, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(8445.5449, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 10, batch: 1500\n",
      "{'kld': tensor(65.8931, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(10391.1973, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(10384.6084, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 20, batch: 0\n",
      "{'kld': tensor(67.8155, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(9578.2119, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(9571.4307, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 20, batch: 300\n",
      "{'kld': tensor(68.8566, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(9059.9053, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(9053.0195, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 20, batch: 600\n",
      "{'kld': tensor(66.5642, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(9653.8008, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(9647.1445, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 20, batch: 900\n",
      "{'kld': tensor(66.3351, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(9625.1064, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(9618.4727, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 20, batch: 1200\n",
      "{'kld': tensor(66.5328, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(8576.9814, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(8570.3281, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 20, batch: 1500\n",
      "{'kld': tensor(70.1421, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(7739.2524, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(7732.2383, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 30, batch: 0\n",
      "{'kld': tensor(69.5096, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(9726.8477, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(9719.8965, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 30, batch: 300\n",
      "{'kld': tensor(68.9275, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(7746.0215, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(7739.1289, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 30, batch: 600\n",
      "{'kld': tensor(70.5118, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(9285.2539, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(9278.2031, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 30, batch: 900\n",
      "{'kld': tensor(69.6004, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(9431.8330, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(9424.8730, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 30, batch: 1200\n",
      "{'kld': tensor(68.9129, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(8360.5381, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(8353.6465, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 30, batch: 1500\n",
      "{'kld': tensor(67.9846, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(9620.5898, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(9613.7910, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 40, batch: 0\n",
      "{'kld': tensor(69.3413, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(7963.8403, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(7956.9062, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 40, batch: 300\n",
      "{'kld': tensor(68.5989, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(8051.7280, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(8044.8682, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 40, batch: 600\n",
      "{'kld': tensor(70.4617, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(7591.9712, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(7584.9248, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 40, batch: 900\n",
      "{'kld': tensor(67.3888, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(9456.2656, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(9449.5264, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 40, batch: 1200\n",
      "{'kld': tensor(72.5974, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(8708.9199, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(8701.6602, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 40, batch: 1500\n",
      "{'kld': tensor(72.9184, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(9310.8320, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(9303.5400, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 50, batch: 0\n",
      "{'kld': tensor(71.7564, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(10043.5430, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(10036.3672, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 50, batch: 300\n",
      "{'kld': tensor(68.4618, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(7507.0737, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(7500.2275, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 50, batch: 600\n",
      "{'kld': tensor(71.6975, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(8671.5977, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(8664.4277, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 50, batch: 900\n",
      "{'kld': tensor(72.9258, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(10214.5938, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(10207.3008, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 50, batch: 1200\n",
      "{'kld': tensor(70.5030, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(8174.6572, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(8167.6069, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 50, batch: 1500\n",
      "{'kld': tensor(69.3605, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(8340.6758, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(8333.7393, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 60, batch: 0\n",
      "{'kld': tensor(72.9764, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(8984.2256, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(8976.9277, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 60, batch: 300\n",
      "{'kld': tensor(69.5914, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(9146.9375, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(9139.9785, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 60, batch: 600\n",
      "{'kld': tensor(71.4699, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(9149.0127, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(9141.8652, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 60, batch: 900\n",
      "{'kld': tensor(73.1234, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(9265.1660, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(9257.8535, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 60, batch: 1200\n",
      "{'kld': tensor(70.9951, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(9159.9336, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(9152.8340, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 60, batch: 1500\n",
      "{'kld': tensor(68.9167, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(10208.5518, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(10201.6602, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 70, batch: 0\n",
      "{'kld': tensor(73.0029, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(8692.9570, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(8685.6572, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 70, batch: 300\n",
      "{'kld': tensor(70.5728, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(8818.3447, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(8811.2871, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 70, batch: 600\n",
      "{'kld': tensor(72.9089, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(8813.5400, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(8806.2490, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 70, batch: 900\n",
      "{'kld': tensor(72.2289, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(8547.7705, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(8540.5479, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 70, batch: 1200\n",
      "{'kld': tensor(69.3829, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(9413.7109, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(9406.7725, device='cuda:0', grad_fn=<MseLossBackward>)}\n",
      "Ep: 70, batch: 1500\n",
      "{'kld': tensor(69.2053, device='cuda:0', grad_fn=<MeanBackward1>),\n",
      " 'loss': tensor(9387.4072, device='cuda:0', grad_fn=<AddBackward0>),\n",
      " 'recon_loss': tensor(9380.4863, device='cuda:0', grad_fn=<MseLossBackward>)}\n"
     ]
    }
   ],
   "source": [
    "# trainer = pl.Trainer(fast_dev_run=3)\n",
    "trainer = pl.Trainer(**trainer_config)\n",
    "# trainer.tune(model=model, datamodule=dm)\n",
    "\n",
    "# Start exp\n",
    "# Fit model\n",
    "trainer.fit(model, dm)\n",
    "print(f\"Finished at ep {trainer.current_epoch, trainer.batch_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "# Instantiate data module\n",
    "cities = ['paris']\n",
    "styles = ['OSMDefault', 'CartoVoyagerNoLabels']\n",
    "zooms = ['14']\n",
    "dm = MaptilesDataModule(data_root=DATA_ROOT,\n",
    "                        cities=cities,\n",
    "                        styles=styles,\n",
    "                        zooms=zooms,\n",
    "                       bs=1)\n",
    "\n",
    "# Instantiate the pl Module\n",
    "in_shape = (3,64,64)\n",
    "latent_dim = 10\n",
    "hidden_dims = [32,64,128,256,512]\n",
    "act_fn = nn.LeakyReLU()\n",
    "model = VanillaVAE64(in_shape, \n",
    "                     latent_dim,\n",
    "                     hidden_dims,\n",
    "                     act_fn)\n",
    "print(model.hparams)\n",
    "# Instantiate a PL `Trainer` object\n",
    "# -- most basic trainer: uses good defaults, eg: auto-tensorboard logging, checkpoints, logs, etc.\n",
    "# -- Pass the data module along with a pl module\n",
    "# ref: https://www.learnopencv.com/tensorboard-with-pytorch-lightning/\n",
    "tb_logger = pl_loggers.TensorBoardLogger(save_dir='lightning_logs', name='vanilla_vae')\n",
    "trainer_config = {\n",
    "#     'gpus':1,\n",
    "    'max_epochs': 200,\n",
    "    'progress_bar_refresh_rate':20,\n",
    "    'auto_lr_find': True,\n",
    "    'terminate_on_nan':True,\n",
    "    'val_check_interval': 0.25, #iterations\n",
    "    'logger':tb_logger\n",
    "}\n",
    "trainer = pl.Trainer(**trainer_config)\n",
    "# trainer = pl.Trainer(fast_dev_run=True)\n",
    "\n",
    "trainer.fit(model, dm)\n",
    "\n",
    "# Finally,\n",
    "# Log this model's hyperparmeters to tensorboard\n",
    "# hparams = dict(model.hparams)\n",
    "# metrics = {'hparam/acc': model.hparams[\"loss\"]}\n",
    "# model.logger.experiment.add_hparams(hparam_dict=hparams,\n",
    "#                                     metric_dict=metrics) #how to store the 'best' value of the metric?\n",
    "# Alternatively, use pl.Logger's method \"log_hyperparameters\"\n",
    "#         logger.log_hyperparams(hparams, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "# Instantiate data module\n",
    "cities = ['paris']\n",
    "styles = ['OSMDefault', 'CartoVoyagerNoLabels']\n",
    "zooms = ['14']\n",
    "dm = MaptilesDataModule(data_root=DATA_ROOT,\n",
    "                        cities=cities,\n",
    "                        styles=styles,\n",
    "                        zooms=zooms,\n",
    "                       bs=1)\n",
    "\n",
    "# Instantiate the pl Module\n",
    "in_shape = (3,64,64)\n",
    "latent_dim = 20\n",
    "hidden_dims = [32,64,128,256,512]\n",
    "act_fn = nn.LeakyReLU()\n",
    "model = VanillaVAE64(in_shape, \n",
    "                     latent_dim,\n",
    "                     hidden_dims,\n",
    "                     act_fn)\n",
    "print(model.hparams)\n",
    "# Instantiate a PL `Trainer` object\n",
    "# -- most basic trainer: uses good defaults, eg: auto-tensorboard logging, checkpoints, logs, etc.\n",
    "# -- Pass the data module along with a pl module\n",
    "# ref: https://www.learnopencv.com/tensorboard-with-pytorch-lightning/\n",
    "tb_logger = pl_loggers.TensorBoardLogger(save_dir='lightning_logs', name='vanilla_vae')\n",
    "trainer_config = {\n",
    "#     'gpus':1,\n",
    "    'max_epochs': 200,\n",
    "    'progress_bar_refresh_rate':20,\n",
    "    'auto_lr_find': True,\n",
    "    'terminate_on_nan':True,\n",
    "    'val_check_interval': 0.25, #iterations\n",
    "    'logger':tb_logger\n",
    "}\n",
    "trainer = pl.Trainer(**trainer_config)\n",
    "# trainer = pl.Trainer(fast_dev_run=True)\n",
    "\n",
    "trainer.fit(model, dm)\n",
    "\n",
    "# Finally,\n",
    "# Log this model's hyperparmeters to tensorboard\n",
    "# hparams = dict(model.hparams)\n",
    "# metrics = {'hparam/acc': model.hparams[\"loss\"]}\n",
    "# model.logger.experiment.add_hparams(hparam_dict=hparams,\n",
    "#                                     metric_dict=metrics) #how to store the 'best' value of the metric?\n",
    "# Alternatively, use pl.Logger's method \"log_hyperparameters\"\n",
    "#         logger.log_hyperparams(hparams, metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "# Instantiate data module\n",
    "cities = ['paris']\n",
    "styles = ['OSMDefault', 'CartoVoyagerNoLabels']\n",
    "zooms = ['14']\n",
    "dm = MaptilesDataModule(data_root=DATA_ROOT,\n",
    "                        cities=cities,\n",
    "                        styles=styles,\n",
    "                        zooms=zooms,\n",
    "                       bs=1)\n",
    "\n",
    "# Instantiate the pl Module\n",
    "in_shape = (3,64,64)\n",
    "latent_dim = 30\n",
    "hidden_dims = [32,64,128,256,512]\n",
    "act_fn = nn.LeakyReLU()\n",
    "model = VanillaVAE64(in_shape, \n",
    "                     latent_dim,\n",
    "                     hidden_dims,\n",
    "                     act_fn)\n",
    "print(model.hparams)\n",
    "# Instantiate a PL `Trainer` object\n",
    "# -- most basic trainer: uses good defaults, eg: auto-tensorboard logging, checkpoints, logs, etc.\n",
    "# -- Pass the data module along with a pl module\n",
    "# ref: https://www.learnopencv.com/tensorboard-with-pytorch-lightning/\n",
    "tb_logger = pl_loggers.TensorBoardLogger(save_dir='lightning_logs', name='vanilla_vae')\n",
    "trainer_config = {\n",
    "#     'gpus':1,\n",
    "    'max_epochs': 200,\n",
    "    'progress_bar_refresh_rate':20,\n",
    "    'auto_lr_find': True,\n",
    "    'terminate_on_nan':True,\n",
    "    'val_check_interval': 0.25, #iterations\n",
    "    'logger':tb_logger\n",
    "}\n",
    "trainer = pl.Trainer(**trainer_config)\n",
    "# trainer = pl.Trainer(fast_dev_run=True)\n",
    "\n",
    "trainer.fit(model, dm)\n",
    "\n",
    "# Finally,\n",
    "# Log this model's hyperparmeters to tensorboard\n",
    "# hparams = dict(model.hparams)\n",
    "# metrics = {'hparam/acc': model.hparams[\"loss\"]}\n",
    "# model.logger.experiment.add_hparams(hparam_dict=hparams,\n",
    "#                                     metric_dict=metrics) #how to store the 'best' value of the metric?\n",
    "# Alternatively, use pl.Logger's method \"log_hyperparameters\"\n",
    "#         logger.log_hyperparams(hparams, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pl.Metrics Module\n",
    "python-lightning provides a class of metrics that inherits from `nn.Module`\n",
    "`Metrics` base class's `forward(x)` method does the 2 following actions:\n",
    "- Calls `update()` on its input `x`\n",
    "- Simultaneously, returns the value of the metric over the input\n",
    "\n",
    "Other key methods:\n",
    "- `Metric.update()`\n",
    "- `Metric.compute()`\n",
    "- `Metric.reset()`\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualize.utils import show_timgs, show_timg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 36\n",
    "with torch.no_grad():\n",
    "    sampled_recons = model.samaple(n_samples, model.device)\n",
    "    show_timgs(sampled_recons.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recons\n",
    "with torch.no_grad():\n",
    "    for n in range(n_samples):\n",
    "        x,y = next(iter(dm.train_dataloader()))\n",
    "        mu, log_var,recon = model(x)[\"mu\"], model(x)[\"log_var\"], model(x)[\"recon\"]\n",
    "        show_timg(recon.detach().squeeze())\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test]",
   "language": "python",
   "name": "conda-env-test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
