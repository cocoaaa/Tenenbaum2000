{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep, non-linear, two-factor model\n",
    "A standard symmetric bilinear model in Tenanbaum2000 can be described as:\n",
    "$$y^{sc}_k = \\sum_{j} \\sum_{i} w_{ijk}a^s_{i}b^c_{j}$$, which has an equivalent vector form:\n",
    "\n",
    "$$\\mathbf{y}^{sc} = \\sum_{j} \\sum_{i} \\mathbf{W}_{ij}a^s_{i}b^c_{j}$$ where $\\mathbf{W}_{ij}$ is a matrix of size (i,j).\n",
    "\n",
    "This symmetric model has 2 types of model parameters:\n",
    "- content variable $b$ of length $J$\n",
    "- K number of matrix $W_{ij}$ of size $(I,J)$: total number of parameters of this 3Dim tensior $W$ is IxJxK.\n",
    "    - Basis vector interpretation (See Eqn. 2.3): Alternative way to view this interaction weight parameter W is to view as $I \\times J$ number of vectors $w_{ij}$, each of which has a length of $K$.\n",
    "      This vector $w_{ij}$ specifices \n",
    "      - If we want to look at how the ith component of a style vector a^s and the jth component of a content vector b^c interacts over the entire image/data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "# %reset out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import re\n",
    "import math\n",
    "from datetime import datetime\n",
    "import time\n",
    "sys.dont_write_bytecode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "from typing import List, Set, Dict, Tuple, Optional, Iterable, Mapping, Union, Callable\n",
    "\n",
    "from ipdb import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import holoviews as hv\n",
    "# from holoviews import opts\n",
    "# hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from  torch.linalg import norm as tnorm\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def now2str():\n",
    "    now = datetime.now()\n",
    "    now_str = now.strftime(\"%m_%d_%H:%M:%S\")\n",
    "    return now_str\n",
    "\n",
    "def info(arr, header=None):\n",
    "    if header is None:\n",
    "        header = \"=\"*30\n",
    "    print(header)\n",
    "    print(\"shape: \", arr.shape)\n",
    "    print(\"dtype: \", arr.dtype)\n",
    "    print(\"min, max: \", min(np.ravel(arr)), max(np.ravel(arr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "def normalize_img(img, vmin=None, vmax=None, *, use_global_range=False):\n",
    "    if vmin is None:\n",
    "        vmin = img.min()\n",
    "    if vmax is None:\n",
    "        vmax = img.max()\n",
    "    shape = img.shape\n",
    "    return minmax_scale(img.ravel(), feature_range=(0,1)).reshape(shape)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(data: torch.Tensor, \n",
    "              n_styles:int, n_contents:int, img_size:Tuple[int],\n",
    "              *,title:str=None, normalize:bool=False) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Visualize 2 or 3dim data matrix\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "    - data (torch.Tensor)\n",
    "    If data.ndim == 2, data.shape is assumed to be (n_styles*dim_x, n_contents)\n",
    "    If data.ndim == 3:, data.shape is assumed to be (n_styles, n_contents, dim_x)\n",
    "    \n",
    "    - normalize (bool): project the values of each image by mapping the min and max to 0 and 1\n",
    "       - Potentialy useful for visualization of gradients or eigenbasis\n",
    "    \"\"\"\n",
    "    dim_x = np.prod(img_size)\n",
    "    \n",
    "    f, ax = plt.subplots(n_styles, n_contents, figsize=(5*n_contents, 5*n_styles))\n",
    "    if title is not None:\n",
    "        f.suptitle(title)\n",
    "    f.tight_layout()\n",
    "    for s in range(n_styles):\n",
    "        for c in range(n_contents):\n",
    "            if data.ndim == 2:\n",
    "                img = data[s*dim_x:(s+1)*dim_x, c].reshape(img_size)\n",
    "            elif data.ndim == 3:\n",
    "                img = data[s,c].reshape(img_size)\n",
    "            if normalize:\n",
    "                img = normalize_img(img)\n",
    "            ax[s][c].imshow(img)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_vectors(A: torch.Tensor, is_column: bool=True, title:str=None) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Visualize each vectors in the input (2dim) tensor as a bar chart\n",
    "    \n",
    "    - A: 2dim tensor whose columns are individual vectors; Assumed to be detached.\n",
    "    - is_column (bool): if True, assume A to be a collection of column vectors. \n",
    "        - Otherwise, A is assumed to be a collection of row vectors\n",
    "    \"\"\"\n",
    "    if not is_column:\n",
    "        A = A.T\n",
    "    n_vecs = A.shape[1]\n",
    "    \n",
    "    f, ax = plt.subplots(nrows=1, ncols=n_vecs, figsize=(n_vecs*3, 3))\n",
    "    if title is not None:\n",
    "        f.suptitle(title)\n",
    "    f.tight_layout()\n",
    "    ax = ax.flatten()\n",
    "    for i in range(n_vecs):\n",
    "        vec = A[:,i]\n",
    "        ax[i].bar(range(len(vec)), vec, label=f'{i}')\n",
    "        ax[i].set_title(f'Vector {i+1}')\n",
    "    return f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set specific\n",
    "n_styles = 3\n",
    "n_contents = 9\n",
    "\n",
    "\n",
    "# Hyperparams\n",
    "img_size = (64,64,3) #(img_h, img_w, n_channels)\n",
    "dim_content = 4 # J\n",
    "dim_style = 3 # I\n",
    "dim_x = np.prod(img_size)# K\n",
    "\n",
    "# Initialize model parameters\n",
    "# contents = torch.randn((n_contents, 1, dim_content)) # B: each row is a content vector\n",
    "# styles = torch.randn((n_styles, 1, dim_style)) # A: each row is a style vector\n",
    "# W = torch.randn((dim_x, dim_style, dim_content))\n",
    "\n",
    "# Version2: vectorized implmentation for multiple content vectors and multiple style vectors\n",
    "# -- See `00_matmul_broadcasting.ipynb` for details on how to set the shape of the tensors below\n",
    "# -- for correct vectorized implementation of \"generative\" process \n",
    "# A (a tensor of all style vectors): (S,1,1,I)\n",
    "# W (a tensor of all bilinear weights invariant to content, style classes): (K,I,J)\n",
    "# B (a tensor of all content vectors): (C, 1,1, J,1)\n",
    "\n",
    "# out = A.matmul(W) # (S x (K,1,J))\n",
    "# out2 = out.matmul(B) # (C x   (S x (K,1,1))  )\n",
    "\n",
    "styles = torch.randn((n_styles, 1,1, dim_style)) # A: each row is a style vector\n",
    "W = torch.randn((dim_x, dim_style, dim_content))\n",
    "contents = torch.randn((n_contents, 1,1, dim_content,1)) # B: each column is a content vector\n",
    "\n",
    "class TFModel(nn.Module):\n",
    "    \"Two-factor model implemented as a stack of non-linear functions via DNN\"\n",
    "    def __init__(self, styles, contents, W, \n",
    "                 n_layers=1, non_linear: Callable=nn.Identity()):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.non_linear = non_linear\n",
    "        \n",
    "        self.styles = nn.Parameter(styles) #(S,1,1,I)\n",
    "        self.contents = nn.Parameter(contents) #(C, 1,1,1, J)\n",
    "        self.W = nn.Parameter(W) #(K,I,J)\n",
    "\n",
    "        self.n_styles, _,_, self.I = styles.shape\n",
    "        self.n_contents, _,_, self.J,_ = contents.shape\n",
    "        self.K = W.shape[0]\n",
    "        \n",
    "        self.cache = {} # cache to store previous iteration's values (Eg. parameters)\n",
    "        \n",
    "    def forward(self, *, \n",
    "                s=None, c=None):\n",
    "        \"\"\"\n",
    "        s: style label; must be in {0,...,n_styles}\n",
    "        c: content label; must be in {0,..., n_contents}\n",
    "        \"\"\"\n",
    "#         assert self.styles[s].shape == (1,self.I)\n",
    "#         assert self.contents[c].shape == (1,self.J)\n",
    "        A = self.styles\n",
    "        B = self.contents\n",
    "        if s is not None:\n",
    "            A = self.styles[[s]]\n",
    "        if c is not None:\n",
    "            B = self.contents[[c]]\n",
    "        out = A.matmul(self.W)\n",
    "#         print(out.shape)\n",
    "        out = out.matmul(B)\n",
    "#         print(out.shape) #(C,S,K,1,1)\n",
    "        \n",
    "        # By convention, output tensor has size of (S,C,K)\n",
    "        # Apply sigmoid nonlinear functionn \n",
    "        # -- We choose Sigmoid because the target tensor of images will be scaled to [0,1]\n",
    "        out = self.non_linear(out.permute(1,0,2,-2,-1).squeeze())\n",
    "        \n",
    "        # todo: more layers\n",
    "        return out\n",
    "    \n",
    "    def shortname(self):\n",
    "        return f\"bilinearx{self.n_layers}_{self.non_linear}\"\n",
    "    \n",
    "    def descr(self):\n",
    "        return f\"{self.shortname()}_S:{n_styles}_I:{self.I}_C:{n_contents}_J:{self.J}_K:{self.K}\"\n",
    "        \n",
    "    def cache_params(self):\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.named_parameters():\n",
    "                self.cache[name] = param.detach().clone()\n",
    "            \n",
    "    def some_params_not_changed(self) -> bool:\n",
    "        with torch.no_grad():\n",
    "            not_changed = {}\n",
    "            for name, param in self.named_parameters():\n",
    "                if torch.equal(self.cache[name], param):\n",
    "                    d = self.cache[name] - param\n",
    "                    not_changed[name] = torch.linalg.norm(d)\n",
    "                    print(tnorm(self.cache[name]), tnorm(param))\n",
    "#                     print(tnorm(param.grad))\n",
    "            if len(not_changed) < 1:\n",
    "                return False\n",
    "            else:\n",
    "                print(f\"Not changed: \\n\", not_changed)\n",
    "                return True\n",
    "    \n",
    "    def all_params_changed(self) -> bool:\n",
    "        return not self.some_params_not_changed()\n",
    "                \n",
    "    def show_params(self):\n",
    "        with torch.no_grad():\n",
    "            visualize_vectors(self.styles.squeeze(), is_column=False, title='Styles');\n",
    "            visualize_vectors(self.contents.squeeze(), is_column=False, title='Contents');\n",
    "            visualize(self.W.permute(1,2,0), self.I, self.J, img_size, title='W', normalize=True);\n",
    "        \n",
    "    def show_grads(self):\n",
    "        with torch.no_grad():\n",
    "            visualize_vectors(self.styles.grad.squeeze(), is_column=False, title='Styles');\n",
    "            visualize_vectors(self.contents.grad.squeeze(), is_column=False, title='Contents');\n",
    "            visualize(self.W.grad.permute(1,2,0), self.I, self.J, img_size, title='W', normalize=True);\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_3dim(X: torch.Tensor, target_size: Tuple[int,int,int], dtype=torch.float32)->torch.Tensor:\n",
    "    \"\"\"\n",
    "    Rearragne data matrix X of size (n_styles*dim_x, n_contents) \n",
    "    to (n_styles, n_contents, dim_x)\n",
    "    \n",
    "    Args: \n",
    "    - X: torch.Tensor of 2dim data matrix\n",
    "    - target_size: tuple of n_style, n_contents, dim_x\n",
    "    \"\"\"\n",
    "    assert X.ndim == 2\n",
    "    n_styles, n_contents, dim_x = target_size\n",
    "    assert X.shape[0] == n_styles * dim_x\n",
    "    assert X.shape[1] == n_contents\n",
    "\n",
    "    target = torch.zeros(target_size, dtype=X.dtype)\n",
    "    \n",
    "    for s in range(n_styles):\n",
    "        for c in range(n_contents):\n",
    "            img = X[s*dim_x: (s+1)*dim_x, c]\n",
    "            target[s,c] = img\n",
    "    return target.to(dtype)\n",
    "    \n",
    "        \n",
    "def mse(out, target):\n",
    "    \"\"\"\n",
    "    Return a \n",
    "    out: a minibatch of reconstructed images: (S,C,K)\n",
    "    target: a minibatch of ground-truth images: (S,C,K)\n",
    "    \"\"\"\n",
    "    assert out.shape == target.shape\n",
    "    n_styles, n_contents, dim_x = out.shape\n",
    "    n_samples = n_stlyes * n_contents\n",
    "    return nn.MSELoss()\n",
    "\n",
    "loss_fn = nn.MSELoss()   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styles.shape,contents.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents.shape[-2:] == (dim_content,1), styles.shape[-2:] == (1,dim_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFModel(styles, contents, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, p in model.named_parameters():\n",
    "    print(f\"{name}: {p.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(0,0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model()\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore data matrix variable X as saved from the notebook \"02\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r X\n",
    "%store -r TARGET_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test create_target\n",
    "def test_create_target():\n",
    "    pass\n",
    "\n",
    "# 3 styles, 9 contents, x_dim = np.prod(TARGET_SIZE), TARGET_SIZE = (64,64,3) \n",
    "sx, n_contents = X.shape\n",
    "dim_x = np.prod(TARGET_SIZE)\n",
    "img_size = TARGET_SIZE\n",
    "n_styles = int(sx/dim_x)\n",
    "print(X.shape)\n",
    "print(\"n_styles, n_contents, dim_x: \", n_styles, n_contents, dim_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_3d = to_3dim(X, (n_styles, n_contents, dim_x) )\n",
    "X_3d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize(X, n_styles, n_contents, img_size);\n",
    "# visualize(X_3d, n_styles, n_contents, img_size);\n",
    "visualize(out.detach(), n_styles, n_contents, img_size, \n",
    "          normalize=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiled training specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(p: Path, parents=True):\n",
    "    if not p.exists():\n",
    "        p.mkdir(parents=parents)\n",
    "        print(\"Created: \", p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_exp_name(hyperparams):\n",
    "    pass\n",
    "\n",
    "# Hyperparameters\n",
    "n_styles, dim_style = 3, 3\n",
    "n_contents, dim_content = 9, 4\n",
    "img_size = (64,64,3)\n",
    "dim_x = np.prod(img_size)\n",
    "\n",
    "# Define model\n",
    "styles = torch.randn((n_styles, 1,1, dim_style)) # A: each row is a style vector\n",
    "W = torch.randn((dim_x, dim_style, dim_content))\n",
    "contents = torch.randn((n_contents, 1,1, dim_content,1)) # B: each column is a content vector\n",
    "\n",
    "model = TFModel(styles, contents, W)\n",
    "# model.show_params()\n",
    "\n",
    "# Gradient computation\n",
    "## learn_rate depending on the type of reduction on computing the MSELoss\n",
    "lrs = {'mean': 1e-2,\n",
    "      'sum': 1e-6}\n",
    "\n",
    "\n",
    "# Specify loss function and learning rate\n",
    "reduction = 'mean'\n",
    "lr = lrs[reduction]\n",
    "lr_W = lr*30\n",
    "# Optimizer\n",
    "optim_params = [\n",
    "    {'params': [model.styles, model.contents]},\n",
    "    {'params': [model.W], 'lr': lr_W}\n",
    "]\n",
    "optimizer = optim.Adam(optim_params, lr=lr)\n",
    "\n",
    "\n",
    "# Training configs\n",
    "max_epoches = 100\n",
    "print_every = 10\n",
    "show_every = 30\n",
    "\n",
    "# data\n",
    "target = to_3dim(X, (n_styles, n_contents, dim_x))\n",
    "\n",
    "# Start training\n",
    "start = time.time()\n",
    "losses = []\n",
    "for ep in range(max_epoches):\n",
    "    # Compute loss, and compute partial derivatives wrt each parameters, which will be stored \n",
    "    # in each parameter (tensor)'s `.grad` property\n",
    "    out = model()\n",
    "    loss = nn.MSELoss(reduction=reduction)(out, target) #per-dim of x (pixel)\n",
    "    \n",
    "    # Make sure all the `.grad`s of the model parameters are zero \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    \n",
    "    # Check if the parameters are changing before/after the gradient step\n",
    "    model.cache_params()\n",
    "    # Update the parameter values using the current partial derivatives based on the current loss\n",
    "    optimizer.step()\n",
    "    model.all_params_changed()\n",
    "   \n",
    "#     set_trace()\n",
    "    \n",
    "    # Log\n",
    "    with torch.no_grad():\n",
    "        if (ep+1)%print_every == 0:\n",
    "            print(f\"Ep {ep}: {loss.item()}\")\n",
    "            for n,p in model.named_parameters():\n",
    "                print(n)\n",
    "                print('\\t', tnorm(p), tnorm(p.grad))\n",
    "        if (ep+1)%show_every == 0:\n",
    "            model.show_params()\n",
    "            visualize(out, n_styles, n_contents, img_size, normalize=True);\n",
    "print(f\"Took {time.time() - start} sec. Loss: {losses[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment name\n",
    "result_dir = Path(\"../results/batch_bilinear/{model.descr()}\")\n",
    "mkdir(result_dir)\n",
    "exp_descr = f\"reduction:{reduction}_lr:{lr}_lrW:{lr_W}_ep:{ep}\"\n",
    "\n",
    "# save model parameters\n",
    "# save last reconstructions\n",
    "f_params = model.show_params()\n",
    "f_params.savefig(result_dir/f\"params_{exp_descr}\")\n",
    "with torch.no_grad():\n",
    "    out = model()\n",
    "    f_out = visualize(out, n_styles, n_contents, img_size, normalize=True);\n",
    "    f_out.savefig(result_dir/f\"xhat_{exp_descr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch2]",
   "language": "python",
   "name": "conda-env-torch2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
