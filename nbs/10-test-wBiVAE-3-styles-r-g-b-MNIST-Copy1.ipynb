{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the wBiVAE on 3 (r,g,b) MNIST dataset\n",
    "- with adv_loss_weight in [15., 45., 125., 375, 1500.]\n",
    "- with style accuracy metric\n",
    "- Jan 9, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import re\n",
    "import math\n",
    "import datetime as dt\n",
    "\n",
    "import time\n",
    "sys.dont_write_bytecode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Set, Dict, Tuple, Optional, Iterable, Mapping, Union, Callable, TypeVar\n",
    "\n",
    "from pprint import pprint\n",
    "from ipdb import set_trace as brpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from  torch.linalg import norm as tnorm\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.tuner.tuning import Tuner\n",
    "\n",
    "\n",
    "# Select Visible GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Path \n",
    "1. Add project root and src folders to `sys.path`\n",
    "2. Set DATA_ROOT to `maptile_v2` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_nb_path = Path(os.getcwd())\n",
    "ROOT = this_nb_path.parent\n",
    "SRC = ROOT/'src'\n",
    "DATA_ROOT = Path(\"/data/hayley-old/maptiles_v2/\")\n",
    "paths2add = [this_nb_path, ROOT]\n",
    "\n",
    "print(\"Project root: \", str(ROOT))\n",
    "print('Src folder: ', str(SRC))\n",
    "print(\"This nb path: \", str(this_nb_path))\n",
    "\n",
    "\n",
    "for p in paths2add:\n",
    "    if str(p) not in sys.path:\n",
    "        sys.path.insert(0, str(p))\n",
    "        print(f\"\\n{str(p)} added to the path.\")\n",
    "        \n",
    "# print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.transforms.transforms import Identity, Unnormalizer, LinearRescaler\n",
    "from src.data.transforms.functional import unnormalize\n",
    "\n",
    "from src.visualize.utils import show_timg, show_timgs, show_batch, make_grid_from_tensors\n",
    "from src.utils.misc import info, get_next_version_path\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start experiment \n",
    "Given a maptile, predict its style as one of OSM, CartoVoyager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.plmodules.vanilla_vae import VanillaVAE\n",
    "from src.models.plmodules.bilatent_vae import BiVAE\n",
    "from src.models.plmodules.three_fcs import ThreeFCs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For reproducibility, set seed like following:\n",
    "# seed = 100\n",
    "# pl.seed_everything(seed)\n",
    "# # sets seeds for numpy, torch, python.random and PYTHONHASHSEED.\n",
    "# model = Model()\n",
    "# trainer = pl.Trainer(deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial model\n",
    "\n",
    "TODO:\n",
    "\n",
    "---\n",
    "### For batch in dataloader:\n",
    "- x: (BS, C, h, w): a mini-batch of (c,h,w) tensor\n",
    "\n",
    "### mu, log_var = model.encoder(x) \n",
    "- mu: (BS, latent_dim)\n",
    "- log_var: (BS, latent_dim)\n",
    "\n",
    "### z = model.rsample(mu, log_var, self.n_samples) \n",
    "- z: (BS, n_samples, latent_dim)\n",
    "-`z[n]` constains `n_samples` number of latent codes, sampled from the same distribution `N(mu[n], logvar[n])`\n",
    " \n",
    "### recon = model.decoder(z) \n",
    "- recon: (BS, n_samples, c, h, w)\n",
    "- `recon[n]` contains `n_samples` number of (c,h,w)-sized $mu_{x}$, corresponding to the center of the factorized Gaussian for the latent code $z^{(n,l)}$ ($l$th z_sample from $N(\\mu[n], logvar[n])$, ie. $\\mu_{x}^{(n,l)}$\n",
    "\n",
    "### out = model.forward(x)\n",
    "- out (dict): keys are \"mu\", \"logvar\", \"recon\"\n",
    "\n",
    "### loss_dict = loss_function(out, x, self.n_samples)\n",
    "- loss_dict (dict): keys are \"loss\", \"kl\", \"recon_loss\"\n",
    "- kl is computed the same way as in the Vanillia_VAE model's `loss_function`\n",
    "- recon_loss is a generalized version with `self.n_samples` (>=1) number of samples to estimated each datapoint's MSE_loss as the average over the loss's from the `n_samples` number of $z_{n,l}$ samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.datamodules import MNISTDataModule, MNISTMDataModule, MonoMNISTDataModule\n",
    "from src.data.datamodules import MultiMonoMNISTDataModule\n",
    "from src.models.plmodules.bilatent_vae import BiVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataModule configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init MNIST-M DataModule\n",
    "# data_root = ROOT/'data'\n",
    "# in_shape = (3,32,32)\n",
    "# batch_size = 32\n",
    "# dm = MNISTMDataModule(\n",
    "#     data_root=data_root, \n",
    "#     in_shape=in_shape,\n",
    "#     batch_size=batch_size)\n",
    "# dm.setup('fit')\n",
    "# # show_batch(dm, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Init Monochrome datamodule\n",
    "# color='green'\n",
    "# n_styles = 1\n",
    "# colorstr_transform = transforms.Lambda(lambda color_name: 0)\n",
    "# dm = MonoMNISTDataModule(\n",
    "#     data_root=Path('/data/hayley-old/Tenanbaum2000/data/Mono-MNIST/'),\n",
    "#     color=color,\n",
    "#     seed=123,\n",
    "#     colorstr_transform=colorstr_transform,\n",
    "#     in_shape=in_shape,\n",
    "#     batch_size=batch_size\n",
    "# )\n",
    "# dm.setup('fit')\n",
    "\n",
    "# # Show a batch\n",
    "# dl = dm.train_dataloader()\n",
    "# sample = next(iter(dl))\n",
    "# x = sample['img']\n",
    "# c_label = sample['digit']\n",
    "# s_label = sample['color']\n",
    "# show_timgs(x)\n",
    "# print(\"Content labels: \", c_label) #digit-id\n",
    "# print(\"Style labels: \", s_label) #color\n",
    "# print(\"===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Multisource-Monochrome MNIST datamodule\n",
    "mono_dir = ROOT/'data/Mono-MNIST'\n",
    "colors = ['red', 'green', 'blue']\n",
    "seed = 123\n",
    "in_shape = (3,32,32)\n",
    "batch_size = 128\n",
    "\n",
    "# Create a multi-source dataset\n",
    "dm = MultiMonoMNISTDataModule(\n",
    "    data_root=mono_dir,\n",
    "    colors=colors,\n",
    "    seed=seed,\n",
    "    in_shape=in_shape,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "dm.setup('fit')\n",
    "# dm.setup('test')\n",
    "# Show a batch\n",
    "dl = dm.train_dataloader()\n",
    "sample = next(iter(dl))\n",
    "x = sample['img']\n",
    "c_label = sample['digit']\n",
    "s_label = sample['color']\n",
    "show_timgs(x)\n",
    "print(\"Content labels: \", c_label) #digit-id\n",
    "print(\"Style labels: \", s_label) #color\n",
    "print(\"===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init plModule\n",
    "latent_dim = 10\n",
    "hidden_dims = [32,64,128,256]#,512]\n",
    "lr = 3e-3\n",
    "act_fn = nn.ReLU()\n",
    "# Specific for BiVAE\n",
    "adversary_dims = [32,32,32] \n",
    "is_contrasive = True # If true, use adv. loss from both content and style codes. Else just style codes\n",
    "kld_weight = 1.0 # vae_loss = recon_loss + kld_weight * kld_weight\n",
    "adv_loss_weight = 45. #[15., 45., 125., 375, 1500.] # loss = vae_loss + adv_loss_weight * adv_loss\n",
    "\n",
    "model = BiVAE(\n",
    "    in_shape=dm.size(), \n",
    "    n_styles=dm.n_styles,\n",
    "    latent_dim=latent_dim,\n",
    "    hidden_dims=hidden_dims,\n",
    "    adversary_dims=adversary_dims,\n",
    "    learning_rate=lr, \n",
    "    act_fn=act_fn,\n",
    "    size_average=False,\n",
    "    is_contrasive=is_contrasive,\n",
    "    kld_weight=kld_weight,\n",
    "    adv_loss_weight=adv_loss_weight,\n",
    ")\n",
    "\n",
    "# model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start LR:  0.001\n",
      "81.50306701660156\n",
      "Updated LR:  0.001\n",
      "===\n",
      "Start LR:  0.001\n",
      "72.26151275634766\n",
      "Updated LR:  0.001\n",
      "===\n",
      "Start LR:  0.001\n",
      "64.0684585571289\n",
      "Updated LR:  0.001\n",
      "===\n",
      "Start LR:  0.001\n",
      "56.8049430847168\n",
      "Epoch     4: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Updated LR:  0.0001\n",
      "===\n",
      "Start LR:  0.0001\n",
      "50.365509033203125\n",
      "Updated LR:  0.0001\n",
      "===\n",
      "Start LR:  0.0001\n",
      "49.77915573120117\n",
      "Updated LR:  0.0001\n",
      "===\n",
      "Start LR:  0.0001\n",
      "49.199642181396484\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Updated LR:  1e-05\n",
      "===\n",
      "Start LR:  1e-05\n",
      "48.62688446044922\n",
      "Updated LR:  1e-05\n",
      "===\n",
      "Start LR:  1e-05\n",
      "48.57012176513672\n",
      "Updated LR:  1e-05\n",
      "===\n",
      "Start LR:  1e-05\n",
      "48.513431549072266\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Updated LR:  1.0000000000000002e-06\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "# Add learning rate schedulers\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "model = nn.Linear(1,1)\n",
    "x = torch.tensor(range(10), dtype=torch.float32).reshape((10,1))\n",
    "y = 2*x\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer, 'min', \n",
    "                                 patience=2, verbose=True)\n",
    "for ep in range(10):\n",
    "    print(\"Start LR: \", optimizer.param_groups[0]['lr'])\n",
    "    out = model(x)\n",
    "    loss = nn.MSELoss()(out, y)\n",
    "    \n",
    "    print(loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "#     lr_scheduler.step(loss)\n",
    "    lr_scheduler.step(0.1)\n",
    "\n",
    "    print(\"Updated LR: \", optimizer.param_groups[0]['lr'])\n",
    "    print(\"===\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Callbacks\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from src.callbacks.hist_logger import HistogramLogger\n",
    "from src.callbacks.recon_logger import ReconLogger\n",
    "\n",
    "# Model wrapper from graph viz\n",
    "from src.models.model_wrapper import ModelWrapper\n",
    "\n",
    "callbacks = [\n",
    "#         HistogramLogger(hist_epoch_interval=1),\n",
    "#         ReconLogger(recon_epoch_interval=1),\n",
    "#         EarlyStopping('val_loss', patience=10),\n",
    "]\n",
    "\n",
    "# Start the experiment\n",
    "today_str = dt.datetime.today().strftime('%Y-%m-%d')\n",
    "exp_name = f'{model.name}_{dm.name}'\n",
    "tb_logger = pl_loggers.TensorBoardLogger(save_dir=f'{ROOT}/temp-logs/{today_str}', \n",
    "                                         name=exp_name,\n",
    "                                         log_graph=False,\n",
    "                                        default_hp_metric=False)\n",
    "print(\"Log dir: \", tb_logger.log_dir)\n",
    "\n",
    "log_dir = Path(tb_logger.log_dir)\n",
    "if not log_dir.exists():\n",
    "    log_dir.mkdir(parents=True)\n",
    "    print(\"Created: \", log_dir)\n",
    "    \n",
    "\n",
    "# Log computational graph\n",
    "# model_wrapper = ModelWrapper(model)\n",
    "# tb_logger.experiment.add_graph(model_wrapper, model.example_input_array.to(model.device))\n",
    "# tb_logger.log_graph(model)\n",
    "\n",
    "trainer_config = {\n",
    "    'gpus':1,\n",
    "    'max_epochs': 200,\n",
    "    'progress_bar_refresh_rate':20,\n",
    "#     'auto_lr_find': True,\n",
    "    'terminate_on_nan':True,\n",
    "#     'num_sanity_val_steps':0.25,\n",
    "    'check_val_every_n_epoch':10,\n",
    "    'logger':tb_logger,\n",
    "#     'callbacks':callbacks,\n",
    "}\n",
    "\n",
    "# \n",
    "# trainer = pl.Trainer(fast_dev_run=3)\n",
    "trainer = pl.Trainer(**trainer_config)\n",
    "# trainer.tune(model=model, datamodule=dm)\n",
    "\n",
    "# Start exp\n",
    "# Fit model\n",
    "trainer.fit(model, dm)\n",
    "print(f\"Finished at ep {trainer.current_epoch, trainer.batch_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.current_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log  hparmeters and `best_score` to tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = model.hparams.copy()\n",
    "hparams.update(dm.hparams)\n",
    "best_score = trainer.checkpoint_callback.best_model_score.item()\n",
    "metrics = {'hparam/best_score': best_score} #todo: define a metric and use it here\n",
    "pprint(hparams)\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pl.Logger's method \"log_hyperparameters\" which handles the \n",
    "# hparams' element's formats to be suitable for Tensorboard logging\n",
    "# See: \n",
    "# https://sourcegraph.com/github.com/PyTorchLightning/pytorch-lightning@be3e8701cebfc59bec97d0c7717bb5e52afc665e/-/blob/pytorch_lightning/loggers/tensorboard.py#explorer:~:text=def%20log_hyperparams\n",
    "best_score = trainer.checkpoint_callback.best_model_score.item()\n",
    "metrics = {'hparam/best_score': best_score} #todo: define a metric and use it here\n",
    "trainer.logger.log_hyperparams(hparams, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " TODO:\n",
    " OPTIMIZER\n",
    " def configure_optimizers(self):\n",
    "        #TODO: ADD optimizer for discriminator\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.get(\"learning_rate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: \n",
    "Showing the changes in the scores based on c and scores based on s will be super intersting to see as the model learns!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.plmodules.utils import get_best_ckpt, load_model, load_best_model\n",
    "from pytorch_lightning.utilities.cloud_io import load as pl_load\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load best model recorded during the training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = get_best_ckpt(model,verbose=True)\n",
    "ckpt = pl_load(ckpt_path, map_location=lambda storage, loc: storage)  # dict object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(ckpt['state_dict'])\n",
    "# load_best_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_recon(model, \n",
    "               tb_logger=None,\n",
    "               global_step:int=0,\n",
    "               unnorm:bool=True, \n",
    "               to_show:bool=True, \n",
    "               verbose:bool=False):\n",
    "    model.eval()\n",
    "    dm = model.trainer.datamodule\n",
    "    cmap = 'gray' if dm.size()[0] ==1 else None\n",
    "    train_mean, train_std = dm.train_mean, dm.train_std\n",
    "    with torch.no_grad():\n",
    "        for mode in ['train', 'val']:\n",
    "            dl = getattr(model, f\"{mode}_dataloader\")()\n",
    "            batch = next(iter(dl))\n",
    "            \n",
    "            x = batch['img']\n",
    "            label_c = batch['digit']  # digit/content label (int) -- currently not used\n",
    "            label_s = batch['color']\n",
    "            x = x.to(model.device)\n",
    "            x_recon = model.generate(x)\n",
    "            \n",
    "            # Move to cpu for visualization\n",
    "            x = x.cpu()\n",
    "            x_recon = x_recon.cpu()\n",
    "            \n",
    "            if verbose: \n",
    "                info(x, f\"{mode}_x\")\n",
    "                info(x_recon, f\"{mode}_x_recon\")\n",
    "                \n",
    "            if unnorm:\n",
    "                x_unnormed = unnormalize(x, train_mean, train_std)\n",
    "                x_recon_unnormed = unnormalize(x_recon, train_mean, train_std)\n",
    "                if verbose:\n",
    "                    print(\"===After unnormalize===\")\n",
    "                    info(x_unnormed, f\"{mode}_x_unnormed\")\n",
    "                    info(x_recon_unnormed, f\"{mode}_x_recon_unnormed\")\n",
    "                    \n",
    "            if to_show:\n",
    "                _x = x_unnormed if unnorm else x\n",
    "                _x_recon = x_recon_unnormed if unnorm else x_recon\n",
    "                show_timgs(_x, title=f\"Input: {mode}\", cmap=cmap)\n",
    "#                 show_timgs(_x_recon, title=f\"Recon: {mode}\", cmap=cmap)\n",
    "                show_timgs(LinearRescaler()(_x_recon), title=f\"Recon(linearized): {mode}\", cmap=cmap)\n",
    "\n",
    "            # Log input-recon grid to TB\n",
    "            if tb_logger is not None:\n",
    "                input_grid = torchvision.utils.make_grid(x_unnormed) # (C, gridh, gridw)\n",
    "                recon_grid = torchvision.utils.make_grid(x_recon_unnormed) # (C, gridh, gridw)\n",
    "                normed_recon_grid = torchvision.utils.make_grid(LinearRescaler()(x_recon_unnormed))\n",
    "                \n",
    "                grid = torch.cat([input_grid, normed_recon_grid], dim=-1) #inputs | recons\n",
    "                tb_logger.experiment.add_image(f\"{mode}/recons\", grid, global_step=global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_recon(model, tb_logger, global_step=1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separation of content vs. style in the latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Case 1: Take two inputs of same content (digit id) and very different colorization styles\n",
    "- Case 2: Take two inputs of different contents (eg. digit 0 and digit 5)\n",
    "    - First, pass the first image to encoder -> get c^(1) and s^(1)\n",
    "    - Now pass the second image to encoder -> get c^(2) and s^(2)\n",
    "    - recon1 = decoder(z=[c^(1), s^(1)])\n",
    "    - recon2 = decoder(z=[c^(2), s^(1)])\n",
    "    - ---\n",
    "    - recon3 = decoder(z=[c^(1), s^(2)])\n",
    "    - recon4 = decoder(z=[c^(2), s^(2)])\n",
    "---\n",
    "Do it for a dataset from multiple styles\n",
    "- Create a concatenated dataseta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect one image per digit id to create a fixed evaluation image set\n",
    "Save the test image set to compare different models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler\n",
    "# See https://github.com/pytorch/pytorch/blob/master/torch/utils/data/sampler.py\n",
    "\n",
    "class DeterministicSampler(Sampler):\n",
    "\n",
    "    def __init__(self, dataset, seed:int):\n",
    "        self.dataset = dataset\n",
    "        self.seed = seed\n",
    "        self.generator = torch.Generator()\n",
    "        self.generator.manual_seed(self.seed)\n",
    "        self.indices = torch.randperm(len(self.dataset), generator=self.generator).tolist()\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield from torch.randperm(len(self.dataset), generator=self.generator).tolist()\n",
    "#         return iter(self.indices) #  yield from iter(self.indices)\n",
    "\n",
    "def test_deterministic_sampler():\n",
    "    seeded_sampler = DeterministicSampler(ds, seed=10)\n",
    "    for i in range(10):\n",
    "        ind = next(iter(seeded_sampler))\n",
    "        print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_reps(dl: DataLoader) -> Dict[Union[str,int], torch.Tensor]:\n",
    "    class_reps = {}\n",
    "    for i in range(len(dl.dataset)):\n",
    "        batch = dl.dataset[i]\n",
    "        x = batch['img']\n",
    "        label_c = batch['digit']  # digit/content label (int) -- currently not used\n",
    "        label_s = batch['color']\n",
    "\n",
    "        if len(class_reps) >= 10:\n",
    "            break\n",
    "        if isinstance(label_c, torch.Tensor):\n",
    "            label_c = label_c.item()\n",
    "        label_c = str(label_c)\n",
    "        if label_c in class_reps:\n",
    "            continue\n",
    "        class_reps[label_c] = x\n",
    "    return class_reps\n",
    "\n",
    "def create_mnistm_class_reps(to_save=True) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"todo: make it compatible with other datamodules\"\"\"\n",
    "    mode = 'train'\n",
    "    dm = MNISTMDataModule(data_root=data_root, in_shape=in_shape, batch_size=36,\n",
    "                         shuffle=True)\n",
    "    dm.setup()\n",
    "    train_mean, train_std = dm.train_mean, dm.train_std\n",
    "\n",
    "    dl = getattr(dm, f\"{mode}_dataloader\")()\n",
    "    class_reps = get_class_reps(dl)\n",
    "\n",
    "    # Show original digit images (one for each class, ie. \"representatives\" of each digit id)\n",
    "    timgs = torch.stack(tuple(class_reps.values()))\n",
    "    unnormed = unnormalize(timgs, dm.train_mean, dm.train_std)\n",
    "    show_timgs(unnormed, nrows=2)\n",
    "    print(class_reps.keys())\n",
    "    \n",
    "    if to_save:\n",
    "        out_dir = ROOT/'data/test-transfers'\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        out_path = out_dir/f'class_reps_{now2str()}'\n",
    "        print(\"Saved class_reps to: \", out_path)\n",
    "        joblib.dump(class_reps, out_path)\n",
    "    return class_reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_reps = create_mnistm_class_reps()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'train'\n",
    "dl = getattr(dm, f\"{mode}_dataloader\")()\n",
    "class_reps = get_class_reps(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1: constant content code $c$, varying style code $s^{(i)} \\in \\{s^{(1)}, \\dots \\}$\n",
    "  - $z = [c, s^{(i)}]$ --> decoder --> $\\mu^{x^{(i)}}_{pred}$ (aka. recon)\n",
    "\n",
    "First, we fix the content code to that of a constant input (eg. an ith datapt in the training set),\n",
    "and use various style codes (eg. any $j \\neq i$th datapt). We combine the (constant) content code with each of the style code and see the output of the decoder (ie. `mu_x_pred`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct each digit-representative image by \n",
    "# interlacing a single content code with various style codes\n",
    "linearlize = True\n",
    "model.eval()\n",
    "train_mean, train_std = dm.train_mean, dm.train_std\n",
    "ids = [str(i) for i in range(10)]\n",
    "grids = {} \n",
    "for i, id_a in enumerate(ids):\n",
    "    \n",
    "    grids[id_a] = []\n",
    "    for j, id_b in enumerate(ids):\n",
    "\n",
    "        img_a = class_reps[id_a]\n",
    "        img_b = class_reps[id_b]\n",
    "        img_pair = torch.stack([img_a, img_b], dim=0)\n",
    "        unnormed_img_pair = unnormalize(img_pair, train_mean, train_std)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            dict_qparams = model.encode(img_pair)\n",
    "            dict_z = model.rsample(dict_qparams)\n",
    "    #         pprint(dict_z)\n",
    "\n",
    "            # Fix content to c[0]\n",
    "            content = dict_z[\"c\"][[0]]\n",
    "            style = dict_z[\"s\"][[1]]\n",
    "            test_dict_z = {\"c\": content, \"s\": style}\n",
    "    #         pprint(test_dict_z)\n",
    "\n",
    "            # Reconstruct\n",
    "            z = model.combine_content_style(test_dict_z)\n",
    "            recons = model.decode(z)\n",
    "\n",
    "            # Optional: for better viz, unnormalize or/and linearlize\n",
    "            unnormed_recons = unnormalize(recons, train_mean, train_std)\n",
    "            if linearlize:\n",
    "                img_pair = LinearRescaler()(img_pair)\n",
    "                unnormed_recons = LinearRescaler()(unnormed_recons)\n",
    "#             info(recons, 'recons')\n",
    "#             info(unnormed_recons, 'unnormed_recons')    \n",
    "            \n",
    "            grid = torchvision.utils.make_grid(\n",
    "                torch.cat([unnormed_img_pair,unnormed_recons], dim=0)\n",
    "            ) # (3, gridh, gridw)\n",
    "            grids[id_a].append(grid)\n",
    "\n",
    "# Concatenate the grids to make a single grid by putting each grid in row dim(ie. dim=1)    #log_dir/content_transfers/version_x\n",
    "# -- Optionally, save the image results    \n",
    "log_dir = Path(model.logger.log_dir)\n",
    "save_dir = get_next_version_path(log_dir, name='content_transfers') \n",
    "save_dir.mkdir()\n",
    "print(\"Created: \", save_dir)\n",
    "\n",
    "for id_a, recons in grids.items():\n",
    "    recons = torch.cat(recons, dim=1)\n",
    "    save_path = save_dir/f\"content_transfers_{id_a}.png\"\n",
    "    show_timg(recons, \n",
    "              title=id_a, \n",
    "              save_path=save_path,\n",
    "             )\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: fix the style code, and apply the style to various contents.\n",
    "We have a single stlye code provider, and multiple content providers.\n",
    "- We first extract the style code $s$ from the style provider\n",
    "- Then, for each content provider x^{(i)}, extract the content code $c^{(i)}$\n",
    "- Combine the constant style code with each content code and pass into the decoder.\n",
    "\n",
    "Ideal result will show that each reconstructed image preserves the content of each content provider, while stylizing the content in the style of the style provider.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viz options\n",
    "linearlize = True\n",
    "\n",
    "# Reconstruct each digit-representative image by \n",
    "# interlacing a single content code with various style codes\n",
    "model.eval()\n",
    "ids = [str(i) for i in range(10)]\n",
    "grids = {} \n",
    "for i, id_a in enumerate(ids):\n",
    "    \n",
    "    grids[id_a] = []\n",
    "    for j, id_b in enumerate(ids):\n",
    "\n",
    "        img_a = class_reps[id_a]\n",
    "        img_b = class_reps[id_b]\n",
    "        img_pair = torch.stack([img_a, img_b], dim=0)\n",
    "        unnormed_img_pair = unnormalize(img_pair, train_mean, train_std)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            dict_qparams = model.encode(img_pair)\n",
    "            dict_z = model.rsample(dict_qparams)\n",
    "    #         pprint(dict_z)\n",
    "\n",
    "            # Fix style to s[0]\n",
    "            style = dict_z[\"s\"][[0]]\n",
    "            content = dict_z[\"c\"][[1]]\n",
    "            test_dict_z = {\"c\": content, \"s\": style}\n",
    "\n",
    "            # Reconstruct\n",
    "            z = model.combine_content_style(test_dict_z)\n",
    "            recons = model.decode(z)\n",
    "            \n",
    "            # Optional: for better viz, unnormalize or/and linearlize\n",
    "            unnormed_recons = unnormalize(recons, train_mean, train_std)\n",
    "            if linearlize:\n",
    "                img_pair = LinearRescaler()(img_pair)\n",
    "                unnormed_recons = LinearRescaler()(unnormed_recons)\n",
    "#             info(recons, 'recons')\n",
    "#             info(unnormed_recons, 'unnormed_recons')    \n",
    "            \n",
    "            # Make a grid of [input pairs, reconstructed image of the mixed codes]\n",
    "            grid = torchvision.utils.make_grid(\n",
    "                torch.cat([unnormed_img_pair,unnormed_recons], dim=0)\n",
    "            ) # (3, gridh, gridw)\n",
    "            grids[id_a].append(grid)\n",
    "\n",
    "# Concatenate the grids to make a single grid by putting each grid in row dim(ie. dim=1)    #log_dir/content_transfers/version_x\n",
    "# -- Optionally, save the image results    \n",
    "log_dir = Path(model.logger.log_dir)\n",
    "save_dir = get_next_version_path(log_dir, name='style_transfers') \n",
    "save_dir.mkdir()\n",
    "print(\"Created: \", save_dir)\n",
    "\n",
    "for id_a, recons in grids.items():\n",
    "    recons = torch.cat(recons, dim=1)\n",
    "    save_path = save_dir/f\"style_transfers_{id_a}.png\"\n",
    "    show_timg(recons, \n",
    "              title=id_a, \n",
    "              save_path=save_path,\n",
    "             )\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo\n",
    "- [x] Get the version code from pl.log\n",
    "- [ ] Make logger for loss_c and loss_s\n",
    "- [ ] Impl. evaluation code\n",
    "\n",
    "\n",
    "Here\n",
    "- [ ] Impl. code for latent space traversal\n",
    "- [ ] Look at the embeddings based on z_c\n",
    "  - any clusters? -- run kmeans\n",
    "    - use digit id as labels in TB Projector\n",
    "- [ ] Look at the embeddings based on z_s\n",
    "  - any clusters? does it match clusters based on style class? \n",
    "    - use style id as labels\n",
    "    \n",
    "---\n",
    "Experiment adv vae model on maptiles\n",
    "- [ ] Train a model with maptiles w/ one style\n",
    "- [ ] Train a model with maptiles w/ two style (eg. tonerbackground, cartolight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent space \n",
    "- Embeddings (cluster, Nearest neighbor)\n",
    "  - Content space: clustering, NNeighbor queries\n",
    "  - Style space: clustering, NNeighbor queries\n",
    "- Distribution of qc's parameters\n",
    "- Distribution of qs's parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize embeddings\n",
    "- collect a batch of inputs -> encoder -> [mu, log_var] -> sample -> a batch of z's (embeddings)\n",
    "- use tb logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = trainer.checkpoint_callback.best_model_path\n",
    "ckpt = pl_load(ckpt_path, map_location=lambda storage, loc: storage) #dict object\n",
    "model.load_state_dict(ckpt['state_dict'])\n",
    "best_global_step = ckpt['global_step']\n",
    "for k,v in ckpt.items():\n",
    "    if 'state' in k:\n",
    "        continue\n",
    "    pprint(f\"{k}:{v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "# best_epoch = ckpt[\"epoch\"]\n",
    "\n",
    "ds = model.train_dataloader().dataset\n",
    "dl = DataLoader(ds, batch_size=1028)\n",
    "with torch.no_grad():\n",
    "#     x, y = next(iter(model.train_dataloader()))\n",
    "    x, y = next(iter(dl))\n",
    "    dict_qparams = model.encode(x)\n",
    "    dict_z = model.rsample(dict_qparams)\n",
    "    \n",
    "#     z = out['z']\n",
    "    \n",
    "    # log embedding of z_c to tensorboard \n",
    "    writer = model.logger.experiment\n",
    "    writer.add_embedding(dict_z['c'],\n",
    "                         label_img=LinearRescaler()(x), \n",
    "                         metadata=y.tolist(),\n",
    "                         global_step=best_global_step, #todo\n",
    "                         tag=\"z_c\"\n",
    "                        )\n",
    "    \n",
    "    # log embedding of z_s to tensorboard \n",
    "    writer = model.logger.experiment\n",
    "    writer.add_embedding(dict_z['s'],\n",
    "                         label_img=LinearRescaler()(x), \n",
    "                         metadata=y.tolist(),\n",
    "                         global_step=best_global_step, #todo\n",
    "                         tag=\"z_s\"\n",
    "                        )\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "todo:\n",
    "    project many more (Eg. the whole dataset) to embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize original images of the close neighbors in the latent space\n",
    "- Compute pairwise distance using cosine similarity\n",
    "- For each row (ie. a latent code), get the index of the smallest values. \n",
    "- Select the images in the batch x and visualize (can do this all in show_timgs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "mode = 'train'\n",
    "query_size = 64 #1024\n",
    "metric = 'cosine' #pairwise distance metric in content space\n",
    "ds = getattr(dm, f\"{mode}_ds\")\n",
    "dl = DataLoader(ds, batch_size=query_size, shuffle=True)\n",
    "with torch.no_grad():\n",
    "    batch = next(iter(dl))\n",
    "    \n",
    "    x = batch['img']\n",
    "    y = batch['digit']  # digit/content label (int) -- currently not used\n",
    "#     label_s = batch['color']\n",
    "    dict_qparams = model.encode(x)\n",
    "    dict_z = model.rsample(dict_qparams)\n",
    "    c = dict_z['c']\n",
    "    s = dict_z['s']\n",
    "    z = model.combine_content_style(dict_z)\n",
    "\n",
    "    for name, embedding in zip([\"c\", \"s\", \"z\"], [c,s,z]):\n",
    "        # Compute pairwise distance of the embeddings\n",
    "        pdists = pairwise_distances(embedding.numpy(), metric=metric)\n",
    "        plt.imshow(pdists, cmap='gray')\n",
    "        plt.title(f\"Pairwise dists of {name}'s\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        # smaller values means closer in distance\n",
    "        n_ngbrs = 10\n",
    "        n_rows = query_size #100\n",
    "\n",
    "        selected_rows = range(len(x)) #np.random.choice(len(x), size=n_rows)\n",
    "        for idx in selected_rows:\n",
    "            args = np.argsort(pdists[idx])[:n_ngbrs]\n",
    "    #         print(args)\n",
    "            show_timgs(LinearRescaler()(x[args]), cmap='gray', factor=2, \n",
    "                       nrows=1, title=f'Nearest of digit {y[idx].item()}: {name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent partition via. distribution of q's parameters\n",
    "1. Distribution of qc's parameters per digit_id\n",
    "2. Distribution of qs's parameters per style label\n",
    "\n",
    "Step:\n",
    "- Input a batch of input images of the same content (ie. digit_id), \n",
    "- Get the distribution of its $mu^{(c)}_j$'s at each of its dimensions $j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualize.utils import get_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Collect a batch of images of the same content\n",
    "# digit_id = 0\n",
    "# n_samples = 128\n",
    "# xs = []\n",
    "# n_collected = 0\n",
    "# while n_collected <= n_samples:\n",
    "#     (x,y) = next(iter(dl))\n",
    "#     selected = x[y==digit_id]\n",
    "#     xs.append(selected)\n",
    "#     n_collected += len(selected)\n",
    "# xs = torch.cat(xs, dim=0)\n",
    "# print(f\" Digit {digit_id} collected. Input: {xs.shape}\")\n",
    "\n",
    "# # Set output dir\n",
    "# out_dir = model.logger.log_dir/\"dist-qparams\"\n",
    "\n",
    "# # Plot the distributions of qc parameters and qs parameters\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     dict_qparams = model(xs)\n",
    "    \n",
    "#     # Dist. of each content dim's parameters\n",
    "#     mu_qc, var_qc = dict_qparams['mu_qc'], dict_qparams['logvar_qc'].exp() #(BS, content_dim), (BS, style_dim)\n",
    "#     f, ax = plt.subplots(1, model.content_dim, figsize=(20,2))\n",
    "#     title = f\"Digit {digit_id}: \" + r\"Dist. of $mu^{c}_j$\"\n",
    "#     f.suptitle(title)\n",
    "#     for j in range(model.content_dim):\n",
    "#         ax[j].hist(mu_qc[:,j])\n",
    "#         ax[j].set_xlim((-4,4))\n",
    "#         ax[j].set_title(f\"dim {j}\")\n",
    "#     f.tight_layout()\n",
    "#     plt.show()\n",
    "#     f.savefig(out_dir/f\"digit-{digit_id}-mu_qc.png\" \n",
    "    \n",
    "#     # Dist. of each style dim's parameters\n",
    "#     mu_qs, var_qs = dict_qparams['mu_qs'], dict_qparams['logvar_qs'].exp()\n",
    "#     f, ax = plt.subplots(1, model.style_dim, figsize=(20,2))\n",
    "#     title = f\"Digit {digit_id}: \" + r\"Dist. of $mu^{s}_j$\"\n",
    "#     f.suptitle(title)\n",
    "#     for j in range(model.style_dim):\n",
    "#         ax[j].hist(mu_qs[:,j])\n",
    "#         ax[j].set_xlim((0,2))\n",
    "#         ax[j].set_title(f\"dim {j}\")\n",
    "#     f.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_qc.max().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_xlim = (-4., 4.)\n",
    "# var_xlim = (0, 0.05)\n",
    "n_samples = 1024\n",
    "# Set output dir\n",
    "out_dir = Path(model.logger.log_dir)/\"dist_qparams\"\n",
    "if not out_dir.exists():\n",
    "    out_dir.mkdir(parents=True)\n",
    "    print(\"Created and saving to: \", out_dir)\n",
    "    \n",
    "for digit_id in range(10):\n",
    "    # Collect a batch of images of the same content\n",
    "    xs = []\n",
    "    n_collected = 0\n",
    "    while n_collected <= n_samples:\n",
    "        (x,y) = next(iter(dl))\n",
    "        selected = x[y==digit_id]\n",
    "        xs.append(selected)\n",
    "        n_collected += len(selected)\n",
    "    xs = torch.cat(xs, dim=0)\n",
    "    print(f\" Digit {digit_id} collected. Input: {xs.shape}\")\n",
    "\n",
    "\n",
    "    # Plot the distributions of qc parameters and qs parameters\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dict_qparams = model(xs)\n",
    "\n",
    "        # Dist. of each content dim's parameters\n",
    "        mu_qc, var_qc = dict_qparams['mu_qc'], dict_qparams['logvar_qc'].exp() #(BS, content_dim), (BS, style_dim)\n",
    "        # -- mu_qc's\n",
    "#         mu_xlim = (0, max(mu_qc)\n",
    "        f, ax = plt.subplots(1, model.content_dim, figsize=(20,2))\n",
    "        title = f\"Digit {digit_id}: \" + r\"Dist. of $mu^{c}_j$\"\n",
    "        f.suptitle(title)\n",
    "        for j in range(model.content_dim):\n",
    "            ax[j].hist(mu_qc[:,j])\n",
    "            ax[j].set_xlim(mu_xlim)\n",
    "            ax[j].set_title(f\"dim {j}\")\n",
    "        f.tight_layout()\n",
    "        f.savefig(out_dir/f\"digit-{digit_id}-mu_qc.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        # -- var_qc's\n",
    "        var_xlim = (0, var_qc.max().item())\n",
    "        f, ax = plt.subplots(1, model.style_dim, figsize=(20,2))\n",
    "        title = f\"Digit {digit_id}: \" + r\"Dist. of $var^{c}_j$\"\n",
    "        f.suptitle(title)\n",
    "        for j in range(model.content_dim):\n",
    "            ax[j].hist(var_qc[:,j])\n",
    "            ax[j].set_xlim(var_xlim)\n",
    "            ax[j].set_title(f\"dim {j}\")\n",
    "        f.tight_layout()\n",
    "        f.savefig(out_dir/f\"digit-{digit_id}-var_qc.png\")\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        # Dist. of each style dim's parameters\n",
    "        mu_qs, var_qs = dict_qparams['mu_qs'], dict_qparams['logvar_qs'].exp()\n",
    "        # -- mu_qs's\n",
    "#         mu_xlim = (0, max(mu_qs))\n",
    "        f, ax = plt.subplots(1, model.style_dim, figsize=(20,2))\n",
    "        title = f\"Digit {digit_id}: \" + r\"Dist. of $mu^{s}_j$\"\n",
    "        f.suptitle(title)\n",
    "        for j in range(model.style_dim):\n",
    "            ax[j].hist(mu_qs[:,j])\n",
    "            ax[j].set_xlim(mu_xlim)\n",
    "            ax[j].set_title(f\"dim {j}\")\n",
    "        f.tight_layout()\n",
    "        f.savefig(out_dir/f\"digit-{digit_id}-mu_qs.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        # -- var_qs's\n",
    "        var_xlim = (0, var_qs.max().item())\n",
    "        f, ax = plt.subplots(1, model.style_dim, figsize=(20,2))\n",
    "        title = f\"Digit {digit_id}: \" + r\"Dist. of $var^{s}_j$\"\n",
    "        f.suptitle(title)\n",
    "        for j in range(model.style_dim):\n",
    "            ax[j].hist(var_qs[:,j])\n",
    "            ax[j].set_xlim(var_xlim)\n",
    "            ax[j].set_title(f\"dim {j}\")\n",
    "        f.tight_layout()\n",
    "        f.savefig(out_dir/f\"digit-{digit_id}-var_qs.png\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(var_qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test BiVAE implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- [ ] Check output sizes of BiVAE's \n",
    "    - [x] encode\n",
    "    - [x] rsample\n",
    "    - [x] combine_content_and_style\n",
    "    - [x] decode\n",
    "    - [x] forward\n",
    "- [ ] Check losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(dm.train_dataloader()))\n",
    "info(x), info(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check `encode` and `rsample`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_qparams = model.encode(x)\n",
    "for k,v in dict_qparams.items():\n",
    "    print(f\"\\n{k}:  {v.shape}\")\n",
    "    if 'mu' in k:\n",
    "        print(v[0])\n",
    "    else:\n",
    "        print(v[0].exp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_z = model.rsample(dict_qparams)\n",
    "for k,v in dict_z.items():\n",
    "    print(f\"\\n{k}:  {v.shape}\")\n",
    "    print(v[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check `combine_content_style` and `decode`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = model.combine_content_style(dict_z)\n",
    "assert z.shape == (batch_size, latent_dim)\n",
    "print(\"z shape: \", z.shape) #(BS, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_x_pred = model.decode(z)\n",
    "assert mu_x_pred.shape == (batch_size, *in_shape)\n",
    "print(\"mu_x_pred shape: \", mu_x_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check the entire forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict  = model(x)\n",
    "for k,v in out_dict.items():\n",
    "    print(f\"\\n{k}:  {v.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check the component's of the optimization objective (ie. loss)\n",
    "    - [x] partition_z: z -> dict_z (keys are \"c\" and \"s\")\n",
    "    - [ ] predict_y: z_partition -> scores\n",
    "    - [ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_z = model.partition_z(z)\n",
    "for k,v in dict_z.items():\n",
    "    print(f\"{k}: {v.shape}\")\n",
    "    assert v.shape == (batch_size, model.content_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c,s = dict_z[\"c\"], dict_z[\"s\"]\n",
    "c.shape, s.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "- [ ] Showing the changes in the scores based on c and scores based on s will be super intersting to see as the model learns!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_c = model.predict_y(c)\n",
    "scores_s = model.predict_y(s)\n",
    "assert scores_c.shape == (batch_size, model.n_classes)\n",
    "assert scores_s.shape == (batch_size, model.n_classes)\n",
    "\n",
    "print(scores_c[0]) # TODO: Showing the changes in the scores based on c and scores based on s will be super intersting to see as the model learns!!!\n",
    "print(scores_s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check `compute_loss_c` and `compute_loss_s`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_c = model.compute_loss_c(c)\n",
    "print(\"loss_c: \", loss_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_s = model.compute_loss_s(s, y)\n",
    "print(\"loss_s: \", loss_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Full loss workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict = model(x)\n",
    "loss_dict = model.loss_function(out_dict, [x,y], 'train')\n",
    "pprint(loss_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones((5,2))\n",
    "b = torch.zeros((5,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([a,b], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.LogSoftmax()\n",
    "m(a).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with Metrics in Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.metrics import Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.tensor([\n",
    "    0,\n",
    "    0,\n",
    "    1,\n",
    "])\n",
    "# scores = torch.tensor([\n",
    "#     [10., 1.],\n",
    "#     [1., 5.],\n",
    "#     [1., 5.],\n",
    "# ])\n",
    "scores = torch.tensor([\n",
    "    [10., 1.],\n",
    "    [10., 5.],\n",
    "    [11., 5.],\n",
    "])\n",
    "\n",
    "logprobs = nn.LogSoftmax(dim=1)(scores)\n",
    "print(\"Logprobs: \", logprobs)\n",
    "acc_metric = Accuracy()\n",
    "\n",
    "print(acc_metric(logprobs, target))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_metric.compute()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with MNISTM and USPS datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MNISTM\n",
    "    - original size of an image: (1, 16,16)\n",
    "    - labels: {0, ..., 9}\n",
    "- USPS\n",
    "    - original size of an image: (3, 28, 28)\n",
    "    - labels\" {0, ..., 9}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.datasets.mnistm import MNISTM\n",
    "from torchvision.datasets import USPS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNISTM Dataset\n",
    "bs = 16\n",
    "num_workers = 16\n",
    "pin_memory = True\n",
    "xforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "# target_xforms = \n",
    "ds = MNISTM(ROOT/'data', \n",
    "          transform=xforms,\n",
    "          download=True)\n",
    "\n",
    "dl = DataLoader(ds, batch_size=bs, shuffle=True, \n",
    "               num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "\n",
    "x,y = next(iter(dl))\n",
    "info(x)\n",
    "info(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_timgs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USPS Dataset\n",
    "bs = 16\n",
    "num_workers = 16\n",
    "pin_memory = True\n",
    "xforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "# target_xforms = \n",
    "ds = USPS(ROOT/'data', \n",
    "          transform=xforms,\n",
    "          download=True)\n",
    "\n",
    "dl = DataLoader(ds, batch_size=bs, shuffle=True, \n",
    "               num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "\n",
    "x,y = next(iter(dl))\n",
    "info(x)\n",
    "info(y)\n",
    "show_timgs(x, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
