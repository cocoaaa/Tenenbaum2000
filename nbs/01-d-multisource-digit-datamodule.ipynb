{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Multi-source datamodule for Maptiles\n",
    "- Jan 6, 2021\n",
    "\n",
    "Each \"style\" of monochrome-mnist dataset puts a different color for the digit pixels.\n",
    "This notebooks shows \n",
    "\n",
    "- how we can create each of the datasets so that it outputs a consistent data sample\n",
    "at each call for the `__getitem__` method (eg. via indexing `myDataset[item_idx]`)\n",
    "\n",
    "- how to create a single dataset that outputs a datapoint from multiple datasets\n",
    "in a balanced way, ie. sampling as uniformly as possible to sample from any one of the \n",
    "datasets: \n",
    "\n",
    "Let's say we have 3 datasets, ds0, ds1, ds2, each of which contains n0, n1, n2 datapoints/observations\n",
    "respectively. Currently the implementation of `ConcatDataset` in `pytorch` samples a datapoint x from \n",
    "a single datasets d = [ds0, ds1, ds2] under a uniform distribution: p(x) = 1/(n0+n1+n2). Consequently, \n",
    "this \"uniform\" distribution puts a uniform probability mass on each datasample in the concatenated dataset, \n",
    "but the probability distribution of a sample coming from each dataset, say $\\pi = [\\pi_0, \\pi_1, \\pi_2]$ is not uniform, but rather a ratio of the number of samples, ie. $[n_0/n, n_1/n, n_2/n]$ where $n = n_0+n_1+n_2$.  \n",
    "If we want $\\pi$ to be a uniform distribution of selected source dataset, we \n",
    "could first compute the ratio of the dataset sizes, and input weighted number of datasets when creating \n",
    "the final, single dataset (of multiple sources).\n",
    "\n",
    "We will demonstrate how to use the ratio of dataset sizes to create a single, multi-source dataset from multiple datasources, so that the final, multi-course dataset outputs a datapoint, uniformly from any consitutent data source.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import re\n",
    "import math\n",
    "from datetime import datetime\n",
    "import time\n",
    "sys.dont_write_bytecode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Any, List, Set, Dict, Tuple, Optional, Iterable, Mapping, Union, Callable, TypeVar\n",
    "\n",
    "from pprint import pprint\n",
    "from ipdb import set_trace as brpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from  torch.linalg import norm as tnorm\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.tuner.tuning import Tuner\n",
    "\n",
    "\n",
    "# Select Visible GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Path \n",
    "1. Add project root and src folders to `sys.path`\n",
    "2. Set DATA_ROOT to `maptile_v2` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_nb_path = Path(os.getcwd())\n",
    "ROOT = this_nb_path.parent\n",
    "SRC = ROOT/'src'\n",
    "DATA_ROOT = Path(\"/data/hayley-old/maptiles_v2/\")\n",
    "paths2add = [this_nb_path, ROOT]\n",
    "\n",
    "print(\"Project root: \", str(ROOT))\n",
    "print('Src folder: ', str(SRC))\n",
    "print(\"This nb path: \", str(this_nb_path))\n",
    "\n",
    "\n",
    "for p in paths2add:\n",
    "    if str(p) not in sys.path:\n",
    "        sys.path.insert(0, str(p))\n",
    "        print(f\"\\n{str(p)} added to the path.\")\n",
    "# print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.datamodules.maptiles_datamodule import MaptilesDataModule\n",
    "\n",
    "from src.data.transforms.transforms import Identity, Unnormalizer, LinearRescaler, Monochromizer\n",
    "from src.data.transforms.functional import unnormalize, to_monochrome\n",
    "\n",
    "from src.visualize.utils import show_timg, show_timgs, show_batch, make_grid_from_tensors\n",
    "from src.utils.misc import info\n",
    "from collections import OrderedDict, defaultdict\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate MNIST-M and USPS datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MNISTM\n",
    "    - original size of an image: (1, 16,16)\n",
    "    - labels: {0, ..., 9}\n",
    "- USPS\n",
    "    - original size of an image: (3, 28, 28)\n",
    "    - labels\" {0, ..., 9}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.datasets.mnistm import MNISTM\n",
    "from torchvision.datasets import USPS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNISTM Dataset\n",
    "bs = 16\n",
    "num_workers = 16\n",
    "pin_memory = True\n",
    "in_shape = (3, 32,32)\n",
    "xforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(in_shape[-2:]),\n",
    "    ])\n",
    "# target_xforms = transforms.Lambda(lambda y: torch.tensor(y)) # already-so\n",
    "mnistm_ds = MNISTM(ROOT/'data', \n",
    "          transform=xforms,\n",
    "          download=True)\n",
    "\n",
    "mnistm_dl = DataLoader(mnistm_ds, batch_size=bs, shuffle=True, \n",
    "               num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "\n",
    "x,y = next(iter(mnistm_dl))\n",
    "info(x)\n",
    "info(y)\n",
    "# show_timgs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USPS Dataset\n",
    "bs = 16\n",
    "num_workers = 16\n",
    "pin_memory = True\n",
    "n_channels = 3\n",
    "in_shape = (n_channels, 32,32)\n",
    "xforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(in_shape[-2:]),\n",
    "    transforms.Lambda(lambda x: x.repeat((n_channels, 1, 1)))\n",
    "    ])\n",
    "target_xforms = transforms.Lambda(lambda y: torch.tensor(y))\n",
    "usps_ds = USPS(ROOT/'data', \n",
    "          transform=xforms,\n",
    "          target_transform=target_xforms,\n",
    "          download=True)\n",
    "\n",
    "usps_dl = DataLoader(usps_ds, batch_size=bs, shuffle=True, \n",
    "               num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "\n",
    "x,y = next(iter(usps_dl))\n",
    "info(x)\n",
    "info(y)\n",
    "# show_timgs(x, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenated dataset\n",
    "ds = ConcatDataset([mnistm_ds, usps_ds])\n",
    "\n",
    "# DataLoader w/o shuffling will iterate over the datasets in order \n",
    "# -- So, iterate over mnistm_ds and then iterate over usps_ds\n",
    "ordered_dl = DataLoader(ds, batch_size=16, shuffle=False)\n",
    "x, y = next(iter(ordered_dl))\n",
    "show_timgs(x)\n",
    "info(x)\n",
    "info(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader w/ shuffling will iterate over the concatenated dataset\n",
    "# in random order\n",
    "# -- So, iterate over a mixed images from mnistm_ds and usps_ds\n",
    "shuffled_dl = DataLoader(ds, batch_size=32, shuffle=True)\n",
    "x, y = next(iter(shuffled_dl))\n",
    "show_timgs(x)\n",
    "info(x)\n",
    "info(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mnistm_ds), len(usps_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice however, MNIST-M dataset has a lot more samples (60,000 vs. 7291). \n",
    "A quick fix to create a single dataloader (from multi dataset sources) so that each mini-batch of sample to have equal/balanced number of samples from each dataset, is to... pass in that many copies of the smaller-sized dataset:\n",
    "\n",
    "- see: https://discuss.pytorch.org/t/train-simultaneously-on-two-datasets/649/36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced Concatenated dataset\n",
    "n_copies = len(mnistm_ds)//len(usps_ds)\n",
    "dsets = [mnistm_ds]\n",
    "dsets.extend([usps_ds for i in range(n_copies)])\n",
    "\n",
    "balanced_ds = ConcatDataset(dsets)\n",
    "\n",
    "# DataLoader w/o shuffling will iterate over the datasets in order \n",
    "# -- So, iterate over mnistm_ds and then iterate over usps_ds\n",
    "ordered_dl = DataLoader(balanced_ds, batch_size=16, shuffle=False)\n",
    "x, y = next(iter(ordered_dl))\n",
    "show_timgs(x)\n",
    "info(x)\n",
    "info(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader w/ shuffling will iterate over the concatenated dataset\n",
    "# in random order\n",
    "# -- So, iterate over a mixed images from mnistm_ds and usps_ds\n",
    "shuffled_dl = DataLoader(balanced_ds, batch_size=32, shuffle=True)\n",
    "x, y = next(iter(shuffled_dl))\n",
    "show_timgs(x)\n",
    "info(x)\n",
    "info(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
