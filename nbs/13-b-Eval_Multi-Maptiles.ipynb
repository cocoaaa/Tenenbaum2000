{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate a BiVAE model trained on Rotated MNIST\n",
    "Jan 23, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import re\n",
    "import math\n",
    "from datetime import datetime\n",
    "import time\n",
    "sys.dont_write_bytecode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Set, Dict, Tuple, Optional, Iterable, Mapping, Union, Callable, TypeVar\n",
    "\n",
    "from pprint import pprint\n",
    "from ipdb import set_trace as brpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from  torch.linalg import norm as tnorm\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.tuner.tuning import Tuner\n",
    "\n",
    "\n",
    "# Select Visible GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Path \n",
    "1. Add project root and src folders to `sys.path`\n",
    "2. Set DATA_ROOT to `maptile_v2` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_nb_path = Path(os.getcwd())\n",
    "ROOT = this_nb_path.parent\n",
    "SRC = ROOT/'src'\n",
    "DATA_ROOT = Path(\"/data/hayley-old/maptiles_v2/\")\n",
    "paths2add = [this_nb_path, ROOT]\n",
    "\n",
    "print(\"Project root: \", str(ROOT))\n",
    "print('Src folder: ', str(SRC))\n",
    "print(\"This nb path: \", str(this_nb_path))\n",
    "\n",
    "\n",
    "for p in paths2add:\n",
    "    if str(p) not in sys.path:\n",
    "        sys.path.insert(0, str(p))\n",
    "        print(f\"\\n{str(p)} added to the path.\")\n",
    "        \n",
    "# print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transforms\n",
    "from src.data.transforms.transforms import Identity, Unnormalizer, LinearRescaler\n",
    "from src.data.transforms.functional import unnormalize\n",
    "\n",
    "# Utils\n",
    "from src.visualize.utils import show_timg, show_timgs, show_batch, make_grid_from_tensors\n",
    "from src.utils.misc import info, get_next_version_path\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataModules\n",
    "from src.data.datamodules import MNISTDataModule, MNISTMDataModule, MonoMNISTDataModule\n",
    "from src.data.datamodules import MultiMonoMNISTDataModule\n",
    "from src.data.datamodules.multisource_rotated_mnist_datamodule import MultiRotatedMNISTDataModule\n",
    "from src.data.datamodules.multisource_maptiles_datamodule import MultiMaptilesDataModule\n",
    "\n",
    "\n",
    "# plModules\n",
    "from src.models.plmodules.vanilla_vae import VanillaVAE\n",
    "from src.models.plmodules.iwae import IWAE\n",
    "from src.models.plmodules.bilatent_vae import BiVAE\n",
    "\n",
    "# Evaluations\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pytorch_lightning.utilities.cloud_io import load as pl_load\n",
    "from src.evaluator.qualitative import save_content_transfers, save_style_transfers, run_both_transfers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation NB to load and evaluate a trained model\n",
    "Steps:\n",
    "- Define the architecutre of the model to load\n",
    "- Load the model at `ckpt_path`\n",
    "- Run the following evaluations\n",
    "\n",
    "Evaluations:\n",
    "1. Evaluation of the generative model\n",
    "- Quantitative: `best_score`, which is the lowest loss computed as an average loss per datapt in the validation set. The loss is the estimate of the negative maginal log-likelihood of the observed data based on the trained model\n",
    "\n",
    "- Qualitative: \n",
    "  - Reconstruction of datapts from train/val datasets\n",
    "    - This evaluates how well the generative model (encoder-decoder) preserves the information needed to reconstruct the input data after having learned/trained/optimized jointly with/in the presence of its adversary, the style-classifier/discriminator\n",
    "\n",
    "2. Evaluation of the discriminator\n",
    "- How well does it discriminate? \n",
    "  - based on a style code: the model should predict the style label of the input datapt well\n",
    "    - Compute the `loss_s` over the train/val datasets (as an expectation, ie. loss value per datapt/image)\n",
    "  - based on a content code: the model should say \"I'm not sure, aka. all style labels seem equally probable\"\n",
    "    - Compute the `loss_s` over the train/val datasets (as an expectation, ie. loss value per datapt/image)\n",
    "  - Q: what is the range of the `loss_s` or `loss_c` for a good style-classifer?\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Multisource Maptiles DataModule\n",
    "data_root = Path(\"/data/hayley-old/maptiles_v2/\")\n",
    "cities = ['paris'] #['berlin', 'rome'] #['paris']\n",
    "styles = ['StamenTonerBackground']#['OSMDefault', 'CartoVoyagerNoLabels']#'StamenWatercolor']#, 'StamenTonerLines' 'StamenTonerBackground']\n",
    "zooms = ['14']\n",
    "in_shape = (3,128,128)\n",
    "batch_size = 32\n",
    "\n",
    "dm = MultiMaptilesDataModule(\n",
    "    data_root=data_root,\n",
    "    cities=cities,\n",
    "    styles=styles,\n",
    "    zooms=zooms,\n",
    "    in_shape=in_shape,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "dm.setup('fit')\n",
    "\n",
    "# show a batch\n",
    "dl = dm.train_dataloader()\n",
    "batch = next(iter(dl))\n",
    "x, label_c, label_s = dm.unpack(batch)\n",
    "info(x)\n",
    "show_timgs(x)\n",
    "print(label_c)\n",
    "print(label_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init plModule\n",
    "latent_dim = 10\n",
    "hidden_dims = [32,64,128,256] #,512]\n",
    "lr = 1e-4\n",
    "act_fn = nn.ReLU()\n",
    "# Specific for BiVAE\n",
    "adversary_dims = [32,32,32] \n",
    "is_contrasive = True # If true, use adv. loss from both content and style codes. Else just style codes\n",
    "kld_weight = 1.0 # vae_loss = recon_loss + kld_weight * kld_weight\n",
    "adv_loss_weight = 15. # loss = vae_loss + adv_loss_weight * adv_loss\n",
    "\n",
    "model = BiVAE(\n",
    "    in_shape=dm.size(), \n",
    "    n_styles=dm.n_styles,\n",
    "    latent_dim=latent_dim,\n",
    "    hidden_dims=hidden_dims,\n",
    "    adversary_dims=adversary_dims,\n",
    "    learning_rate=lr, \n",
    "    act_fn=act_fn,\n",
    "    size_average=False,\n",
    "    is_contrasive=is_contrasive,\n",
    "    kld_weight=kld_weight,\n",
    "    adv_loss_weight=adv_loss_weight,\n",
    ")\n",
    "\n",
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.misc import get_ckpt_path\n",
    "log_dir = Path((\n",
    "    \"/data/hayley-old/Tenanbaum2000/temp-logs/\"\n",
    "    \"BiVAE-C_Maptiles/version_0\"\n",
    "))\n",
    "ckpt_path = get_ckpt_path(log_dir)\n",
    "ckpt = pl_load(ckpt_path, map_location=lambda storage, loc: storage)  # dict object\n",
    "print(ckpt.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load state\n",
    "model.load_state_dict(ckpt['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a TB writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the root log_dir correpsonding to the ckpt_path\n",
    "log_dir = ckpt_path.parent.parent # eg. Folder called `temp-logs/f{model.name+dm.name}/version7`\n",
    "tb_writer = SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_writer.log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recons of inputs from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluator.qualitative import show_recon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_global_step = ckpt['global_step']\n",
    "best_global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_recon(\n",
    "    model=model, \n",
    "    dm=dm, \n",
    "    tb_writer=tb_writer, \n",
    "    global_step=best_global_step, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "- Test src.evaluator.qualitative.evaluate_transfers\n",
    "  - when constant_code is 'c' (ie. content transfers)\n",
    "  - when 's' is contant (ie. style transfers)\n",
    "  \n",
    "- The evaulation script on the top 3 trained BiVAE from yesterday\n",
    "- Run more training?\n",
    "- OSMNX - compare embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_samples = dm.get_content_samples(12)\n",
    "# Show content-representative images\n",
    "for label_c, timg in content_samples.items():\n",
    "    show_timg(timg, title=label_c)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_content_transfers(model,\n",
    "                      class_reps=content_samples, \n",
    "                       log_dir=log_dir, \n",
    "                       train_mean=dm.train_mean, \n",
    "                       train_std=dm.train_std);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_style_transfers(model,\n",
    "                      class_reps=content_samples, \n",
    "                       log_dir=log_dir, \n",
    "                       train_mean=dm.train_mean, \n",
    "                       train_std=dm.train_std);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize original images of the close neighbors in the latent space\n",
    "- Compute pairwise distance using cosine similarity\n",
    "- For each row (ie. a latent code), get the index of the smallest values. \n",
    "- Select the images in the batch x and visualize (can do this all in show_timgs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "mode = 'train'\n",
    "ds = getattr(dm, f\"{mode}_ds\")\n",
    "query_size = min(len(ds), 50000)# #4096#2048#1024\n",
    "dl = DataLoader(ds, batch_size=query_size, shuffle=True)\n",
    "\n",
    "\n",
    "pdist_metric = 'cosine' #pairwise distance metric in content space\n",
    "\n",
    "# tsne params\n",
    "tsne_dim = 2\n",
    "tsne_p = 5. #10 #perplexity\n",
    "# tsne_metric = 'euclidean'\n",
    "tsne_metric = 'cosine' \n",
    "tsne = TSNE(n_components=tsne_dim, metric=tsne_metric, perplexity=tsne_p)\n",
    "\n",
    "# Nearest code query param\n",
    "n_ngbrs = 10\n",
    "max_num_query = 32\n",
    "with torch.no_grad():\n",
    "    batch = next(iter(dl))\n",
    "    x, label_c, label_s = dm.unpack(batch)\n",
    "\n",
    "    dict_qparams = model.encode(x)\n",
    "    dict_z = model.rsample(dict_qparams)\n",
    "    c = dict_z['c']\n",
    "    s = dict_z['s']\n",
    "    z = model.combine_content_style(dict_z)\n",
    "\n",
    "    for name, embedding in zip([\"c\", \"s\", \"z\"], [c, s, z]):\n",
    "        # Compute pairwise distance of the embeddings\n",
    "        pdists = pairwise_distances(embedding.numpy(), metric=pdist_metric)\n",
    "        plt.imshow(pdists, cmap='gray')\n",
    "        plt.title(f\"Pairwise dists of {name}'s\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        # show TSNE2d of each code\n",
    "        embedding_2d = tsne.fit_transform(embedding)\n",
    "        \n",
    "        f, ax = plt.subplots(1,2, figsize=(20,10))\n",
    "        # first plot the 2dim embeddings and color-code by content id\n",
    "        ax[0].scatter(embedding_2d[:,0], embedding_2d[:,1],\n",
    "                     c = label_c)\n",
    "        ax[0].set_title(f\"Code: {name}, colored by content-id\")\n",
    "        \n",
    "        # same embedding plot, but color-code by style-id\n",
    "        ax[1].scatter(embedding_2d[:,0], embedding_2d[:,1],\n",
    "                     c = label_s),\n",
    "        ax[1].set_title(f\"Code: {name}, colored by style-id\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        # Nearest neighbor queries\n",
    "        # smaller values means closer in distance\n",
    "        n_rows = min(query_size, max_num_query)\n",
    "\n",
    "        selected_rows = np.random.choice(len(x), size=n_rows)\n",
    "        for idx in selected_rows:\n",
    "            args = np.argsort(pdists[idx])[:n_ngbrs]\n",
    "    #         print(args)\n",
    "            show_timgs(LinearRescaler()(x[args]), cmap='gray', factor=2, \n",
    "                       nrows=1, title=f'Nearest of digit {label_c[idx].item()}: {name}')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate new style codes via. style code interpolation\n",
    "Added: Jan 18, 2021\n",
    "1. Get the cluster centers from style code embeddings: r,g,b codes\n",
    "2. new style code = interpolate (r,g,b)\n",
    "3. Take an image -> get its content code -> z = [c, new_style] -> generator?\n",
    "  - z = [c, r] -> generator\n",
    "  - z = [c, g] -> gen\n",
    "  - z = [c, b] -> gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from torch import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_styles = dm.n_styles\n",
    "model.eval()\n",
    "mode = 'train'\n",
    "query_size = 128 #1024\n",
    "metric = 'cosine' #pairwise distance metric in content space\n",
    "ds = getattr(dm, f\"{mode}_ds\")\n",
    "dl = DataLoader(ds, batch_size=query_size, shuffle=True)\n",
    "\n",
    "# tsne params\n",
    "tsne_dim = 2\n",
    "tsne_p = 5. #10 #perplexity\n",
    "# tsne_metric = 'euclidean'\n",
    "tsne_metric = 'cosine' \n",
    "tsne = TSNE(n_components=tsne_dim, metric=tsne_metric, perplexity=tsne_p )\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch = next(iter(dl))\n",
    "    x, label_c, label_s = dm.unpack(batch)\n",
    "\n",
    "    dict_qparams = model.encode(x)\n",
    "    dict_z = model.rsample(dict_qparams)\n",
    "    c = dict_z['c']\n",
    "    s = dict_z['s']\n",
    "    z = model.combine_content_style(dict_z)\n",
    "    # mean norm of each code\n",
    "    norm_c = LA.norm(c, dim=-1)\n",
    "    norm_s = LA.norm(s, dim=-1)\n",
    "    print(\"Avg. norm of content content: \", norm_c.mean())\n",
    "    print(\"Avg. norm of style content: \", norm_s.mean())\n",
    "    \n",
    "    # kmeans clustering on style codes\n",
    "    kmeans = KMeans(n_clusters=n_styles, random_state=0).fit(s)\n",
    "    centers = torch.tensor(kmeans.cluster_centers_)\n",
    "    breakpoint()\n",
    "    \n",
    "    # add centers to the s codes:\n",
    "    s_and_centers = torch.cat([s, centers], dim=0)\n",
    "    print(s_and_centers.shape)\n",
    "    \n",
    "    for name, embedding in [(\"s\", s_and_centers)]:# zip([\"c\", \"s\", \"z\"], [c,s,z]):\n",
    "        # Compute pairwise distance of the embeddings\n",
    "        pdists = pairwise_distances(embedding.numpy(), metric=metric)\n",
    "        plt.imshow(pdists, cmap='gray')\n",
    "        plt.title(f\"Pairwise dists of {name}'s\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        # show the 2dim view on the codes\n",
    "        embedding_2d = tsne.fit_transform(embedding)\n",
    "        colors = torch.cat([label_s, torch.tensor([n_styles]*dm.n_styles)])\n",
    "        f, ax = plt.subplots(figsize=(20,10))\n",
    "        # Plot the 2dim embeddings and color-code by style id\n",
    "        ax.scatter(embedding_2d[:,0], embedding_2d[:,1],\n",
    "                     c = colors)\n",
    "        ax.set_title(f\"Code: {name}, colored by style-id\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "# todo: use the r,g,b representative codes + any content code -> decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each content-label, compute the mean content code\n",
    "- For each style-label, compute the mean style code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_styles = dm.n_styles\n",
    "# n_contents = dm.n_contents\n",
    "# model.eval()\n",
    "# mode = 'train'\n",
    "# ds = getattr(dm, f\"{mode}_ds\")\n",
    "# query_size = len(ds) #1024\n",
    "# dl = DataLoader(ds, batch_size=query_size, num_workers=16, pin_memory=True)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     batch = next(iter(dl))\n",
    "#     x = batch['img']\n",
    "#     label_c = batch['digit']  # digit/content label (int) -- currently not used\n",
    "#     label_s = batch['color']\n",
    "\n",
    "#     dict_qparams = model.encode(x)\n",
    "#     mu_qc = dict_qparams['mu_qc']\n",
    "#     mu_qs = dict_qparams['mu_qs']\n",
    "    \n",
    "#     dict_z = model.rsample(dict_qparams)\n",
    "#     c = dict_z['c']\n",
    "#     s = dict_z['s']\n",
    "#     z = model.combine_content_style(dict_z)\n",
    "#     # mean norm of each code\n",
    "#     norm_c = LA.norm(c, dim=-1)\n",
    "#     norm_s = LA.norm(s, dim=-1)\n",
    "#     print(\"Avg. norm of content content: \", norm_c.mean())\n",
    "#     print(\"Avg. norm of style content: \", norm_s.mean())\n",
    "    \n",
    "\n",
    "#     content_avgs = {}\n",
    "#     mu_qc_avgs = {}\n",
    "#     min_mu_qcs = {}\n",
    "#     max_mu_qcs = {}\n",
    "#     for content_id in range(n_contents):\n",
    "#         mean_mu_qc = mu_qc[label_c == content_id].mean(dim=0)\n",
    "#         mu_qc_avgs[content_id] = mean_mu_qc # this fails to make desired reconstruction\n",
    "\n",
    "#         min_mu_qcs[content_id]= mu_qc[label_c == content_id].min(dim=0)\n",
    "#         max_mu_qcs[content_id]= mu_qc[label_c == content_id].max(dim=0)\n",
    "        \n",
    "#         # Must use the mean of samples!\n",
    "#         mean_c = c[label_c == content_id].mean(dim=0)\n",
    "#         content_avgs[content_id] = mean_c\n",
    "        \n",
    "        \n",
    "#     style_avgs = {}\n",
    "#     mu_qs_avgs = {}\n",
    "#     min_mu_qss = {}\n",
    "#     max_mu_qss = {}\n",
    "#     for style_id in range(n_styles):\n",
    "#         mean_mu_qs = mu_qs[label_s == style_id].mean(dim=0)\n",
    "#         mu_qs_avgs[style_id] = mean_mu_qs # this fails to make desired reconstruction\n",
    "        \n",
    "#         min_mu_qss[style_id]= mu_qs[label_s == style_id].min(dim=0)\n",
    "#         max_mu_qss[style_id]= mu_qs[label_s == style_id].max(dim=0)\n",
    "        \n",
    "#         mean_s = s[label_s == style_id].mean(dim=0)\n",
    "#         style_avgs[style_id] = mean_s      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluator.qualitative import compute_avg_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_codes = compute_avg_codes(model, dm)\n",
    "mu_qc_avgs = avg_codes['mu_qc_avgs']\n",
    "mu_qs_avgs = avg_codes['mu_qs_avgs']\n",
    "\n",
    "mu_qc_mins = avg_codes['mu_qc_mins']\n",
    "mu_qs_mins = avg_codes['mu_qs_mins']\n",
    "\n",
    "mu_qc_maxs = avg_codes['mu_qc_maxs']\n",
    "mu_qs_maxs = avg_codes['mu_qs_maxs']\n",
    "\n",
    "content_avgs = avg_codes['content_avgs']\n",
    "style_avgs = avg_codes['style_avgs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content/Style transfer using the content-class and style-class representatives\n",
    "content_reps = content_avgs\n",
    "style_reps = style_avgs\n",
    "\n",
    "# Or\n",
    "# content_reps = mu_qc_avgs\n",
    "# style_reps = mu_qs_avgs\n",
    "\n",
    "n_contents = dm.n_contents\n",
    "linearlize = True\n",
    "cmap = 'gray' if dm.in_shape[0] == 1 else None\n",
    "train_mean, train_std = dm.train_mean, dm.train_std\n",
    "f, axes = plt.subplots(nrows=n_contents, ncols=n_styles, figsize=(n_styles*3.,n_contents*3.))\n",
    "# ax = ax.flatten()\n",
    "for content_id, c in content_reps.items():\n",
    "    for style_id, s in style_reps.items():\n",
    "        dict_z = {'c': c[None], 's': s[None]}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            z = model.combine_content_style(dict_z)\n",
    "            \n",
    "            # Reconstruct\n",
    "            recons = model.decode(z)\n",
    "\n",
    "            # Optional: for better viz, unnormalize or/and linearlize\n",
    "            unnormed_recons = unnormalize(recons, train_mean, train_std)\n",
    "            if linearlize:\n",
    "                unnormed_recons = LinearRescaler()(unnormed_recons)\n",
    "            ax = axes[content_id][style_id]\n",
    "            ax.imshow(unnormed_recons[0].permute((1,2,0)), cmap=cmap)\n",
    "            ax.set_axis_off()\n",
    "            ax.set_title(f\"Content: {content_id}, Style: {style_id}\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "## Distribution of qparams for content/style codes\n",
    "Given a batch of images with the same content, \n",
    "we hypothesize/an ideally leanred model will put their content codes in a close neighborhood in the content latent space. Then, the average of a batch of mu_qc's will be a vector that indicates the center/mean of the mu_qc of each image (whose content-id is the same across the input batch).\n",
    "\n",
    "- Show the histogram of each dimension's mu_qc_j across the batch: one histogram for each dimension j\n",
    "- Compute the average mu_qc over the batch of mu_qc's. Then, use it as the input content code with some input image's style code to the generator. What is the generated output? Does its content look something like the content of the batch of image?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_xlim = (-4., 4.)\n",
    "# var_xlim = (0, 0.05)\n",
    "n_samples = 1024\n",
    "# Set output dir\n",
    "out_dir = log_dir/\"dist_qparams_per_content\"\n",
    "if not out_dir.exists():\n",
    "    out_dir.mkdir(parents=True)\n",
    "    print(\"Created and saving to: \", out_dir)\n",
    "    \n",
    "for digit_id in range(10):\n",
    "    # Collect a batch of images of the same content\n",
    "    xs = []\n",
    "    n_collected = 0\n",
    "    while n_collected <= n_samples:\n",
    "        batch = next(iter(dl))\n",
    "        x = batch['img']\n",
    "        label_c = batch['digit']\n",
    "        label_s = batch['color']\n",
    "        selected = x[label_c==digit_id]\n",
    "        xs.append(selected)\n",
    "        n_collected += len(selected)\n",
    "    xs = torch.cat(xs, dim=0)\n",
    "    print(f\" Digit {digit_id} collected. Input: {xs.shape}\")\n",
    "\n",
    "\n",
    "    # Plot the distributions of qc parameters and qs parameters\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dict_qparams = model(xs)\n",
    "\n",
    "        # Dist. of each content dim's parameters\n",
    "        mu_qc, var_qc = dict_qparams['mu_qc'], dict_qparams['logvar_qc'].exp() #(BS, content_dim), (BS, style_dim)\n",
    "        # -- mu_qc's\n",
    "#         mu_xlim = (0, max(mu_qc)\n",
    "        f, ax = plt.subplots(1, model.content_dim, figsize=(20,2))\n",
    "        title = f\"Digit {digit_id}: \" + r\"Dist. of $mu^{c}_j$\"\n",
    "        f.suptitle(title)\n",
    "        for j in range(model.content_dim):\n",
    "            ax[j].hist(mu_qc[:,j])\n",
    "            ax[j].set_xlim(mu_xlim)\n",
    "            ax[j].set_title(f\"dim {j}\")\n",
    "        f.tight_layout()\n",
    "        f.savefig(out_dir/f\"digit-{digit_id}-mu_qc.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        # -- var_qc's\n",
    "        var_xlim = (0, var_qc.max().item())\n",
    "        f, ax = plt.subplots(1, model.style_dim, figsize=(20,2))\n",
    "        title = f\"Digit {digit_id}: \" + r\"Dist. of $var^{c}_j$\"\n",
    "        f.suptitle(title)\n",
    "        for j in range(model.content_dim):\n",
    "            ax[j].hist(var_qc[:,j])\n",
    "            ax[j].set_xlim(var_xlim)\n",
    "            ax[j].set_title(f\"dim {j}\")\n",
    "        f.tight_layout()\n",
    "        f.savefig(out_dir/f\"digit-{digit_id}-var_qc.png\")\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        # Dist. of each style dim's parameters\n",
    "        mu_qs, var_qs = dict_qparams['mu_qs'], dict_qparams['logvar_qs'].exp()\n",
    "        # -- mu_qs's\n",
    "#         mu_xlim = (0, max(mu_qs))\n",
    "        f, ax = plt.subplots(1, model.style_dim, figsize=(20,2))\n",
    "        title = f\"Digit {digit_id}: \" + r\"Dist. of $mu^{s}_j$\"\n",
    "        f.suptitle(title)\n",
    "        for j in range(model.style_dim):\n",
    "            ax[j].hist(mu_qs[:,j])\n",
    "            ax[j].set_xlim(mu_xlim)\n",
    "            ax[j].set_title(f\"dim {j}\")\n",
    "        f.tight_layout()\n",
    "        f.savefig(out_dir/f\"digit-{digit_id}-mu_qs.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        # -- var_qs's\n",
    "        var_xlim = (0, var_qs.max().item())\n",
    "        f, ax = plt.subplots(1, model.style_dim, figsize=(20,2))\n",
    "        title = f\"Digit {digit_id}: \" + r\"Dist. of $var^{s}_j$\"\n",
    "        f.suptitle(title)\n",
    "        for j in range(model.style_dim):\n",
    "            ax[j].hist(var_qs[:,j])\n",
    "            ax[j].set_xlim(var_xlim)\n",
    "            ax[j].set_title(f\"dim {j}\")\n",
    "        f.tight_layout()\n",
    "        f.savefig(out_dir/f\"digit-{digit_id}-var_qs.png\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Collect a batch of images with the same style: one such batch for style.\n",
    "Then plot the histogram of mu_qs_j over the batch: one histogram for a dimension of a style code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_xlim = (-4., 4.)\n",
    "# var_xlim = (0, 0.05)\n",
    "n_samples = 1024\n",
    "# Set output dir\n",
    "out_dir = log_dir/\"dist_qparams_per_style\"\n",
    "if not out_dir.exists():\n",
    "    out_dir.mkdir(parents=True)\n",
    "    print(\"Created and saving to: \", out_dir)\n",
    "    \n",
    "for style_id in range(n_styles):\n",
    "    # Collect a batch of images of the same content\n",
    "    xs = []\n",
    "    n_collected = 0\n",
    "    while n_collected <= n_samples:\n",
    "        batch = next(iter(dl))\n",
    "        x = batch['img']\n",
    "        label_c = batch['digit']\n",
    "        label_s = batch['color']\n",
    "        selected = x[label_c==style_id]\n",
    "        xs.append(selected)\n",
    "        n_collected += len(selected)\n",
    "    xs = torch.cat(xs, dim=0)\n",
    "    print(f\"Style {style_id} collected. Input: {xs.shape}\")\n",
    "\n",
    "\n",
    "    # Plot the distributions of qc parameters and qs parameters\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dict_qparams = model(xs)\n",
    "\n",
    "        # Dist. of each content dim's parameters\n",
    "        mu_qc, var_qc = dict_qparams['mu_qc'], dict_qparams['logvar_qc'].exp() #(BS, content_dim), (BS, style_dim)\n",
    "        # -- mu_qc's\n",
    "#         mu_xlim = (0, max(mu_qc)\n",
    "        f, ax = plt.subplots(1, model.content_dim, figsize=(20,2))\n",
    "        title = f\"Style {style_id}: \" + r\"Dist. of $mu^{c}_j$\"\n",
    "        f.suptitle(title)\n",
    "        for j in range(model.content_dim):\n",
    "            ax[j].hist(mu_qc[:,j])\n",
    "            ax[j].set_xlim(mu_xlim)\n",
    "            ax[j].set_title(f\"dim {j}\")\n",
    "        f.tight_layout()\n",
    "        f.savefig(out_dir/f\"Style-{style_id}-mu_qc.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        # -- var_qc's\n",
    "        var_xlim = (0, var_qc.max().item())\n",
    "        f, ax = plt.subplots(1, model.style_dim, figsize=(20,2))\n",
    "        title = f\"Style {stlye_id}: \" + r\"Dist. of $var^{c}_j$\"\n",
    "        f.suptitle(title)\n",
    "        for j in range(model.content_dim):\n",
    "            ax[j].hist(var_qc[:,j])\n",
    "            ax[j].set_xlim(var_xlim)\n",
    "            ax[j].set_title(f\"dim {j}\")\n",
    "        f.tight_layout()\n",
    "        f.savefig(out_dir/f\"Style-{stlye_id}-var_qc.png\")\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        # Dist. of each style dim's parameters\n",
    "        mu_qs, var_qs = dict_qparams['mu_qs'], dict_qparams['logvar_qs'].exp()\n",
    "        # -- mu_qs's\n",
    "#         mu_xlim = (0, max(mu_qs))\n",
    "        f, ax = plt.subplots(1, model.style_dim, figsize=(20,2))\n",
    "        title = f\"Style {stlye_id}: \" + r\"Dist. of $mu^{s}_j$\"\n",
    "        f.suptitle(title)\n",
    "        for j in range(model.style_dim):\n",
    "            ax[j].hist(mu_qs[:,j])\n",
    "            ax[j].set_xlim(mu_xlim)\n",
    "            ax[j].set_title(f\"dim {j}\")\n",
    "        f.tight_layout()\n",
    "        f.savefig(out_dir/f\"Style-{stlye_id}-mu_qs.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        # -- var_qs's\n",
    "        var_xlim = (0, var_qs.max().item())\n",
    "        f, ax = plt.subplots(1, model.style_dim, figsize=(20,2))\n",
    "        title = f\"Style {stlye_id}: \" + r\"Dist. of $var^{s}_j$\"\n",
    "        f.suptitle(title)\n",
    "        for j in range(model.style_dim):\n",
    "            ax[j].hist(var_qs[:,j])\n",
    "            ax[j].set_xlim(var_xlim)\n",
    "            ax[j].set_title(f\"dim {j}\")\n",
    "        f.tight_layout()\n",
    "        f.savefig(out_dir/f\"Style-{stlye_id}-var_qs.png\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Latent Space Traversal: Content space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear traversal in a single dimension\n",
    "\n",
    "For each content-id's representative code:\n",
    "  - Fix a style code representative\n",
    "  - Traverse for each dim of the content code\n",
    "  - Put the results in a grid where each colm is the dimension of the content and each row shows the traversed content code's reconstruction result (in combination with the fixed style code representative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluator.qualitative import get_traversals, run_content_traversal, run_style_traversal\n",
    "from src.utils.misc import now2str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_reps = content_avgs\n",
    "style_reps = style_avgs\n",
    "\n",
    "# Or\n",
    "# content_reps = mu_qc_avgs\n",
    "# style_reps = mu_qs_avgs\n",
    "\n",
    "# Run content traversals for each content code that is a representative of each content class\n",
    "n_traversals = 20\n",
    "out_dir = log_dir/\"content_traversals_of_reps\"\n",
    "run_name = now2str()\n",
    "to_save = True\n",
    "to_show = False\n",
    "if not out_dir.exists():\n",
    "    out_dir.mkdir(parents=True)\n",
    "    print(\"Created: \", out_dir)\n",
    "\n",
    "for content_id in range(n_contents):\n",
    "    c = content_reps[content_id]\n",
    "    content_dim = c.shape[-1]\n",
    "    traversal_start = mu_qc_mins[content_id]\n",
    "    traversal_end = mu_qc_maxs[content_id]\n",
    "#     traversal_start = torch.zeros_like(c).fill_(-3.0)#mu_qc_mins[content_id]\n",
    "#     traversal_end =  torch.zeros_like(c).fill_(3.0)# mu_qc_maxs[content_id]\n",
    "    \n",
    "    \n",
    "    for style_id in range(n_styles):\n",
    "        s = style_reps[style_id]\n",
    "        \n",
    "        # Traverse for each dim\n",
    "        title = f\"Content Traversal: content {content_id}, style {style_id}\"\n",
    "        run_content_traversal(model, c, s, \n",
    "                              traversal_start=traversal_start,\n",
    "                             traversal_end=traversal_end,\n",
    "                             n_traversals=n_traversals,\n",
    "                            show=to_show, \n",
    "                            title=title,\n",
    "                             to_save=to_save,\n",
    "                             out_path=out_dir/f\"content_traversals_c-{content_id}_s-{style_id}_{run_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for content_id in range(n_contents):\n",
    "#     c = content_reps[content_id]\n",
    "#     content_dim = c.shape[-1]\n",
    "\n",
    "#     for style_id in range(n_styles):\n",
    "#         s = style_reps[style_id]\n",
    "        \n",
    "#         # Traverse for each dim\n",
    "#         n_traversals = 10\n",
    "#         grids = [] #k,v = dim_i, batch of recons while traversing in dim_i direction (n_traversals, *dim_x)\n",
    "#         for dim_i in range(content_dim):\n",
    "#             min_dim_i = mu_qc_mins[content_id][dim_i]\n",
    "#             max_dim_i = mu_qc_maxs[content_id][dim_i]\n",
    "#             print(min_dim_i, max_dim_i)\n",
    "\n",
    "#             c_traversals = get_traversals(c, dim_i, min_dim_i, max_dim_i, n_traversals)\n",
    "#             dict_z = {\n",
    "#                 \"c\": c_traversals, \n",
    "#                 \"s\": s.repeat((n_traversals, 1))\n",
    "#                      }\n",
    "#             z = model.combine_content_style(dict_z)\n",
    "#             recons = model.decode(z)\n",
    "#         #     show_timgs(recons, nrows=n_traversals, title=f\"Traversal: Content_id:{content_id}, dim:{dim_i}\")\n",
    "\n",
    "#             grid = torchvision.utils.make_grid(recons, nrow=1) # Caveat: nrow is num of colms!\n",
    "#             grids.append(grid)\n",
    "#         grids = torch.cat(grids, dim=2)\n",
    "\n",
    "#         show_timg(grids, title=f\"Content Traversal: content {content_id}, style {style_id}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General traversal on an input data point\n",
    "Unlike #1 above where we traversed per content-id code, we do a general traversal on the whole content space, where we used the content/style representative codes, we traverse on the learned latent dimension.\n",
    "\n",
    "input x -> mu_c, logvar_c\n",
    "        -> mu_s, logvar_s -> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent content space -- Conditioned on the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "mode = 'train'\n",
    "ds = getattr(dm, f\"{mode}_ds\")\n",
    "query_size = 1\n",
    "dl = DataLoader(ds, batch_size=query_size, shuffle=True, \n",
    "                num_workers=16, pin_memory=True)\n",
    "\n",
    "to_show = True\n",
    "to_save = True\n",
    "n_samples = 10\n",
    "for i in range(n_samples):\n",
    "    \n",
    "    # Encode x to c,s\n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(dl))\n",
    "        x, label_c, label_s =dm.unpack(batch)\n",
    "        dict_qparams = model.encode(x)\n",
    "        dict_z = model.rsample(dict_qparams)\n",
    "        c = dict_z['c'][0]\n",
    "        s = dict_z['s'][0]\n",
    "\n",
    "    # Do Content Traversal\n",
    "    title = f\"Content Traversal of {mode} datapt\"\n",
    "    out_dir = log_dir/\"content_traversals_of_input\"\n",
    "    to_save = True\n",
    "    if not out_dir.exists():\n",
    "        out_dir.mkdir(parents=True)\n",
    "        print(\"Created: \", out_dir)\n",
    "\n",
    "    n_traversals = 20\n",
    "    traversal_start = -3\n",
    "    traversal_end = 3\n",
    "    run_content_traversal(model, c, s, \n",
    "                          traversal_start=traversal_start,\n",
    "                         traversal_end=traversal_end,\n",
    "                         n_traversals=n_traversals,\n",
    "                        show=to_show, \n",
    "                        title=title,\n",
    "                         to_save=to_save,\n",
    "                         out_path=out_dir/f\"content_traversals_{now2str()}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent content space -- Unconditioned on the input; Based on the prior distirbution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "mode = 'train'\n",
    "ds = getattr(dm, f\"{mode}_ds\")\n",
    "query_size = 1\n",
    "dl = DataLoader(ds, batch_size=query_size, shuffle=True, \n",
    "                num_workers=16, pin_memory=True)\n",
    "\n",
    "\n",
    "n_samples = 10\n",
    "for i in range(n_samples):\n",
    "    # Encode x to c,s\n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(dl))\n",
    "        x, label_c, label_s =dm.unpack(batch)\n",
    "        dict_qparams = model.encode(x)\n",
    "        dict_z = model.rsample(dict_qparams)\n",
    "        c = dict_z['c'][0]\n",
    "        s = dict_z['s'][0]\n",
    "\n",
    "\n",
    "    # Content Traversal\n",
    "    # Use c from the prior distribution\n",
    "    title = f\"Content Traversal based on prior\"\n",
    "    out_dir = log_dir/\"content_traversals_unconditioned\"\n",
    "    to_save = True\n",
    "    if not out_dir.exists():\n",
    "        out_dir.mkdir(parents=True)\n",
    "        print(\"Created: \", out_dir)\n",
    "\n",
    "    n_traversals = 20\n",
    "    traversal_start = -4\n",
    "    traversal_end = 4\n",
    "    c_prior = torch.zeros_like(c)\n",
    "    run_content_traversal(model, c_prior, s, \n",
    "                          traversal_start=traversal_start,\n",
    "                         traversal_end=traversal_end,\n",
    "                         n_traversals=n_traversals,\n",
    "                        show=True, \n",
    "                        title=title,\n",
    "                         to_save=to_save,\n",
    "                         out_path=out_dir/(\n",
    "                             \"content_traversals_unconditioned_\"\n",
    "                             f\"start-{traversal_start}_end-{traversal_end}_{now2str()}.png\"\n",
    "                         )\n",
    "                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Latent Space Traversal: Style space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear traversal in a single dimension\n",
    "\n",
    "For each content-id's representative code:\n",
    "  - Fix a style code representative\n",
    "  - Traverse for each dim of the content code\n",
    "  - Put the results in a grid where each colm is the dimension of the content and each row shows the traversed content code's reconstruction result (in combination with the fixed style code representative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluator.qualitative import get_traversals\n",
    "from src.utils.misc import now2str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run content traversals for each content code that is a representative of each content class\n",
    "n_traversals = 20\n",
    "out_dir = log_dir/\"style_traversals_of_reps\"\n",
    "run_name = now2str()\n",
    "\n",
    "to_save = True\n",
    "if not out_dir.exists():\n",
    "    out_dir.mkdir(parents=True)\n",
    "    print(\"Created: \", out_dir)\n",
    "\n",
    "for style_id in range(n_styles):\n",
    "    s = style_reps[style_id]\n",
    "    for content_id in range(n_contents):\n",
    "        c = content_reps[content_id]\n",
    "        \n",
    "        # Traverse for each dim\n",
    "        traversal_start = mu_qs_mins[style_id]\n",
    "        traversal_end = mu_qs_maxs[style_id]\n",
    "    #     traversal_start = torch.zeros_like(c).fill_(-3.0)#mu_qc_mins[content_id]\n",
    "    #     traversal_end =  torch.zeros_like(c).fill_(3.0)# mu_qc_maxs[content_id]\n",
    "        title = f\"Style Traversal: content {content_id}, style {style_id}\"\n",
    "        run_style_traversal(model, c, s, \n",
    "                              traversal_start=traversal_start,\n",
    "                             traversal_end=traversal_end,\n",
    "                             n_traversals=n_traversals,\n",
    "                            show=True, \n",
    "                            title=title,\n",
    "                             to_save=to_save,\n",
    "                             out_path=out_dir/f\"style_traversals_c-{content_id}_s-{style_id}_{run_name}.png\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### General traversal on an input data point\n",
    "Unlike #1 above where we traversed per style-id code, we do a general traversal on the whole style space, where we used the content/style representative codes, we traverse on the learned latent style dimension.\n",
    "\n",
    "input x -> mu_c, logvar_c\n",
    "        -> mu_s, logvar_s -> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent style space -- Conditioned on the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "mode = 'train'\n",
    "ds = getattr(dm, f\"{mode}_ds\")\n",
    "query_size = 1\n",
    "dl = DataLoader(ds, batch_size=query_size, shuffle=True, \n",
    "                num_workers=16, pin_memory=True)\n",
    "\n",
    "\n",
    "n_samples = 10\n",
    "for i in range(n_samples):\n",
    "    # Encode x to c,s\n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(dl))\n",
    "        x, label_c, label_s =dm.unpack(batch)\n",
    "        dict_qparams = model.encode(x)\n",
    "        dict_z = model.rsample(dict_qparams)\n",
    "        c = dict_z['c'][0]\n",
    "        s = dict_z['s'][0]\n",
    "\n",
    "    # Style Traversal\n",
    "    title = f\"Style Traversal conditioned on input\"\n",
    "    out_dir = log_dir/\"style_traversals_of_input\"\n",
    "    to_save = True\n",
    "    if not out_dir.exists():\n",
    "        out_dir.mkdir(parents=True)\n",
    "        print(\"Created: \", out_dir)\n",
    "\n",
    "    n_traversals = 30\n",
    "    traversal_start = -3.5\n",
    "    traversal_end = 3.5\n",
    "    run_style_traversal(model, c, s, \n",
    "                          traversal_start=traversal_start,\n",
    "                         traversal_end=traversal_end,\n",
    "                         n_traversals=n_traversals,\n",
    "                        show=True, \n",
    "                        title=title,\n",
    "                         to_save=to_save,\n",
    "                         out_path=out_dir/(\n",
    "                         \"style_traversals_conditioned_\"\n",
    "                             f\"start-{traversal_start}_end-{traversal_end}_{now2str()}.png\"\n",
    "                         )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent style space -- Unconditioned on the input; Based on the prior distirbution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "mode = 'train'\n",
    "ds = getattr(dm, f\"{mode}_ds\")\n",
    "query_size = 1\n",
    "dl = DataLoader(ds, batch_size=query_size, shuffle=True, \n",
    "                num_workers=16, pin_memory=True)\n",
    "\n",
    "\n",
    "n_samples = 10\n",
    "for i in range(n_samples):\n",
    "    # Encode x to c,s\n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(dl))\n",
    "        x, label_c, label_s =dm.unpack(batch)\n",
    "        dict_qparams = model.encode(x)\n",
    "        dict_z = model.rsample(dict_qparams)\n",
    "        c = dict_z['c'][0]\n",
    "        s = dict_z['s'][0]\n",
    "\n",
    "    # Style Traversal\n",
    "    # Use c from the prior distribution\n",
    "    title = f\"Style Traversal based on prior\"\n",
    "    out_dir = log_dir/\"style_traversals_unconditioned\"\n",
    "    to_save = True\n",
    "    if not out_dir.exists():\n",
    "        out_dir.mkdir(parents=True)\n",
    "        print(\"Created: \", out_dir)\n",
    "\n",
    "    n_traversals = 20\n",
    "    traversal_start = -4\n",
    "    traversal_end = 4\n",
    "    s_prior = torch.zeros_like(s)\n",
    "    run_content_traversal(model, c, s_prior, \n",
    "                          traversal_start=traversal_start,\n",
    "                          traversal_end=traversal_end,\n",
    "                          n_traversals=n_traversals,\n",
    "                          show=True, \n",
    "                          title=title,\n",
    "                          to_save=to_save,\n",
    "                          out_path=out_dir/(\n",
    "                             \"style_traversals_unconditioned_\"\n",
    "                             f\"start-{traversal_start}_end-{traversal_end}_{now2str()}.png\"\n",
    "                             )\n",
    "                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
