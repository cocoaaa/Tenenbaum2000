{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the impact of contrasive samples (`loss_c`) in the quality of partitioned latent space\n",
    "Jan 18, 2021\n",
    "- via. embedding of content codes + color-coding by (1) content-labels and (2) style-labels\n",
    "- also, repeat the same visualization for the embedding of style codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import re\n",
    "import math\n",
    "from datetime import datetime\n",
    "import time\n",
    "sys.dont_write_bytecode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Set, Dict, Tuple, Optional, Iterable, Mapping, Union, Callable, TypeVar\n",
    "\n",
    "from pprint import pprint\n",
    "from ipdb import set_trace as brpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from  torch.linalg import norm as tnorm\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.tuner.tuning import Tuner\n",
    "\n",
    "\n",
    "# Select Visible GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Path \n",
    "1. Add project root and src folders to `sys.path`\n",
    "2. Set DATA_ROOT to `maptile_v2` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_nb_path = Path(os.getcwd())\n",
    "ROOT = this_nb_path.parent\n",
    "SRC = ROOT/'src'\n",
    "DATA_ROOT = Path(\"/data/hayley-old/maptiles_v2/\")\n",
    "paths2add = [this_nb_path, ROOT]\n",
    "\n",
    "print(\"Project root: \", str(ROOT))\n",
    "print('Src folder: ', str(SRC))\n",
    "print(\"This nb path: \", str(this_nb_path))\n",
    "\n",
    "\n",
    "for p in paths2add:\n",
    "    if str(p) not in sys.path:\n",
    "        sys.path.insert(0, str(p))\n",
    "        print(f\"\\n{str(p)} added to the path.\")\n",
    "        \n",
    "# print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transforms\n",
    "from src.data.transforms.transforms import Identity, Unnormalizer, LinearRescaler\n",
    "from src.data.transforms.functional import unnormalize\n",
    "\n",
    "# Utils\n",
    "from src.visualize.utils import show_timg, show_timgs, show_batch, make_grid_from_tensors\n",
    "from src.utils.misc import info, get_next_version_path\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataModules\n",
    "from src.data.datamodules import MNISTDataModule, MNISTMDataModule, MonoMNISTDataModule\n",
    "from src.data.datamodules import MultiMonoMNISTDataModule\n",
    "\n",
    "# plModules\n",
    "from src.models.plmodules.vanilla_vae import VanillaVAE\n",
    "from src.models.plmodules.iwae import IWAE\n",
    "from src.models.plmodules.bilatent_vae import BiVAE\n",
    "from src.models.plmodules.three_fcs import ThreeFCs\n",
    "\n",
    "\n",
    "# Evaluations\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pytorch_lightning.utilities.cloud_io import load as pl_load\n",
    "from src.evaluator.qualitative import save_content_transfers, save_style_transfers, run_both_transfers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start experiment \n",
    "Given a maptile, predict its style as one of OSM, CartoVoyager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.plmodules.vanilla_vae import VanillaVAE\n",
    "from src.models.plmodules.bilatent_vae import BiVAE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation NB to load and evaluate a trained model\n",
    "Steps:\n",
    "- Define the architecutre of the model to load\n",
    "- Load the model at `ckpt_path`\n",
    "- Run the following evaluations\n",
    "\n",
    "Evaluations:\n",
    "1. Evaluation of the generative model\n",
    "- Quantitative: `best_score`, which is the lowest loss computed as an average loss per datapt in the validation set. The loss is the estimate of the negative maginal log-likelihood of the observed data based on the trained model\n",
    "\n",
    "- Qualitative: \n",
    "  - Reconstruction of datapts from train/val datasets\n",
    "    - This evaluates how well the generative model (encoder-decoder) preserves the information needed to reconstruct the input data after having learned/trained/optimized jointly with/in the presence of its adversary, the style-classifier/discriminator\n",
    "\n",
    "2. Evaluation of the discriminator\n",
    "- How well does it discriminate? \n",
    "  - based on a style code: the model should predict the style label of the input datapt well\n",
    "    - Compute the `loss_s` over the train/val datasets (as an expectation, ie. loss value per datapt/image)\n",
    "  - based on a content code: the model should say \"I'm not sure, aka. all style labels seem equally probable\"\n",
    "    - Compute the `loss_s` over the train/val datasets (as an expectation, ie. loss value per datapt/image)\n",
    "  - Q: what is the range of the `loss_s` or `loss_c` for a good style-classifer?\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Multisource-Monochrome MNIST datamodule\n",
    "mono_dir = ROOT/'data/Mono-MNIST'\n",
    "colors = ['red', 'green', 'blue']\n",
    "seed = 123\n",
    "in_shape = (3,32,32)\n",
    "batch_size = 128\n",
    "\n",
    "# Create a multi-source dataset\n",
    "dm = MultiMonoMNISTDataModule(\n",
    "    data_root=mono_dir,\n",
    "    colors=colors,\n",
    "    seed=seed,\n",
    "    in_shape=in_shape,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "dm.setup('fit')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_class(model_name: str) -> object:\n",
    "    model_name = model_name.lower()\n",
    "    return {\n",
    "        \"three_fcs\": ThreeFCs,\n",
    "        \"vae\": VanillaVAE,\n",
    "        \"iwae\": IWAE,\n",
    "        \"bivae\": BiVAE,\n",
    "\n",
    "    }[model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init plModule\n",
    "latent_dim = 20\n",
    "hidden_dims = [32,64,128,256]#,512]\n",
    "lr = 3e-3\n",
    "act_fn = nn.ReLU()\n",
    "# Specific for BiVAE\n",
    "adversary_dims = [32,32,32] \n",
    "is_contrasive = False # If true, use adv. loss from both content and style codes. Else just style codes\n",
    "kld_weight = 1.0 # vae_loss = recon_loss + kld_weight * kld_weight\n",
    "adv_loss_weight = 15. # loss = vae_loss + adv_loss_weight * adv_loss\n",
    "\n",
    "model = BiVAE(\n",
    "    in_shape=dm.size(), \n",
    "    n_styles=dm.n_styles,\n",
    "    latent_dim=latent_dim,\n",
    "    hidden_dims=hidden_dims,\n",
    "    adversary_dims=adversary_dims,\n",
    "    learning_rate=lr, \n",
    "    act_fn=act_fn,\n",
    "    size_average=False,\n",
    "    is_contrasive=is_contrasive,\n",
    "    kld_weight=kld_weight,\n",
    "    adv_loss_weight=adv_loss_weight,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ckpt_path(log_dir:Path):\n",
    "    \"\"\"Get the path to the ckpt file from the pytorch-lightning's log_dir of the model\n",
    "    Assume there is a single ckpt file under the .../<model_name>/<version_x>/checkpoints\n",
    "    \n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    log_dir_root = Path(\"/data/hayley-old/Tenanbaum2000/lightning_logs\")    \n",
    "    log_dir = log_dir_root/ \"2021-01-12-ray/BiVAE_MNIST-red-green-blue_seed-123/version_1\"\n",
    "    ckpt_path = get_ckpt_path(log_dir)\n",
    "    # Use the ckpt_path to load the saved model\n",
    "    ckpt = pl_load(ckpt_path, map_location=lambda storage, loc: storage)  # dict object\n",
    "    \n",
    "    \"\"\"\n",
    "    ckpt_dir = log_dir / \"checkpoints\"\n",
    "    for p in ckpt_dir.iterdir():\n",
    "        return p\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir_root = Path(\"/data/hayley-old/Tenanbaum2000/lightning_logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the best models of BiVAE and BiVAE-C\n",
    "  - Constant parameters:\n",
    "    - latent_dim: 10 or 20\n",
    "    - batch_size: 32\n",
    "    - kld_weight = 1.0 (constant)\n",
    "    - no kld annealing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Latent_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Non contrasive model (BiVAE)\n",
    "# # latent dim: 10\n",
    "# # adv_loss_weight: 15\n",
    "# # lr: 1e-4\n",
    "# log_dir = log_dir_root/ \"2021-01-12-ray/BiVAE_MNIST-red-green-blue_seed-123/version_1\"\n",
    "# ckpt_path = get_ckpt_path(log_dir)\n",
    "# ckpt = pl_load(ckpt_path, map_location=lambda storage, loc: storage)  # dict object\n",
    "# print(ckpt.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Contrasive model (BiVAE-C)\n",
    "# # latent dim: 10\n",
    "# # adv_loss_weight: 15\n",
    "# # lr: 1e-3\n",
    "# log_dir = log_dir_root/ \"2021-01-12-ray/BiVAE-C_MNIST-red-green-blue_seed-123/version_1\"\n",
    "# ckpt_path = get_ckpt_path(log_dir)\n",
    "# ckpt = pl_load(ckpt_path, map_location=lambda storage, loc: storage)  # dict object\n",
    "# print(ckpt.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Latent dim = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Non contrasive model (BiVAE)\n",
    "# # latent dim: 20\n",
    "# # adv_loss_weight: 45\n",
    "# # lr: 1e-4\n",
    "# log_dir = log_dir_root/ \"2021-01-14-ray/BiVAE_MNIST-red-green-blue_seed-123/version_21\"\n",
    "# ckpt_path = get_ckpt_path(log_dir)\n",
    "# ckpt = pl_load(ckpt_path, map_location=lambda storage, loc: storage)  # dict object\n",
    "# print(ckpt.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4\n",
    "# Contrasive model (BiVAE-C)\n",
    "# latent dim: 20\n",
    "# adv_loss_weight: 15\n",
    "# lr: 1e-4\n",
    "log_dir = log_dir_root/ \"2021-01-14-ray/BiVAE-C_MNIST-red-green-blue_seed-123/version_1\"\n",
    "ckpt_path = get_ckpt_path(log_dir)\n",
    "ckpt = pl_load(ckpt_path, map_location=lambda storage, loc: storage)  # dict object\n",
    "print(ckpt.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(ckpt['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "## Define a TB writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the root log_dir correpsonding to the ckpt_path\n",
    "log_dir = ckpt_path.parent.parent # eg. Folder called `temp-logs/f{model.name+dm.name}/version7`\n",
    "tb_writer = SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_writer.log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recons of inputs from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluator.qualitative import show_recon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_global_step = ckpt['global_step']\n",
    "best_global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_recon(\n",
    "#     model=model, \n",
    "#     dm=dm, \n",
    "#     tb_writer=tb_writer, \n",
    "#     global_step=best_global_step, \n",
    "#     verbose=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Content/Style Transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_reps(dl: DataLoader) -> Dict[Union[str,int], torch.Tensor]:\n",
    "    ds = dl.dataset\n",
    "    class_reps = {}\n",
    "    for i in range(len(ds)):\n",
    "        if len(class_reps) >= 10:\n",
    "            break\n",
    "        try:\n",
    "            x,label_c = ds[i]\n",
    "        except ValueError as e:\n",
    "            batch = ds[i]\n",
    "            x = batch['img']\n",
    "            label_c = batch['digit']\n",
    "            label_s = batch['color']\n",
    "        if isinstance(label_c, torch.Tensor):\n",
    "            label_c = label_c.item()\n",
    "        label_c = str(label_c)\n",
    "        if label_c in class_reps:\n",
    "            continue\n",
    "        class_reps[label_c] = x\n",
    "    return class_reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_reps = get_class_reps(dm.train_dataloader())\n",
    "\n",
    "# Show content-representative images\n",
    "# for label_c, timg in class_reps.items():\n",
    "#     show_timg(timg, title=label_c)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = save_content_transfers(model,\n",
    "                      class_reps=class_reps, \n",
    "                       log_dir=log_dir, \n",
    "                       train_mean=dm.train_mean, \n",
    "                       train_std=dm.train_std)\n",
    "show_timgs(results, nrows=1, factor=10.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = save_style_transfers(model,\n",
    "                      class_reps=class_reps, \n",
    "                       log_dir=log_dir, \n",
    "                       train_mean=dm.train_mean, \n",
    "                       train_std=dm.train_std)\n",
    "show_timgs(results, nrows=1, factor=10.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_both_transfers(model,\n",
    "#                   class_reps=class_reps, \n",
    "#                        log_dir=log_dir, \n",
    "#                        train_mean=dm.train_mean, \n",
    "#                        train_std=dm.train_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis on the latent space\n",
    "1. Visualize embeddings\n",
    "    - collect a batch of inputs -> encoder -> [mu, log_var] -> sample -> a batch of z's (embeddings)\n",
    "    - use tb logger\n",
    "  \n",
    "2. Nearest neighbor query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize embeddings\n",
    "- collect a batch of inputs -> encoder -> [mu, log_var] -> sample -> a batch of z's (embeddings)\n",
    "- use tb logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "best_global_step = 0#ckpt[\"global_step\"]+1\n",
    "\n",
    "dl = DataLoader(dm.train_ds, \n",
    "                batch_size=128,\n",
    "               num_workers=16,\n",
    "               pin_memory=True,\n",
    "               )\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dl:\n",
    "        batch = next(iter(dl))\n",
    "        x = batch['img']\n",
    "        label_c = batch['digit']\n",
    "        label_s = batch['color']\n",
    "        \n",
    "        dict_qparams = model.encode(x)\n",
    "        dict_z = model.rsample(dict_qparams)\n",
    "\n",
    "    #     z = out['z']\n",
    "\n",
    "        # log embedding of z_c to tensorboard \n",
    "        tb_writer.add_embedding(dict_z['c'],\n",
    "                             label_img=LinearRescaler()(x), \n",
    "                             metadata=label_c.tolist(),\n",
    "                             global_step=best_global_step, \n",
    "                             tag=\"c\"\n",
    "                            )\n",
    "\n",
    "        # log embedding of z_s to tensorboard \n",
    "        tb_writer.add_embedding(dict_z['s'],\n",
    "                             label_img=LinearRescaler()(x), \n",
    "                             metadata=label_s.tolist(),\n",
    "                             global_step=best_global_step, \n",
    "                             tag=\"s\"\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize original images of the close neighbors in the latent space\n",
    "- Compute pairwise distance using cosine similarity\n",
    "- For each row (ie. a latent code), get the index of the smallest values. \n",
    "- Select the images in the batch x and visualize (can do this all in show_timgs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "mode = 'train'\n",
    "query_size = 256 #1024\n",
    "metric = 'cosine' #pairwise distance metric in content space\n",
    "ds = getattr(dm, f\"{mode}_ds\")\n",
    "dl = DataLoader(ds, batch_size=query_size, shuffle=True)\n",
    "# tsne params\n",
    "tsne_dim = 2\n",
    "tsne_p = 5. #10 #perplexity\n",
    "# tsne_metric = 'euclidean'\n",
    "tsne_metric = 'cosine' \n",
    "tsne = TSNE(n_components=tsne_dim, metric=tsne_metric, perplexity=tsne_p )\n",
    "with torch.no_grad():\n",
    "    batch = next(iter(dl))\n",
    "    x = batch['img']\n",
    "    label_c = batch['digit']  # digit/content label (int) -- currently not used\n",
    "    label_s = batch['color']\n",
    "\n",
    "    dict_qparams = model.encode(x)\n",
    "    dict_z = model.rsample(dict_qparams)\n",
    "    c = dict_z['c']\n",
    "    s = dict_z['s']\n",
    "    z = model.combine_content_style(dict_z)\n",
    "\n",
    "    for name, embedding in zip([\"c\", \"s\", \"z\"], [c,s,z]):\n",
    "        # Compute pairwise distance of the embeddings\n",
    "        pdists = pairwise_distances(embedding.numpy(), metric=metric)\n",
    "        plt.imshow(pdists, cmap='gray')\n",
    "        plt.title(f\"Pairwise dists of {name}'s\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        # show the 2dim view on the codes\n",
    "        embedding_2d = tsne.fit_transform(embedding)\n",
    "        \n",
    "        f, ax = plt.subplots(1,2, figsize=(20,10))\n",
    "        # first plot the 2dim embeddings and color-code by content id\n",
    "        ax[0].scatter(embedding_2d[:,0], embedding_2d[:,1],\n",
    "                     c = label_c)\n",
    "        \n",
    "        ax[0].set_title(f\"Code: {name}, colored by content-id\")\n",
    "        \n",
    "        # same embedding plot, but color-code by style-id\n",
    "        ax[1].scatter(embedding_2d[:,0], embedding_2d[:,1],\n",
    "                     c = label_s),\n",
    "        ax[1].set_title(f\"Code: {name}, colored by style-id\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "#         # Nearest neighbor queries\n",
    "#         # smaller values means closer in distance\n",
    "#         n_ngbrs = 10\n",
    "#         n_rows = min(query_size, 32)\n",
    "\n",
    "#         selected_rows = np.random.choice(len(x), size=n_rows)\n",
    "#         for idx in selected_rows:\n",
    "#             args = np.argsort(pdists[idx])[:n_ngbrs]\n",
    "#     #         print(args)\n",
    "#             show_timgs(LinearRescaler()(x[args]), cmap='gray', factor=2, \n",
    "#                        nrows=1, title=f'Nearest of digit {y[idx].item()}: {name}')\n",
    "#             plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of qparams for content/style codes\n",
    "Given a batch of images with the same content, \n",
    "we hypothesize/an ideally leanred model will put their content codes in a close neighborhood in the content latent space. Then, the average of a batch of mu_qc's will be a vector that indicates the center/mean of the mu_qc of each image (whose content-id is the same across the input batch).\n",
    "\n",
    "- Show the histogram of each dimension's mu_qc_j across the batch: one histogram for each dimension j\n",
    "- Compute the average mu_qc over the batch of mu_qc's. Then, use it as the input content code with some input image's style code to the generator. What is the generated output? Does its content look something like the content of the batch of image?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_xlim = (-4., 4.)\n",
    "# var_xlim = (0, 0.05)\n",
    "n_samples = 1024\n",
    "# Set output dir\n",
    "out_dir = log_dir/\"dist_qparams_per_content\"\n",
    "if not out_dir.exists():\n",
    "    out_dir.mkdir(parents=True)\n",
    "    print(\"Created and saving to: \", out_dir)\n",
    "    \n",
    "for digit_id in range(10):\n",
    "    # Collect a batch of images of the same content\n",
    "    xs = []\n",
    "    n_collected = 0\n",
    "    while n_collected <= n_samples:\n",
    "        batch = next(iter(dl))\n",
    "        x = batch['img']\n",
    "        label_c = batch['digit']\n",
    "        label_s = batch['color']\n",
    "        selected = x[label_c==digit_id]\n",
    "        xs.append(selected)\n",
    "        n_collected += len(selected)\n",
    "    xs = torch.cat(xs, dim=0)\n",
    "    print(f\" Digit {digit_id} collected. Input: {xs.shape}\")\n",
    "\n",
    "\n",
    "    # Plot the distributions of qc parameters and qs parameters\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dict_qparams = model(xs)\n",
    "\n",
    "        # Dist. of each content dim's parameters\n",
    "        mu_qc, var_qc = dict_qparams['mu_qc'], dict_qparams['logvar_qc'].exp() #(BS, content_dim), (BS, style_dim)\n",
    "        # -- mu_qc's\n",
    "#         mu_xlim = (0, max(mu_qc)\n",
    "        f, ax = plt.subplots(1, model.content_dim, figsize=(20,2))\n",
    "        title = f\"Digit {digit_id}: \" + r\"Dist. of $mu^{c}_j$\"\n",
    "        f.suptitle(title)\n",
    "        for j in range(model.content_dim):\n",
    "            ax[j].hist(mu_qc[:,j])\n",
    "            ax[j].set_xlim(mu_xlim)\n",
    "            ax[j].set_title(f\"dim {j}\")\n",
    "        f.tight_layout()\n",
    "        f.savefig(out_dir/f\"digit-{digit_id}-mu_qc.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        # -- var_qc's\n",
    "        var_xlim = (0, var_qc.max().item())\n",
    "        f, ax = plt.subplots(1, model.style_dim, figsize=(20,2))\n",
    "        title = f\"Digit {digit_id}: \" + r\"Dist. of $var^{c}_j$\"\n",
    "        f.suptitle(title)\n",
    "        for j in range(model.content_dim):\n",
    "            ax[j].hist(var_qc[:,j])\n",
    "            ax[j].set_xlim(var_xlim)\n",
    "            ax[j].set_title(f\"dim {j}\")\n",
    "        f.tight_layout()\n",
    "        f.savefig(out_dir/f\"digit-{digit_id}-var_qc.png\")\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        # Dist. of each style dim's parameters\n",
    "        mu_qs, var_qs = dict_qparams['mu_qs'], dict_qparams['logvar_qs'].exp()\n",
    "        # -- mu_qs's\n",
    "#         mu_xlim = (0, max(mu_qs))\n",
    "        f, ax = plt.subplots(1, model.style_dim, figsize=(20,2))\n",
    "        title = f\"Digit {digit_id}: \" + r\"Dist. of $mu^{s}_j$\"\n",
    "        f.suptitle(title)\n",
    "        for j in range(model.style_dim):\n",
    "            ax[j].hist(mu_qs[:,j])\n",
    "            ax[j].set_xlim(mu_xlim)\n",
    "            ax[j].set_title(f\"dim {j}\")\n",
    "        f.tight_layout()\n",
    "        f.savefig(out_dir/f\"digit-{digit_id}-mu_qs.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        # -- var_qs's\n",
    "        var_xlim = (0, var_qs.max().item())\n",
    "        f, ax = plt.subplots(1, model.style_dim, figsize=(20,2))\n",
    "        title = f\"Digit {digit_id}: \" + r\"Dist. of $var^{s}_j$\"\n",
    "        f.suptitle(title)\n",
    "        for j in range(model.style_dim):\n",
    "            ax[j].hist(var_qs[:,j])\n",
    "            ax[j].set_xlim(var_xlim)\n",
    "            ax[j].set_title(f\"dim {j}\")\n",
    "        f.tight_layout()\n",
    "        f.savefig(out_dir/f\"digit-{digit_id}-var_qs.png\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Collect a batch of images with the same style: one such batch for style.\n",
    "Then plot the histogram of mu_qs_j over the batch: one histogram for a dimension of a style code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_xlim = (-4., 4.)\n",
    "n_styles = dm.n_styles\n",
    "# var_xlim = (0, 0.05)\n",
    "n_samples = 1024\n",
    "# Set output dir\n",
    "out_dir = log_dir/\"dist_qparams_per_style\"\n",
    "if not out_dir.exists():\n",
    "    out_dir.mkdir(parents=True)\n",
    "    print(\"Created and saving to: \", out_dir)\n",
    "    \n",
    "for style_id in range(n_styles):\n",
    "    # Collect a batch of images of the same content\n",
    "    xs = []\n",
    "    n_collected = 0\n",
    "    while n_collected <= n_samples:\n",
    "        batch = next(iter(dl))\n",
    "        x = batch['img']\n",
    "        label_c = batch['digit']\n",
    "        label_s = batch['color']\n",
    "        selected = x[label_c==style_id]\n",
    "        xs.append(selected)\n",
    "        n_collected += len(selected)\n",
    "    xs = torch.cat(xs, dim=0)\n",
    "    print(f\"Style {style_id} collected. Input: {xs.shape}\")\n",
    "\n",
    "\n",
    "    # Plot the distributions of qc parameters and qs parameters\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dict_qparams = model(xs)\n",
    "\n",
    "        # Dist. of each content dim's parameters\n",
    "        mu_qc, var_qc = dict_qparams['mu_qc'], dict_qparams['logvar_qc'].exp() #(BS, content_dim), (BS, style_dim)\n",
    "        # -- mu_qc's\n",
    "#         mu_xlim = (0, max(mu_qc)\n",
    "        f, ax = plt.subplots(1, model.content_dim, figsize=(20,2))\n",
    "        title = f\"Style {style_id}: \" + r\"Dist. of $mu^{c}_j$\"\n",
    "        f.suptitle(title)\n",
    "        for j in range(model.content_dim):\n",
    "            ax[j].hist(mu_qc[:,j])\n",
    "            ax[j].set_xlim(mu_xlim)\n",
    "            ax[j].set_title(f\"dim {j}\")\n",
    "        f.tight_layout()\n",
    "        f.savefig(out_dir/f\"Style-{style_id}-mu_qc.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        # -- var_qc's\n",
    "        var_xlim = (0, var_qc.max().item())\n",
    "        f, ax = plt.subplots(1, model.style_dim, figsize=(20,2))\n",
    "        title = f\"Style {stlye_id}: \" + r\"Dist. of $var^{c}_j$\"\n",
    "        f.suptitle(title)\n",
    "        for j in range(model.content_dim):\n",
    "            ax[j].hist(var_qc[:,j])\n",
    "            ax[j].set_xlim(var_xlim)\n",
    "            ax[j].set_title(f\"dim {j}\")\n",
    "        f.tight_layout()\n",
    "        f.savefig(out_dir/f\"Style-{stlye_id}-var_qc.png\")\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        # Dist. of each style dim's parameters\n",
    "        mu_qs, var_qs = dict_qparams['mu_qs'], dict_qparams['logvar_qs'].exp()\n",
    "        # -- mu_qs's\n",
    "#         mu_xlim = (0, max(mu_qs))\n",
    "        f, ax = plt.subplots(1, model.style_dim, figsize=(20,2))\n",
    "        title = f\"Style {stlye_id}: \" + r\"Dist. of $mu^{s}_j$\"\n",
    "        f.suptitle(title)\n",
    "        for j in range(model.style_dim):\n",
    "            ax[j].hist(mu_qs[:,j])\n",
    "            ax[j].set_xlim(mu_xlim)\n",
    "            ax[j].set_title(f\"dim {j}\")\n",
    "        f.tight_layout()\n",
    "        f.savefig(out_dir/f\"Style-{stlye_id}-mu_qs.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        # -- var_qs's\n",
    "        var_xlim = (0, var_qs.max().item())\n",
    "        f, ax = plt.subplots(1, model.style_dim, figsize=(20,2))\n",
    "        title = f\"Style {stlye_id}: \" + r\"Dist. of $var^{s}_j$\"\n",
    "        f.suptitle(title)\n",
    "        for j in range(model.style_dim):\n",
    "            ax[j].hist(var_qs[:,j])\n",
    "            ax[j].set_xlim(var_xlim)\n",
    "            ax[j].set_title(f\"dim {j}\")\n",
    "        f.tight_layout()\n",
    "        f.savefig(out_dir/f\"Style-{stlye_id}-var_qs.png\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Space Traversal\n",
    "1. Linear traversal in a single dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_dim = 0 # must be in range(latent_dim)\n",
    "fixed_vec = torch.randn((1, model.latent_dim-1))\n",
    "fixed_values = fixed_vec.repeat((n_samples,1))\n",
    "n_samples = 16\n",
    "zi_min, zi_max = -2,2\n",
    "varying = torch.linspace(zi_min, zi_max, n_samples).view((-1,1))\n",
    "\n",
    "varying.shape,fixed_values.shape\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_from(a_col:torch.Tensor, other_cols:torch.Tensor, ind):\n",
    "    \"\"\"\n",
    "    Make a tensor from a column vector and a matrx containing all the other columns\n",
    "    by inserting the `onc_column` at the final matrix's `ind`th column.\n",
    "    \"\"\"\n",
    "    assert a_\n",
    "    n_cols = 1 + \n",
    "    out = a_col.new_zeros(("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test]",
   "language": "python",
   "name": "conda-env-test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
