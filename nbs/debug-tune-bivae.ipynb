{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-details",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "false-productivity",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acquired-grounds",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "threatened-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import re\n",
    "import math\n",
    "from datetime import datetime\n",
    "import time\n",
    "sys.dont_write_bytecode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "lesser-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Set, Dict, Tuple, Optional, Iterable, Mapping, Union, Callable, TypeVar\n",
    "\n",
    "from pprint import pprint\n",
    "from ipdb import set_trace as brpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cardiac-upset",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from  torch.linalg import norm as tnorm\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.tuner.tuning import Tuner\n",
    "\n",
    "\n",
    "# Select Visible GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-evaluation",
   "metadata": {},
   "source": [
    "## Set Path \n",
    "1. Add project root and src folders to `sys.path`\n",
    "2. Set DATA_ROOT to `maptile_v2` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "copyrighted-fishing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root:  /data/hayley-old/Tenanbaum2000\n",
      "Src folder:  /data/hayley-old/Tenanbaum2000/src\n",
      "This nb path:  /data/hayley-old/Tenanbaum2000/nbs\n",
      "\n",
      "/data/hayley-old/Tenanbaum2000 added to the path.\n"
     ]
    }
   ],
   "source": [
    "this_nb_path = Path(os.getcwd())\n",
    "ROOT = this_nb_path.parent\n",
    "SRC = ROOT/'src'\n",
    "DATA_ROOT = Path(\"/data/hayley-old/maptiles_v2/\")\n",
    "paths2add = [this_nb_path, ROOT]\n",
    "\n",
    "print(\"Project root: \", str(ROOT))\n",
    "print('Src folder: ', str(SRC))\n",
    "print(\"This nb path: \", str(this_nb_path))\n",
    "\n",
    "\n",
    "for p in paths2add:\n",
    "    if str(p) not in sys.path:\n",
    "        sys.path.insert(0, str(p))\n",
    "        print(f\"\\n{str(p)} added to the path.\")\n",
    "        \n",
    "# print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "incorrect-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transforms\n",
    "from src.data.transforms.transforms import Identity, Unnormalizer, LinearRescaler\n",
    "from src.data.transforms.functional import unnormalize\n",
    "\n",
    "# Utils\n",
    "from src.visualize.utils import show_timg, show_timgs, show_batch, make_grid_from_tensors\n",
    "from src.utils.misc import info, get_next_version_path\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dimensional-package",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataModules\n",
    "from src.data.datamodules import MNISTDataModule, MNISTMDataModule, MonoMNISTDataModule\n",
    "from src.data.datamodules import MultiMonoMNISTDataModule\n",
    "from src.data.datamodules.multisource_rotated_mnist_datamodule import MultiRotatedMNISTDataModule\n",
    "from src.data.datamodules.multisource_maptiles_datamodule import MultiMaptilesDataModule\n",
    "\n",
    "\n",
    "# plModules\n",
    "from src.models.plmodules.vanilla_vae import VanillaVAE\n",
    "from src.models.plmodules.iwae import IWAE\n",
    "from src.models.plmodules.bilatent_vae import BiVAE\n",
    "\n",
    "# Evaluations\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pytorch_lightning.utilities.cloud_io import load as pl_load\n",
    "from src.evaluator.qualitative import save_content_transfers, save_style_transfers, run_both_transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "comparable-sentence",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find the best hparam setting for a specific BiVAE model trained on a specific datamodule.\n",
    "\n",
    "Required args:\n",
    "    --model_name: eg. \"vae\", \"iwae\", \"bivae\"\n",
    "    --data_name: eg. \"maptiles\", \"mnist\", \"multi_mono_mnist\"\n",
    "    --latent_dim: int, eg. 10\n",
    "\n",
    "Optional args: (partial)\n",
    "    --hidden_dims: eg. --hidden_dims 32 64 128 256 (which is default)\n",
    "\n",
    "Hyperparameter space:\n",
    "- latent_dim = [16, 32, 63, 128]\n",
    "- is_contrasive =  [False, True]\n",
    "- kld_weight = [\n",
    "- adv_loss_weight = [5, 15, 45, 135, 405, 1215]\n",
    "- batch_size = [32, 64, 128, 256, 514, 1028]\n",
    "- learning_rate =\n",
    "\n",
    "To run: (at the root of the project, ie. /data/hayley-old/Tenanbaum2000\n",
    "# Values for adv_weight, latent_dim, batch_size, lr, is_contrasive will be overwritten\n",
    "# as the searched hyperparmeter values\n",
    "\n",
    " nohup python tune_hparams_bivae.py --model_name=\"bivae\" \\\n",
    "--latent_dim=10 --hidden_dims 32 64 128 256 --adv_dim 32 32 32 --adv_weight 15.0 \\\n",
    "--data_name=\"multi_mono_mnist\" --colors red green blue --n_styles=3 \\\n",
    "--gpu_id=2 --max_epochs=300 --batch_size=128 -lr 1e-3  --terminate_on_nan=True  \\\n",
    "--log_root=\"/data/hayley-old/Tenanbaum2000/lightning_logs/2021-01-13-ray/\" &\n",
    "\n",
    " nohup python tune_hparams_bivae.py --model_name=\"bivae\" \\\n",
    "--latent_dim=10 --hidden_dims 32 64 128 256 --adv_dim 32 32 32 --adv_weight 15.0 \\\n",
    "--use_beta_scheduler \\\n",
    "--data_name=\"multi_mono_mnist\" --colors red green blue --n_styles=3 \\\n",
    "--gpu_id=2 --max_epochs=300 --batch_size=128 -lr 1e-3  --terminate_on_nan=True  \\\n",
    "--log_root=\"/data/hayley-old/Tenanbaum2000/lightning_logs/2021-01-14-ray/\" &\n",
    "\n",
    "# View the Ray dashboard at http://127.0.0.1:8265\n",
    "# Run this at  local terminal:\n",
    "# ssh -NfL 8265:localhost:8265 arya\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "from argparse import ArgumentParser, Namespace\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "from typing import List, Set,Any, Dict, Tuple, Optional, Iterable, Mapping, Union, Callable, TypeVar\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "\n",
    "# Ray\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "\n",
    "from src.callbacks.recon_logger import ReconLogger\n",
    "from src.callbacks.hist_logger import  HistogramLogger\n",
    "from src.callbacks.beta_scheduler import BetaScheduler\n",
    "\n",
    "# src helpers\n",
    "from src.utils.misc import info, n_iter_per_epoch\n",
    "from src.models.model_wrapper import ModelWrapper\n",
    "\n",
    "# utils for instatiating a selected datamodule and a selected model\n",
    "from utils import get_model_class, get_dm_class\n",
    "from utils import instantiate_model, instantiate_dm\n",
    "from utils import add_base_arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "restricted-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tune(args: Union[Dict, Namespace]):\n",
    "    # Init. datamodule and model\n",
    "    dm = instantiate_dm(args)\n",
    "    dm.setup('fit')\n",
    "    model = instantiate_model(args)\n",
    "\n",
    "    # Specify logger\n",
    "    exp_name = f'{model.name}_{dm.name}'\n",
    "    print('Exp name: ', exp_name)\n",
    "    tb_logger = pl_loggers.TensorBoardLogger(save_dir=args.log_root,\n",
    "                                             name=exp_name,\n",
    "                                             default_hp_metric=False,\n",
    "                                             )\n",
    "    log_dir = Path(tb_logger.log_dir)\n",
    "    print(\"Log Dir: \", log_dir)\n",
    "    # breakpoint()\n",
    "    if not log_dir.exists():\n",
    "        log_dir.mkdir(parents=True)\n",
    "        print(\"Created: \", log_dir)\n",
    "\n",
    "    # Specify callbacks\n",
    "    callbacks = [\n",
    "        LearningRateMonitor(logging_interval='epoch'),\n",
    "        TuneReportCallback(\n",
    "            {\n",
    "            'loss': 'val_loss',\n",
    "            'mean_accuracy': 'val/style_acc', # use the string after pl.Module's \"self.log(\"\n",
    "            },\n",
    "            on=\"validation_end\"\n",
    "        ),\n",
    "        # HistogramLogger(hist_epoch_interval=args.hist_epoch_interval),\n",
    "        # ReconLogger(recon_epoch_interval=args.recon_epoch_interval),\n",
    "        #         EarlyStopping('val_loss', patience=10),\n",
    "    ]\n",
    "    if args.use_beta_scheduler:\n",
    "        max_iters = n_iter_per_epoch(dm.train_dataloader()) * args.max_epochs\n",
    "        callbacks.append(BetaScheduler(max_iters,\n",
    "                                       start=args.beta_start,\n",
    "                                       stop=args.beta_stop,\n",
    "                                       n_cycle=args.beta_n_cycle,\n",
    "                                       ratio=args.beta_ratio,\n",
    "                                       log_tag=args.beta_log_tag))\n",
    "\n",
    "    trainer_overwrites = {\n",
    "        'gpus':1, #use a single gpu\n",
    "        'progress_bar_refresh_rate':0, # don't print out progress bar\n",
    "        'terminate_on_nan':True,\n",
    "        'check_val_every_n_epoch':10,\n",
    "        'logger': tb_logger,\n",
    "        'callbacks': callbacks\n",
    "    }\n",
    "\n",
    "    # Init. trainer\n",
    "    trainer = pl.Trainer.from_argparse_args(args, **trainer_overwrites)\n",
    "\n",
    "    # Log model's computational graph\n",
    "    model_wrapper = ModelWrapper(model)\n",
    "    # tb_logger.experiment.add_graph(model_wrapper, model.)\n",
    "    tb_logger.log_graph(model_wrapper)\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Run the experiment\n",
    "    # ------------------------------------------------------------------------\n",
    "    start_time = time.time()\n",
    "    print(f\"{exp_name} started...\")\n",
    "    print(f\"Logging to {Path(tb_logger.log_dir).absolute()}\")\n",
    "    trainer.fit(model, dm)\n",
    "    print(f\"Finished at ep {trainer.current_epoch, trainer.batch_idx}\")\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Log the best score and current experiment's hyperparameters\n",
    "    # ------------------------------------------------------------------------\n",
    "    hparams = model.hparams.copy()\n",
    "    hparams.update(dm.hparams)\n",
    "    best_score = trainer.checkpoint_callback.best_model_score.item()\n",
    "    metrics = {'hparam/best_score': best_score}  # todo: define a metric and use it here\n",
    "    trainer.logger.log_hyperparams(hparams, metrics)\n",
    "\n",
    "    print(\"Logged hparams and metrics...\")\n",
    "    print(\"\\t hparams: \")\n",
    "    pprint(hparams)\n",
    "    print(\"=====\")\n",
    "    print(\"\\t metrics: \", metrics)\n",
    "    print(f\"Training Done: took {time.time() - start_time}\")\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Evaluation\n",
    "    #   1. Reconstructions:\n",
    "    #     x --> model.encoder(x) --> theta_z\n",
    "    #     --> sample N latent codes from the Pr(z; theta_z)\n",
    "    #     --> model.decoder(z) for each sampled z's\n",
    "    #   2. Embedding:\n",
    "    #       a mini-batch input -> mu_z, logvar_z\n",
    "    #       -> rsample\n",
    "    #       -> project to 2D -> visualize\n",
    "    #   3. Inspect the topology/landscape of the learned latent space\n",
    "    #     Latent traversal: Pick a dimension of the latent space.\n",
    "    #     - Keep all other dimensions' values constant.\n",
    "    #     - Vary the chosen dimenion's values (eg. linearly, spherically)\n",
    "    #     - and decode the latent codes. Show the outputs of the decoder.\n",
    "    #   4. Marginal Loglikelihood of train/val/test dataset\n",
    "    # ------------------------------------------------------------------------\n",
    "    # print(\"Evaluations...\")\n",
    "    # model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "further-serbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(user_args: List[str]):\n",
    "    parser = ArgumentParser()\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Add general arguments for this CLI script for training/testing\n",
    "    # ------------------------------------------------------------------------\n",
    "    parser = add_base_arguments(parser)\n",
    "    args, unknown = parser.parse_known_args(user_args)\n",
    "    print(\"Base CLI args: \")\n",
    "    pprint(args)\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Add model/datamodule/trainer specific args\n",
    "    # ------------------------------------------------------------------------\n",
    "    model_class = get_model_class(args.model_name)\n",
    "    dm_class = get_dm_class(args.data_name)\n",
    "    parser = model_class.add_model_specific_args(parser)\n",
    "    parser = dm_class.add_model_specific_args(parser)\n",
    "    parser = pl.Trainer.add_argparse_args(parser)\n",
    "\n",
    "    # RayTune args\n",
    "    parser.add_argument('--n_cpus',  type=int, default=8, help='Num of CPUs per trial')\n",
    "    parser.add_argument(\"--gpu_ids\", type=str, required=True, nargs='*',\n",
    "                        help=\"GPU ID(s) to use\") #Returns an empty list if not specified\n",
    "    parser.add_argument(\"--n_ray_samples\", type=int, default=1,\n",
    "                         help=\"Num of Ray Tune's run argument, num_samples\")\n",
    "    parser.add_argument(\"--ray_log_dir\", type=str, default=\"/data/log/ray\",\n",
    "                        help=\"dir to save training results from Ray\")\n",
    "    # Callback switch args\n",
    "    parser = BetaScheduler.add_argparse_args(parser)\n",
    "    # parser.add_argument(\"--hist_epoch_interval\", type=int, default=10, help=\"Epoch interval to plot histogram of q's parameter\")\n",
    "    # parser.add_argument(\"--recon_epoch_interval\", type=int, default=10, help=\"Epoch interval to plot reconstructions of train and val samples\")\n",
    "    args = parser.parse_args(user_args)\n",
    "    print(\"Final args: \")\n",
    "    pprint(args)\n",
    "\n",
    "    # Select Visible GPU\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(args.gpu_ids)\n",
    "    print(\"===GPUs===\")\n",
    "    print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "\n",
    "    def set_hparam_and_train_closure(config: Dict[str, Any]):\n",
    "        \"\"\"Use the (k,v) in `overwrite` to update the args\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        config: Hyperparam search space as a Dict[hparam-name, value of the hpamram]\n",
    "            This dict object is a sample point from the Ray's Hyperparameter space,\n",
    "            and will be used to overwrite the `args`'s key-value with its key-value.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None. Train the model in the specified hyperparmeter space\n",
    "        \"\"\"\n",
    "        print(\"Inside the clousure===\")\n",
    "        pprint(args)\n",
    "        print(\"===\")\n",
    "        pprint(config)\n",
    "\n",
    "        d_args =  vars(args)\n",
    "        for k, v in config.items():\n",
    "            d_args[k] = v\n",
    "            print(\"Overwrote args: \", k)\n",
    "\n",
    "        # Start experiment with this overwritten hyperparams\n",
    "        train_tune(args)\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Specify hyperparameter search space\n",
    "    # ------------------------------------------------------------------------\n",
    "    search_space = {\n",
    "        # \"latent_dim\": tune.grid_search([10, 20, 60, 100]),\n",
    "        'enc_type': tune.choice(['conv', 'resnet']),\n",
    "        'dec_type': tune.choice(['conv', 'resnet']),\n",
    "        'is_contrasive': tune.choice([False, True]),\n",
    "        'kld_weight': tune.choice(np.array([0.5*(2**i) for i in range(12)])), #[0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32., 64, 128., 256, 512, 1024]), #np.array([0.5*(2**i) for i in range(12)])\n",
    "        'use_beta_scheduler': False, #tune.grid_search([False,True]),\n",
    "        'adv_loss_weight': tune.choice(np.logspace(0.0, 7.0, num=8, base=3.0)),\n",
    "        'learning_rate': tune.loguniform(1e-4, 1e-1), #tune.grid_search(list(np.logspace(-4., -1, num=10))),\n",
    "        'batch_size': tune.choice([32, 64, 128,]),\n",
    "    }\n",
    "    \n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Start hyperparameter search using Ray\n",
    "    # ------------------------------------------------------------------------\n",
    "#     ray.shutdown()\n",
    "#     ray.init(log_to_driver=False)\n",
    "    # search_alg =\n",
    "\n",
    "    reporter = CLIReporter(\n",
    "        parameter_columns=list(search_space.keys()),\n",
    "        metric_columns=[\"loss\", \"mean_accuracy\", \"training_iteration\"])\n",
    "    \n",
    "#     breakpoint()\n",
    "    \n",
    "    analysis = tune.run(\n",
    "        set_hparam_and_train_closure,\n",
    "        config=search_space,\n",
    "        metric='loss', #set to val_loss\n",
    "        mode='min',\n",
    "        # search_alg=search_alg,\n",
    "        num_samples=args.n_ray_samples,\n",
    "        verbose=1,\n",
    "        progress_reporter=reporter,\n",
    "        name=\"Tune-BiVAE\", # name of experiment\n",
    "        local_dir= args.ray_log_dir,\n",
    "        resources_per_trial={\"cpu\":args.n_cpus, \"gpu\": len(args.gpu_ids)}, # there are 16cpus in arya machine; so at a time 16/2=8 trials will be run concurrently\n",
    "    )\n",
    "    print(\"Best hyperparameters found were: \", analysis.best_config)\n",
    "\n",
    "    dfs = analysis.fetch_trial_dataframes()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#         # Debug\n",
    "#     config = {\n",
    "#         # \"latent_dim\": tune.grid_search([10, 20, 60, 100]),\n",
    "#         'enc_type': 'conv',\n",
    "#         'dec_type': 'resnet',\n",
    "#         'is_contrasive': False,\n",
    "#         'kld_weight': 1.0, \n",
    "#         'use_beta_scheduler': False, #tune.grid_search([False,True]),\n",
    "#         'adv_loss_weight': 1.0, \n",
    "#         'learning_rate': 1e-4, \n",
    "#         'batch_size': 32,\n",
    "#     }\n",
    "#     set_hparam_and_train_closure(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "coupled-vaccine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(user_args: List[str]):\n",
    "    parser = ArgumentParser()\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Add general arguments for this CLI script for training/testing\n",
    "    # ------------------------------------------------------------------------\n",
    "    parser = add_base_arguments(parser)\n",
    "    args, unknown = parser.parse_known_args(user_args)\n",
    "    print(\"Base CLI args: \")\n",
    "    pprint(args)\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Add model/datamodule/trainer specific args\n",
    "    # ------------------------------------------------------------------------\n",
    "    model_class = get_model_class(args.model_name)\n",
    "    dm_class = get_dm_class(args.data_name)\n",
    "    parser = model_class.add_model_specific_args(parser)\n",
    "    parser = dm_class.add_model_specific_args(parser)\n",
    "    parser = pl.Trainer.add_argparse_args(parser)\n",
    "\n",
    "    # RayTune args\n",
    "    parser.add_argument('--n_cpus',  type=int, default=8, help='Num of CPUs per trial')\n",
    "    parser.add_argument(\"--gpu_ids\", type=str, required=True, nargs='*',\n",
    "                        help=\"GPU ID(s) to use\") #Returns an empty list if not specified\n",
    "    parser.add_argument(\"--n_ray_samples\", type=int, default=1,\n",
    "                         help=\"Num of Ray Tune's run argument, num_samples\")\n",
    "    parser.add_argument(\"--ray_log_dir\", type=str, default=\"/data/log/ray\",\n",
    "                        help=\"dir to save training results from Ray\")\n",
    "    # Callback switch args\n",
    "    parser = BetaScheduler.add_argparse_args(parser)\n",
    "    # parser.add_argument(\"--hist_epoch_interval\", type=int, default=10, help=\"Epoch interval to plot histogram of q's parameter\")\n",
    "    # parser.add_argument(\"--recon_epoch_interval\", type=int, default=10, help=\"Epoch interval to plot reconstructions of train and val samples\")\n",
    "    args = parser.parse_args(user_args)\n",
    "    print(\"Final args: \")\n",
    "    pprint(args)\n",
    "\n",
    "    # Select Visible GPU\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(args.gpu_ids)\n",
    "    print(\"===GPUs===\")\n",
    "    print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "\n",
    "    def set_hparam_and_train_closure(config: Dict[str, Any]):\n",
    "        \"\"\"Use the (k,v) in `overwrite` to update the args\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        config: Hyperparam search space as a Dict[hparam-name, value of the hpamram]\n",
    "            This dict object is a sample point from the Ray's Hyperparameter space,\n",
    "            and will be used to overwrite the `args`'s key-value with its key-value.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None. Train the model in the specified hyperparmeter space\n",
    "        \"\"\"\n",
    "        print(\"Inside the clousure===\")\n",
    "        pprint(args)\n",
    "        print(\"===\")\n",
    "        pprint(config)\n",
    "\n",
    "        d_args =  vars(args)\n",
    "        for k, v in config.items():\n",
    "            d_args[k] = v\n",
    "            print(\"Overwrote args: \", k)\n",
    "\n",
    "        # Start experiment with this overwritten hyperparams\n",
    "        train_tune(args)\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Specify hyperparameter search space\n",
    "    # ------------------------------------------------------------------------\n",
    "    search_space = {\n",
    "        # \"latent_dim\": tune.grid_search([10, 20, 60, 100]),\n",
    "        'enc_type': tune.choice(['conv', 'resnet']),\n",
    "        'dec_type': tune.choice(['conv', 'resnet']),\n",
    "        'is_contrasive': tune.choice([False, True]),\n",
    "        'kld_weight': tune.choice(np.array([0.5*(2**i) for i in range(12)])), #[0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32., 64, 128., 256, 512, 1024]), #np.array([0.5*(2**i) for i in range(12)])\n",
    "        'use_beta_scheduler': False, #tune.grid_search([False,True]),\n",
    "        'adv_loss_weight': tune.choice(np.logspace(0.0, 7.0, num=8, base=3.0)),\n",
    "        'learning_rate': tune.loguniform(1e-4, 1e-1), #tune.grid_search(list(np.logspace(-4., -1, num=10))),\n",
    "        'batch_size': tune.choice([32, 64, 128,]),\n",
    "    }\n",
    "    \n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Start hyperparameter search using Ray\n",
    "    # ------------------------------------------------------------------------\n",
    "    ray.shutdown()\n",
    "    ray.init(log_to_driver=False)\n",
    "    # search_alg =\n",
    "\n",
    "    reporter = CLIReporter(\n",
    "        parameter_columns=list(search_space.keys()),\n",
    "        metric_columns=[\"loss\", \"mean_accuracy\", \"training_iteration\"])\n",
    "    \n",
    "#     breakpoint()\n",
    "    \n",
    "    analysis = tune.run(\n",
    "        set_hparam_and_train_closure,\n",
    "        config=search_space,\n",
    "        metric='loss', #set to val_loss\n",
    "        mode='min',\n",
    "        # search_alg=search_alg,\n",
    "        num_samples=args.n_ray_samples,\n",
    "        verbose=1,\n",
    "        progress_reporter=reporter,\n",
    "        name=\"Tune-BiVAE\", # name of experiment\n",
    "        local_dir= args.ray_log_dir,\n",
    "        resources_per_trial={\"cpu\":args.n_cpus, \"gpu\": len(args.gpu_ids)}, # there are 16cpus in arya machine; so at a time 16/2=8 trials will be run concurrently\n",
    "    )\n",
    "    print(\"Best hyperparameters found were: \", analysis.best_config)\n",
    "\n",
    "    dfs = analysis.fetch_trial_dataframes()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#         # Debug\n",
    "#     config = {\n",
    "#         # \"latent_dim\": tune.grid_search([10, 20, 60, 100]),\n",
    "#         'enc_type': 'conv',\n",
    "#         'dec_type': 'resnet',\n",
    "#         'is_contrasive': False,\n",
    "#         'kld_weight': 1.0, \n",
    "#         'use_beta_scheduler': False, #tune.grid_search([False,True]),\n",
    "#         'adv_loss_weight': 1.0, \n",
    "#         'learning_rate': 1e-4, \n",
    "#         'batch_size': 32,\n",
    "#     }\n",
    "#     set_hparam_and_train_closure(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "editorial-services",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = \"\"\"--model_name bivae \n",
    "--latent_dim 20 \n",
    "--hidden_dims 32 64 128 256 512 \n",
    "--adv_dim 32 32 32 \n",
    "--data_name \"multi_maptiles\" \n",
    "--cities 'la' 'charlotte' 'vegas' 'boston' 'paris' 'amsterdam' 'shanghai' 'seoul' 'chicago' 'manhattan' 'berlin' 'montreal' 'rome' \n",
    "--styles StamenTonerBackground --n_styles 1 \n",
    "--zooms 14 \n",
    "--gpu_ids 0 --max_epochs 150 --terminate_on_nan True \n",
    "--n_ray_samples 1 \n",
    "--log_root \"/data/hayley-old/Tenanbaum2000/lightning_logs/2021-03-08-ray/\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "found-hindu",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_args = cmd.replace('\\n', '').replace('\"', '').replace('\\'', '').split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "private-point",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--model_name', 'bivae', '--latent_dim', '20', '--hidden_dims', '32', '64', '128', '256', '512', '--adv_dim', '32', '32', '32', '--data_name', 'multi_maptiles', '--cities', 'la', 'charlotte', 'vegas', 'boston', 'paris', 'amsterdam', 'shanghai', 'seoul', 'chicago', 'manhattan', 'berlin', 'montreal', 'rome', '--styles', 'StamenTonerBackground', '--n_styles', '1', '--zooms', '14', '--gpu_ids', '0', '--max_epochs', '150', '--terminate_on_nan', 'True', '--n_ray_samples', '1', '--log_root', '/data/hayley-old/Tenanbaum2000/lightning_logs/2021-03-08-ray/']\n"
     ]
    }
   ],
   "source": [
    "print(user_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "instrumental-breeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base CLI args: \n",
      "Namespace(data_name='multi_maptiles', log_root='/data/hayley-old/Tenanbaum2000/lightning_logs/2021-03-08-ray/', mode='fit', model_name='bivae', verbose=False)\n",
      "Final args: \n",
      "Namespace(accelerator=None, accumulate_grad_batches=1, act_fn='leaky_relu', adv_loss_weight=1.0, adversary_dims=[32, 32, 32], amp_backend='native', amp_level='O2', auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, automatic_optimization=None, batch_size=32, benchmark=False, beta_log_tag='train/beta', beta_n_cycle=4, beta_ratio=0.5, beta_start=0.0, beta_stop=1.0, check_val_every_n_epoch=1, checkpoint_callback=True, cities=['la', 'charlotte', 'vegas', 'boston', 'paris', 'amsterdam', 'shanghai', 'seoul', 'chicago', 'manhattan', 'berlin', 'montreal', 'rome'], data_name='multi_maptiles', data_root='/data/hayley-old/maptiles_v2/', dec_type='conv', default_root_dir=None, deterministic=False, distributed_backend=None, enable_pl_optimizer=True, enc_type='conv', fast_dev_run=False, flush_logs_every_n_steps=100, gpu_ids=['0'], gpus=<function _gpus_arg_default at 0x7f6e1d2b15e0>, gradient_clip_val=0, hidden_dims=[32, 64, 128, 256, 512], in_shape=[3, 32, 32], is_contrasive=True, kld_weight=1.0, latent_dim=20, learning_rate=0.001, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, log_root='/data/hayley-old/Tenanbaum2000/lightning_logs/2021-03-08-ray/', logger=True, max_epochs=150, max_steps=None, min_epochs=1, min_steps=None, mode='fit', model_name='bivae', move_metrics_to_cpu=False, n_cpus=8, n_ray_samples=1, n_styles=1, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=16, overfit_batches=0.0, pin_memory=True, plugins=None, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=1, ray_log_dir='/data/log/ray', reload_dataloaders_every_epoch=False, replace_sampler_ddp=True, resume_from_checkpoint=None, styles=['StamenTonerBackground'], sync_batchnorm=False, terminate_on_nan=True, tpu_cores=<function _gpus_arg_default at 0x7f6e1d2b15e0>, track_grad_norm=-1, truncated_bptt_steps=None, use_beta_scheduler=True, val_check_interval=1.0, verbose=False, weights_save_path=None, weights_summary='top', zooms=['14'])\n",
      "===GPUs===\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-08 15:33:03,392\tINFO services.py:1172 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8267\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 18.4/62.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/16 CPUs, 1/1 GPUs, 0.0/27.78 GiB heap, 0.0/9.57 GiB objects (0/1.0 accelerator_type:X)\n",
      "Result logdir: /data/log/ray/Tune-BiVAE\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-08 15:33:07,023\tERROR trial_runner.py:616 -- Trial set_hparam_and_train_closure_a2338_00000: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/ray/tune/trial_runner.py\", line 586, in _process_trial\n",
      "    results = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\", line 609, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/ray/worker.py\", line 1456, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=27123, ip=68.181.30.41)\n",
      "  File \"python/ray/_raylet.pyx\", line 439, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 473, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 476, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 480, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 432, in ray._raylet.execute_task.function_executor\n",
      "RuntimeError: The actor with name ImplicitFunc failed to be imported, and so cannot execute this method.\n",
      "2021-03-08 15:33:07,028\tWARNING worker.py:1107 -- Failed to unpickle actor class 'ImplicitFunc' for actor ID 4ee449587774c1f0770a083801000000. Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/ray/function_manager.py\", line 496, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "ModuleNotFoundError: No module named 'utils'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 18.5/62.6 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/16 CPUs, 0/1 GPUs, 0.0/27.78 GiB heap, 0.0/9.57 GiB objects (0/1.0 accelerator_type:X)\n",
      "Result logdir: /data/log/ray/Tune-BiVAE\n",
      "Number of trials: 1/1 (1 ERROR)\n",
      "Number of errored trials: 1\n",
      "+------------------------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                               |   # failures | error file                                                                                                                                                                                |\n",
      "|------------------------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| set_hparam_and_train_closure_a2338_00000 |            1 | /data/log/ray/Tune-BiVAE/set_hparam_and_train_closure_a2338_00000_0_adv_loss_weight=81.0,batch_size=32,dec_type=resnet,enc_type=conv,is_contrasive=True,kld_2021-03-08_15-33-06/error.txt |\n",
      "+------------------------------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [set_hparam_and_train_closure_a2338_00000])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-62a220f63468>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-6f43e93b70a6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(user_args)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m#     breakpoint()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     analysis = tune.run(\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mset_hparam_and_train_closure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msearch_space\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/test/lib/python3.8/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, loggers, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [set_hparam_and_train_closure_a2338_00000])"
     ]
    }
   ],
   "source": [
    "main(user_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-today",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-ebony",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-newspaper",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-converter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-chosen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-beatles",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "infrared-fourth",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_args = cmd.replace('\\n', '').replace('\"', '').replace('\\'', '').split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "guilty-fireplace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--model_name', 'bivae', '--latent_dim', '20', '--hidden_dims', '32', '64', '128', '256', '512', '--adv_dim', '32', '32', '32', '--data_name', 'multi_maptiles', '--cities', 'la', 'charlotte', 'vegas', 'boston', 'paris', 'amsterdam', 'shanghai', 'seoul', 'chicago', 'manhattan', 'berlin', 'montreal', 'rome', '--styles', 'StamenTonerBackground', '--n_styles', '1', '--zooms', '14', '--gpu_ids', '0', '--max_epochs', '150', '--terminate_on_nan', 'True', '--n_ray_samples', '1', '--log_root', '/data/hayley-old/Tenanbaum2000/lightning_logs/2021-03-08-ray/']\n"
     ]
    }
   ],
   "source": [
    "print(user_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "regional-london",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base CLI args: \n",
      "Namespace(data_name='multi_maptiles', log_root='/data/hayley-old/Tenanbaum2000/lightning_logs/2021-03-08-ray/', mode='fit', model_name='bivae', verbose=False)\n",
      "Final args: \n",
      "Namespace(accelerator=None, accumulate_grad_batches=1, act_fn='leaky_relu', adv_loss_weight=1.0, adversary_dims=[32, 32, 32], amp_backend='native', amp_level='O2', auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, automatic_optimization=None, batch_size=32, benchmark=False, beta_log_tag='train/beta', beta_n_cycle=4, beta_ratio=0.5, beta_start=0.0, beta_stop=1.0, check_val_every_n_epoch=1, checkpoint_callback=True, cities=['la', 'charlotte', 'vegas', 'boston', 'paris', 'amsterdam', 'shanghai', 'seoul', 'chicago', 'manhattan', 'berlin', 'montreal', 'rome'], data_name='multi_maptiles', data_root='/data/hayley-old/maptiles_v2/', dec_type='conv', default_root_dir=None, deterministic=False, distributed_backend=None, enable_pl_optimizer=True, enc_type='conv', fast_dev_run=False, flush_logs_every_n_steps=100, gpu_ids=['0'], gpus=<function _gpus_arg_default at 0x7fa6246cf1f0>, gradient_clip_val=0, hidden_dims=[32, 64, 128, 256, 512], in_shape=[3, 32, 32], is_contrasive=True, kld_weight=1.0, latent_dim=20, learning_rate=0.001, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, log_root='/data/hayley-old/Tenanbaum2000/lightning_logs/2021-03-08-ray/', logger=True, max_epochs=150, max_steps=None, min_epochs=1, min_steps=None, mode='fit', model_name='bivae', move_metrics_to_cpu=False, n_cpus=8, n_ray_samples=1, n_styles=1, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=16, overfit_batches=0.0, pin_memory=True, plugins=None, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=1, ray_log_dir='/data/log/ray', reload_dataloaders_every_epoch=False, replace_sampler_ddp=True, resume_from_checkpoint=None, styles=['StamenTonerBackground'], sync_batchnorm=False, terminate_on_nan=True, tpu_cores=<function _gpus_arg_default at 0x7fa6246cf1f0>, track_grad_norm=-1, truncated_bptt_steps=None, use_beta_scheduler=True, val_check_interval=1.0, verbose=False, weights_save_path=None, weights_summary='top', zooms=['14'])\n",
      "===GPUs===\n",
      "0\n",
      "Inside the clousure===\n",
      "Namespace(accelerator=None, accumulate_grad_batches=1, act_fn='leaky_relu', adv_loss_weight=1.0, adversary_dims=[32, 32, 32], amp_backend='native', amp_level='O2', auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, automatic_optimization=None, batch_size=32, benchmark=False, beta_log_tag='train/beta', beta_n_cycle=4, beta_ratio=0.5, beta_start=0.0, beta_stop=1.0, check_val_every_n_epoch=1, checkpoint_callback=True, cities=['la', 'charlotte', 'vegas', 'boston', 'paris', 'amsterdam', 'shanghai', 'seoul', 'chicago', 'manhattan', 'berlin', 'montreal', 'rome'], data_name='multi_maptiles', data_root='/data/hayley-old/maptiles_v2/', dec_type='conv', default_root_dir=None, deterministic=False, distributed_backend=None, enable_pl_optimizer=True, enc_type='conv', fast_dev_run=False, flush_logs_every_n_steps=100, gpu_ids=['0'], gpus=<function _gpus_arg_default at 0x7fa6246cf1f0>, gradient_clip_val=0, hidden_dims=[32, 64, 128, 256, 512], in_shape=[3, 32, 32], is_contrasive=True, kld_weight=1.0, latent_dim=20, learning_rate=0.001, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, log_root='/data/hayley-old/Tenanbaum2000/lightning_logs/2021-03-08-ray/', logger=True, max_epochs=150, max_steps=None, min_epochs=1, min_steps=None, mode='fit', model_name='bivae', move_metrics_to_cpu=False, n_cpus=8, n_ray_samples=1, n_styles=1, num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=16, overfit_batches=0.0, pin_memory=True, plugins=None, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=1, ray_log_dir='/data/log/ray', reload_dataloaders_every_epoch=False, replace_sampler_ddp=True, resume_from_checkpoint=None, styles=['StamenTonerBackground'], sync_batchnorm=False, terminate_on_nan=True, tpu_cores=<function _gpus_arg_default at 0x7fa6246cf1f0>, track_grad_norm=-1, truncated_bptt_steps=None, use_beta_scheduler=True, val_check_interval=1.0, verbose=False, weights_save_path=None, weights_summary='top', zooms=['14'])\n",
      "===\n",
      "{'adv_loss_weight': 1.0,\n",
      " 'batch_size': 32,\n",
      " 'dec_type': 'resnet',\n",
      " 'enc_type': 'conv',\n",
      " 'is_contrasive': False,\n",
      " 'kld_weight': 1.0,\n",
      " 'learning_rate': 0.0001,\n",
      " 'use_beta_scheduler': False}\n",
      "Overwrote args:  enc_type\n",
      "Overwrote args:  dec_type\n",
      "Overwrote args:  is_contrasive\n",
      "Overwrote args:  kld_weight\n",
      "Overwrote args:  use_beta_scheduler\n",
      "Overwrote args:  adv_loss_weight\n",
      "Overwrote args:  learning_rate\n",
      "Overwrote args:  batch_size\n",
      "Unique styles:  ['StamenTonerBackground']\n",
      "*** Set the datamodule's df_fns attribute -- Save it for quicker DM init for later runs\n",
      "Unique styles:  ['StamenTonerBackground']\n",
      "Unique styles:  ['StamenTonerBackground']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: /data/hayley-old/Tenanbaum2000/lightning_logs/2021-03-08-ray/BiVAE-conv-resnet-1.0-1.0_Maptiles_la-charlotte-vegas-boston-paris-amsterdam-shanghai-seoul-chicago-manhattan-berlin-montreal-rome_StamenTonerBackground_14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_train, n_val:  3941 1688\n",
      "train channelwise_mean,std:  [0.86187316 0.86187316 0.86187316] [0.33095462 0.33095462 0.33095462]\n",
      "Exp name:  BiVAE-conv-resnet-1.0-1.0_Maptiles_la-charlotte-vegas-boston-paris-amsterdam-shanghai-seoul-chicago-manhattan-berlin-montreal-rome_StamenTonerBackground_14\n",
      "Log Dir:  /data/hayley-old/Tenanbaum2000/lightning_logs/2021-03-08-ray/BiVAE-conv-resnet-1.0-1.0_Maptiles_la-charlotte-vegas-boston-paris-amsterdam-shanghai-seoul-chicago-manhattan-berlin-montreal-rome_StamenTonerBackground_14/version_0\n",
      "Created:  /data/hayley-old/Tenanbaum2000/lightning_logs/2021-03-08-ray/BiVAE-conv-resnet-1.0-1.0_Maptiles_la-charlotte-vegas-boston-paris-amsterdam-shanghai-seoul-chicago-manhattan-berlin-montreal-rome_StamenTonerBackground_14/version_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiVAE-conv-resnet-1.0-1.0_Maptiles_la-charlotte-vegas-boston-paris-amsterdam-shanghai-seoul-chicago-manhattan-berlin-montreal-rome_StamenTonerBackground_14 started...\n",
      "Logging to /data/hayley-old/Tenanbaum2000/lightning_logs/2021-03-08-ray/BiVAE-conv-resnet-1.0-1.0_Maptiles_la-charlotte-vegas-boston-paris-amsterdam-shanghai-seoul-chicago-manhattan-berlin-montreal-rome_StamenTonerBackground_14/version_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: RuntimeWarning: Found unsupported keys in the lr scheduler dict: ['name']\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "   | Name               | Type          | Params\n",
      "------------------------------------------------------\n",
      "0  | act_fn             | LeakyReLU     | 0     \n",
      "1  | out_fn             | Tanh          | 0     \n",
      "2  | encoder            | Sequential    | 1.6 M \n",
      "3  | fc_flatten2qparams | Linear        | 20.5 K\n",
      "4  | fc_latent2flatten  | Linear        | 10.8 K\n",
      "5  | decoder            | ResNetDecoder | 6.3 M \n",
      "6  | out_layer          | Sequential    | 84    \n",
      "7  | adversary          | Sequential    | 2.5 K \n",
      "8  | train_style_acc    | Accuracy      | 0     \n",
      "9  | val_style_acc      | Accuracy      | 0     \n",
      "10 | test_style_acc     | Accuracy      | 0     \n",
      "------------------------------------------------------\n",
      "7.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.9 M     Total params\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 0, batch: 0\n",
      "Ep: 0, batch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 0, batch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-08 14:41:18,751\tWARNING session.py:29 -- Session not detected. You should not be calling `report` outside `tune.run` or while using the class API. \n",
      "2021-03-08 14:41:18,752\tWARNING session.py:33 --   File \"/home/hayley/miniconda3/envs/test/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/traitlets/config/application.py\", line 845, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 612, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/asyncio/events.py\", line 81, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/tornado/ioloop.py\", line 688, in <lambda>\n",
      "    lambda f: self._run_callback(functools.partial(callback, future))\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/tornado/ioloop.py\", line 741, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/tornado/gen.py\", line 814, in inner\n",
      "    self.ctx_run(self.run)\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/tornado/gen.py\", line 775, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 362, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/tornado/gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 265, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/tornado/gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 540, in execute_request\n",
      "    self.do_execute(\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/tornado/gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2877, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2923, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3146, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3338, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-68-62a220f63468>\", line 1, in <module>\n",
      "    main(user_args)\n",
      "  File \"<ipython-input-64-005edaf7de95>\", line 117, in main\n",
      "    set_hparam_and_train_closure(config)\n",
      "  File \"<ipython-input-64-005edaf7de95>\", line 67, in set_hparam_and_train_closure\n",
      "    train_tune(args)\n",
      "  File \"<ipython-input-9-6f37e71b638d>\", line 68, in train_tune\n",
      "    trainer.fit(model, dm)\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 470, in fit\n",
      "    results = self.accelerator_backend.train()\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/pytorch_lightning/accelerators/gpu_accelerator.py\", line 66, in train\n",
      "    results = self.train_or_test()\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 65, in train_or_test\n",
      "    results = self.trainer.train()\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 521, in train\n",
      "    self.train_loop.run_training_epoch()\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 590, in run_training_epoch\n",
      "    self.trainer.run_evaluation(test_mode=False)\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 628, in run_evaluation\n",
      "    self.evaluation_loop.on_evaluation_end()\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\", line 111, in on_evaluation_end\n",
      "    self.trainer.call_hook('on_validation_end', *args, **kwargs)\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 887, in call_hook\n",
      "    trainer_hook(*args, **kwargs)\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/pytorch_lightning/trainer/callback_hook.py\", line 177, in on_validation_end\n",
      "    callback.on_validation_end(self, self.get_model())\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/ray/tune/integration/pytorch_lightning.py\", line 115, in on_validation_end\n",
      "    self._handle(trainer, pl_module)\n",
      "  File \"/home/hayley/miniconda3/envs/test/lib/python3.8/site-packages/ray/tune/integration/pytorch_lightning.py\", line 189, in _handle\n",
      "    tune.report(**report_dict)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 10, batch: 0\n",
      "Ep: 20, batch: 0\n",
      "Ep: 30, batch: 0\n",
      "Ep: 40, batch: 0\n",
      "Ep: 50, batch: 0\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Ep: 60, batch: 0\n",
      "Ep: 70, batch: 0\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Ep: 80, batch: 0\n",
      "Ep: 90, batch: 0\n",
      "Epoch    91: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Ep: 100, batch: 0\n",
      "Epoch   102: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Ep: 110, batch: 0\n",
      "Ep: 120, batch: 0\n",
      "Ep: 130, batch: 0\n",
      "Ep: 140, batch: 0\n",
      "Finished at ep (149, 122)\n",
      "Logged hparams and metrics...\n",
      "\t hparams: \n",
      "{'act_fn': LeakyReLU(negative_slope=0.01),\n",
      " 'adv_loss_weight': 1.0,\n",
      " 'adversary_dims': [32, 32, 32],\n",
      " 'batch_size': 32,\n",
      " 'cities': ['la',\n",
      "            'charlotte',\n",
      "            'vegas',\n",
      "            'boston',\n",
      "            'paris',\n",
      "            'amsterdam',\n",
      "            'shanghai',\n",
      "            'seoul',\n",
      "            'chicago',\n",
      "            'manhattan',\n",
      "            'berlin',\n",
      "            'montreal',\n",
      "            'rome'],\n",
      " 'dec_type': 'resnet',\n",
      " 'enc_type': 'conv',\n",
      " 'hidden_dims': [32, 64, 128, 256, 512],\n",
      " 'in_shape': [3, 32, 32],\n",
      " 'is_contrasive': False,\n",
      " 'kld_weight': 1.0,\n",
      " 'latent_dim': 20,\n",
      " 'learning_rate': 0.0001,\n",
      " 'n_contents': 13,\n",
      " 'n_styles': 1,\n",
      " 'out_fn': Tanh(),\n",
      " 'size_average': False,\n",
      " 'source_names': ['StamenTonerBackground'],\n",
      " 'styles': ['StamenTonerBackground'],\n",
      " 'verbose': False,\n",
      " 'zooms': ['14']}\n",
      "=====\n",
      "\t metrics:  {'hparam/best_score': 34400.3125}\n",
      "Training Done: took 1984.309066772461\n"
     ]
    }
   ],
   "source": [
    "main(user_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-canvas",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-repository",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-circus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-gospel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-failure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-marshall",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-issue",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-madness",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-cargo",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-macintosh",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-liberal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-american",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-tongue",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-jacksonville",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-latin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-composite",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-soviet",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-germany",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-blocking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-triangle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-fourth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-agenda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-anxiety",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-nirvana",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fantastic-handle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args=Namespace(\n",
    "#   act_fn='leaky_relu', \n",
    "#   adv_loss_weight=1.0, \n",
    "#   adversary_dims=[32, 32, 32], \n",
    "#   batch_size=32, \n",
    "#    cities=['la', 'charlotte', 'vegas', 'boston', 'paris', 'amsterdam', 'shanghai', 'seoul', 'chicago', 'manhattan', 'berlin', 'montreal', 'rome'],\n",
    "#     data_name='multi_maptiles', \n",
    "#     data_root='/data/hayley-old/maptiles_v2/', \n",
    "#     dec_type='conv', default_root_dir=None, \n",
    "#     enc_type='conv', \n",
    "#     gpu_ids=['0'], \n",
    "#     hidden_dims=[32, 64, 128, 256, 512], \n",
    "#     in_shape=[3, 32, 32], \n",
    "#     is_contrasive=True, \n",
    "#     kld_weight=1.0, \n",
    "#     latent_dim=20, learning_rate=0.001,  \n",
    "#     log_root='/data/hayley-old/Tenanbaum2000/lightning_logs/2021-03-08-ray/', \n",
    "#     logger=True, \n",
    "#     max_epochs=150,\n",
    "#     mode='fit', \n",
    "#     model_name='bivae',\n",
    "#     n_cpus=8, n_ray_samples=1, n_styles=1, \n",
    "#     ray_log_dir='/data/log/ray',\n",
    "#     styles=['StamenTonerBackground'], \n",
    "#     terminate_on_nan=True, \n",
    "#     use_beta_scheduler=True,\n",
    "#     pin_memory=True,\n",
    "#     num_workers=16,\n",
    "#     verbose=False,\n",
    "#     zooms=['14'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-spokesman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "# train_tune(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-america",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test]",
   "language": "python",
   "name": "conda-env-test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
